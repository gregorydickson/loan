---
phase: 13-lightonocr-gpu-service
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - infrastructure/lightonocr-gpu/Dockerfile
  - infrastructure/lightonocr-gpu/deploy.sh
  - infrastructure/scripts/setup-lightonocr-sa.sh
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Dockerfile builds a vLLM image with LightOnOCR-2-1B model"
    - "Deploy script provisions Cloud Run GPU service with L4"
    - "Service account script creates IAM binding for backend invocation"
  artifacts:
    - path: "infrastructure/lightonocr-gpu/Dockerfile"
      provides: "vLLM-based container for LightOnOCR model serving"
      contains: "vllm/vllm-openai"
    - path: "infrastructure/lightonocr-gpu/deploy.sh"
      provides: "Cloud Run GPU deployment script"
      contains: "gcloud run deploy"
    - path: "infrastructure/scripts/setup-lightonocr-sa.sh"
      provides: "Service account setup for GPU service"
      contains: "iam service-accounts"
  key_links:
    - from: "infrastructure/lightonocr-gpu/Dockerfile"
      to: "lightonai/LightOnOCR-2-1B"
      via: "huggingface-cli download"
      pattern: "huggingface-cli download lightonai/LightOnOCR"
    - from: "infrastructure/lightonocr-gpu/deploy.sh"
      to: "Dockerfile"
      via: "Artifact Registry image reference"
      pattern: "docker.pkg.dev.*lightonocr"
---

<objective>
Create Docker image and deployment infrastructure for LightOnOCR GPU service

Purpose: Prepare all infrastructure artifacts needed to deploy the LightOnOCR model on Cloud Run GPU with L4
Output: Dockerfile with vLLM + transformers, deployment script with gcloud commands, service account setup script
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-lightonocr-gpu-service/13-RESEARCH.md
@.planning/phases/13-lightonocr-gpu-service/13-CONTEXT.md
@infrastructure/cloudbuild/setup-service-account.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LightOnOCR Dockerfile</name>
  <files>infrastructure/lightonocr-gpu/Dockerfile</files>
  <action>
Create Dockerfile based on vLLM official image with LightOnOCR-2-1B model.

Key requirements from RESEARCH.md:
1. Base image: `vllm/vllm-openai:v0.11.2`
2. Install transformers from source (required for LightOnOCR-2-1B)
3. Download model at build time for faster cold starts
4. Configure vLLM entrypoint with LightOnOCR-specific flags

Dockerfile structure:
```dockerfile
# Base vLLM image with CUDA support
FROM vllm/vllm-openai:v0.11.2

# LightOnOCR-2 requires transformers from source (not stable release)
RUN pip install git+https://github.com/huggingface/transformers.git
RUN pip install pillow pypdfium2

# Download model at build time for faster cold starts
ENV HF_HOME=/model-cache
RUN huggingface-cli download lightonai/LightOnOCR-2-1B

# Run in offline mode after download (no network needed at runtime)
ENV HF_HUB_OFFLINE=1

# vLLM entrypoint with LightOnOCR-specific flags
# --limit-mm-per-prompt: one image per request
# --mm-processor-cache-gb 0: disable multimodal cache (not needed for OCR)
# --no-enable-prefix-caching: disable prefix caching (one-off requests)
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server", \
    "--port", "8000", \
    "--model", "lightonai/LightOnOCR-2-1B", \
    "--limit-mm-per-prompt", "{\"image\": 1}", \
    "--mm-processor-cache-gb", "0", \
    "--no-enable-prefix-caching"]
```

Add .dockerignore in same directory to exclude unnecessary files.
  </action>
  <verify>
Run `docker build -t lightonocr-test infrastructure/lightonocr-gpu/` to verify Dockerfile syntax.
Note: Full build requires GPU and ~10 minutes - syntax check is sufficient for this task.
  </verify>
  <done>Dockerfile exists at infrastructure/lightonocr-gpu/Dockerfile with vLLM base, transformers from source, and LightOnOCR-2-1B model download</done>
</task>

<task type="auto">
  <name>Task 2: Create deployment script and service account setup</name>
  <files>infrastructure/lightonocr-gpu/deploy.sh, infrastructure/scripts/setup-lightonocr-sa.sh</files>
  <action>
Create two shell scripts:

**1. infrastructure/scripts/setup-lightonocr-sa.sh**
Service account setup script (run once before deployment):
- Creates lightonocr-gpu service account
- Grants run.invoker role to backend service account
- Pattern: Follow existing setup-service-account.sh in infrastructure/cloudbuild/

```bash
#!/bin/bash
set -euo pipefail

PROJECT_ID="${PROJECT_ID:-$(gcloud config get-value project)}"
REGION="${REGION:-us-central1}"

echo "Setting up LightOnOCR GPU service account..."

# Create service account for GPU service
gcloud iam service-accounts create lightonocr-gpu \
  --display-name="LightOnOCR GPU Service" \
  --project="$PROJECT_ID" 2>/dev/null || echo "Service account already exists"

SA_EMAIL="lightonocr-gpu@${PROJECT_ID}.iam.gserviceaccount.com"
echo "Created service account: $SA_EMAIL"

# Grant invoker role to backend service account (for service-to-service calls)
BACKEND_SA="backend-service@${PROJECT_ID}.iam.gserviceaccount.com"
echo "Granting run.invoker to backend service account..."

# Note: IAM binding will be set after service is deployed via gcloud run services add-iam-policy-binding
echo "Service account setup complete. Run deploy.sh to deploy the service."
```

**2. infrastructure/lightonocr-gpu/deploy.sh**
Full deployment script with build and deploy steps:

```bash
#!/bin/bash
set -euo pipefail

# Configuration
PROJECT_ID="${PROJECT_ID:-$(gcloud config get-value project)}"
REGION="${REGION:-us-central1}"
SERVICE_NAME="lightonocr-gpu"
AR_REPO="cloud-run-source-deploy"  # Default Artifact Registry repo
IMAGE="${REGION}-docker.pkg.dev/${PROJECT_ID}/${AR_REPO}/${SERVICE_NAME}:latest"

echo "=== LightOnOCR GPU Service Deployment ==="
echo "Project: $PROJECT_ID"
echo "Region: $REGION"
echo "Image: $IMAGE"

# Step 1: Build and push Docker image
echo ""
echo "Building Docker image (this may take 10+ minutes for model download)..."
cd "$(dirname "$0")"
docker build -t "$IMAGE" .
docker push "$IMAGE"

# Step 2: Deploy to Cloud Run with GPU
echo ""
echo "Deploying to Cloud Run with L4 GPU..."
gcloud run deploy "$SERVICE_NAME" \
  --image "$IMAGE" \
  --service-account "lightonocr-gpu@${PROJECT_ID}.iam.gserviceaccount.com" \
  --cpu 8 \
  --memory 32Gi \
  --gpu 1 \
  --gpu-type nvidia-l4 \
  --port 8000 \
  --region "$REGION" \
  --no-allow-unauthenticated \
  --min-instances 0 \
  --max-instances 3 \
  --no-cpu-throttling \
  --no-gpu-zonal-redundancy \
  --startup-probe "tcpSocket.port=8000,initialDelaySeconds=240,failureThreshold=1,timeoutSeconds=240,periodSeconds=240"

# Step 3: Grant invoker role to backend service account
echo ""
echo "Granting backend service account permission to invoke GPU service..."
BACKEND_SA="backend-service@${PROJECT_ID}.iam.gserviceaccount.com"
gcloud run services add-iam-policy-binding "$SERVICE_NAME" \
  --member "serviceAccount:${BACKEND_SA}" \
  --role roles/run.invoker \
  --region "$REGION"

# Step 4: Get service URL
SERVICE_URL=$(gcloud run services describe "$SERVICE_NAME" --region "$REGION" --format "value(status.url)")
echo ""
echo "=== Deployment Complete ==="
echo "Service URL: $SERVICE_URL"
echo "Health check: curl -sf ${SERVICE_URL}/health"
echo ""
echo "Add to .env: LIGHTONOCR_SERVICE_URL=$SERVICE_URL"
```

Make both scripts executable (chmod +x).
  </action>
  <verify>
Run `bash -n infrastructure/lightonocr-gpu/deploy.sh` and `bash -n infrastructure/scripts/setup-lightonocr-sa.sh` to verify shell syntax.
Check scripts are executable with `ls -la`.
  </verify>
  <done>
- infrastructure/lightonocr-gpu/deploy.sh exists with docker build, gcloud run deploy with GPU flags, and IAM setup
- infrastructure/scripts/setup-lightonocr-sa.sh exists with service account creation
- Both scripts are executable
  </done>
</task>

</tasks>

<verification>
1. `ls -la infrastructure/lightonocr-gpu/` shows Dockerfile and deploy.sh
2. `ls -la infrastructure/scripts/setup-lightonocr-sa.sh` shows script exists
3. `bash -n infrastructure/lightonocr-gpu/deploy.sh` passes syntax check
4. `grep "vllm/vllm-openai" infrastructure/lightonocr-gpu/Dockerfile` finds base image
5. `grep "nvidia-l4" infrastructure/lightonocr-gpu/deploy.sh` finds GPU type
</verification>

<success_criteria>
- Dockerfile uses vLLM base image with transformers from source
- Dockerfile downloads LightOnOCR-2-1B at build time
- Deploy script includes all required gcloud run deploy flags (gpu, memory, scale-to-zero, startup probe)
- Service account setup script follows project patterns
- All scripts pass shell syntax validation
</success_criteria>

<output>
After completion, create `.planning/phases/13-lightonocr-gpu-service/13-01-SUMMARY.md`
</output>
