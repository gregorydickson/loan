---
phase: 11-langextract-core-integration
plan: 03
type: execute
wave: 2
depends_on: ["11-01", "11-02"]
files_modified:
  - backend/src/extraction/langextract_processor.py
  - backend/src/extraction/offset_translator.py
  - backend/pyproject.toml
autonomous: true
user_setup: []

must_haves:
  truths:
    - "LangExtractProcessor extracts borrower data using Gemini 3.0 Flash"
    - "LangExtractProcessor returns BorrowerRecord with char_start/char_end in SourceReference"
    - "OffsetTranslator maps Docling markdown positions to raw text positions"
    - "LangExtract library is installed and configured with API key"
  artifacts:
    - path: "backend/src/extraction/langextract_processor.py"
      provides: "LangExtract-based extraction processor"
      contains: "class LangExtractProcessor"
      exports: ["LangExtractProcessor"]
    - path: "backend/src/extraction/offset_translator.py"
      provides: "Docling markdown to raw text offset mapping"
      contains: "class OffsetTranslator"
      exports: ["OffsetTranslator"]
    - path: "backend/pyproject.toml"
      provides: "LangExtract dependency"
      contains: "langextract"
  key_links:
    - from: "backend/src/extraction/langextract_processor.py"
      to: "backend/examples/borrower_examples.py"
      via: "import few-shot examples"
      pattern: "from examples.*import"
    - from: "backend/src/extraction/langextract_processor.py"
      to: "backend/src/models/document.py"
      via: "SourceReference with char offsets"
      pattern: "SourceReference.*char_start"
---

<objective>
Implement LangExtractProcessor and OffsetTranslator for character-level source-grounded extraction.

Purpose: LangExtractProcessor wraps LangExtract to extract borrower data with character-level offsets. OffsetTranslator handles the mismatch between Docling markdown positions and original text positions.

Output: Working LangExtract integration that produces BorrowerRecord with char_start/char_end populated in SourceReference.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-langextract-core-integration/11-RESEARCH.md
@.planning/phases/11-langextract-core-integration/11-01-SUMMARY.md
@.planning/phases/11-langextract-core-integration/11-02-SUMMARY.md

@backend/src/extraction/extractor.py
@backend/src/models/document.py
@backend/src/models/borrower.py
@backend/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add langextract dependency to pyproject.toml</name>
  <files>backend/pyproject.toml</files>
  <action>
Add langextract to the dependencies section in backend/pyproject.toml:

```toml
dependencies = [
    # ... existing deps ...
    "langextract>=1.1.1",
]
```

Place it near google-genai since they're related (both Google AI).

After editing, install the package:
```bash
cd /Users/gregorydickson/stackpoint/loan/backend && pip install langextract>=1.1.1
```
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "import langextract; print(f'langextract version: {langextract.__version__}')"`
Expected: Shows version 1.1.1 or higher
  </verify>
  <done>langextract package installed and importable</done>
</task>

<task type="auto">
  <name>Task 2: Create OffsetTranslator for Docling markdown to raw text mapping</name>
  <files>backend/src/extraction/offset_translator.py</files>
  <action>
Create backend/src/extraction/offset_translator.py with:

```python
"""Offset translation between Docling markdown and raw text.

Docling converts PDF/DOCX to structured markdown with formatting changes.
Character positions in the markdown don't correspond directly to positions
in the original document. This module provides translation between the two.
"""

from difflib import SequenceMatcher

from rapidfuzz import fuzz


class OffsetTranslator:
    """Translate character offsets between Docling markdown and raw text.

    Docling's markdown output has different character positions than the
    raw PDF text due to:
    - Added markdown formatting (headers, lists, bold)
    - Whitespace normalization
    - Section reorganization

    This class finds anchor points (exact substring matches) between the
    two texts and interpolates positions between anchors.
    """

    def __init__(self, docling_markdown: str, raw_text: str | None = None):
        """Initialize with Docling markdown and optional raw text.

        Args:
            docling_markdown: Markdown text from Docling processor
            raw_text: Original raw text if available, None if only markdown
        """
        self.markdown = docling_markdown
        self.raw = raw_text
        self._matches: list[tuple[int, int, int]] = []

        if raw_text:
            self._build_alignment_map()

    def _build_alignment_map(self) -> None:
        """Build bidirectional character position mapping using difflib."""
        if not self.raw:
            return

        matcher = SequenceMatcher(None, self.markdown, self.raw, autojunk=False)
        self._matches = matcher.get_matching_blocks()

    def markdown_to_raw(self, start: int, end: int) -> tuple[int | None, int | None]:
        """Convert markdown character positions to raw text positions.

        Args:
            start: Start position in markdown (inclusive)
            end: End position in markdown (exclusive)

        Returns:
            Tuple of (raw_start, raw_end) or (None, None) if not mappable
        """
        if not self.raw or not self._matches:
            return (None, None)

        # Find the matching blocks that contain or are nearest to our positions
        raw_start = self._find_raw_position(start)
        raw_end = self._find_raw_position(end)

        return (raw_start, raw_end)

    def _find_raw_position(self, markdown_pos: int) -> int | None:
        """Find the raw text position corresponding to a markdown position."""
        if not self._matches:
            return None

        # Search through matching blocks
        for md_start, raw_start, length in self._matches:
            if length == 0:  # Skip empty matches
                continue
            if md_start <= markdown_pos < md_start + length:
                # Position is within this matching block
                offset = markdown_pos - md_start
                return raw_start + offset

        # Position not in a matching block - interpolate from nearest
        return self._interpolate_position(markdown_pos)

    def _interpolate_position(self, markdown_pos: int) -> int | None:
        """Interpolate raw position from nearest matching blocks."""
        if not self._matches or len(self._matches) < 2:
            return None

        # Find the blocks before and after the position
        before = None
        after = None

        for md_start, raw_start, length in self._matches:
            if length == 0:
                continue
            if md_start + length <= markdown_pos:
                before = (md_start + length, raw_start + length)
            elif md_start > markdown_pos and after is None:
                after = (md_start, raw_start)
                break

        if before and after:
            # Linear interpolation
            md_before, raw_before = before
            md_after, raw_after = after
            md_range = md_after - md_before
            if md_range == 0:
                return raw_before
            ratio = (markdown_pos - md_before) / md_range
            return int(raw_before + ratio * (raw_after - raw_before))

        return None

    def verify_offset(
        self,
        char_start: int,
        char_end: int,
        expected_text: str,
        threshold: float = 0.85,
    ) -> bool:
        """Verify that offsets correctly locate the expected text.

        Args:
            char_start: Start position (inclusive)
            char_end: End position (exclusive)
            expected_text: Text expected at this position
            threshold: Minimum fuzzy match ratio (0.0 to 1.0)

        Returns:
            True if the text at offsets matches expected text
        """
        if char_start < 0 or char_end > len(self.markdown):
            return False

        actual = self.markdown[char_start:char_end]

        # Exact match
        if actual == expected_text:
            return True

        # Fuzzy match using rapidfuzz (already in project)
        ratio = fuzz.ratio(actual, expected_text) / 100.0
        return ratio >= threshold

    def get_markdown_substring(self, char_start: int, char_end: int) -> str:
        """Get substring from markdown at given offsets.

        Args:
            char_start: Start position (inclusive)
            char_end: End position (exclusive)

        Returns:
            Substring or empty string if positions invalid
        """
        if char_start < 0 or char_end > len(self.markdown) or char_start >= char_end:
            return ""
        return self.markdown[char_start:char_end]
```

Key design decisions:
- Uses difflib.SequenceMatcher for alignment (standard library, reliable)
- Uses rapidfuzz for fuzzy verification (already in project dependencies)
- Handles case where raw text is not available (markdown-only mode)
- Provides verify_offset for substring matching validation (LXTR-08)
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "
from src.extraction.offset_translator import OffsetTranslator

# Test basic functionality
markdown = 'Hello **World** from Docling'
raw = 'Hello World from Docling'

translator = OffsetTranslator(markdown, raw)
result = translator.verify_offset(0, 5, 'Hello')
print(f'verify_offset: {result}')
print('OffsetTranslator OK')
"`
Expected: verify_offset: True, OffsetTranslator OK
  </verify>
  <done>OffsetTranslator created with markdown-to-raw mapping and verification</done>
</task>

<task type="auto">
  <name>Task 3: Create LangExtractProcessor for source-grounded extraction</name>
  <files>backend/src/extraction/langextract_processor.py</files>
  <action>
Create backend/src/extraction/langextract_processor.py following the research Pattern 1 and Pattern 2:

```python
"""LangExtract-based extraction processor for character-level source grounding.

This processor uses Google's LangExtract library to extract borrower data
with precise character-level offsets. Unlike the Docling-based BorrowerExtractor,
this processor populates char_start/char_end in SourceReference.
"""

import logging
import os
from dataclasses import dataclass, field
from decimal import Decimal
from uuid import UUID, uuid4

import langextract as lx
from langextract.core.data import AnnotatedDocument, CharInterval

from examples import ALL_EXAMPLES
from src.extraction.offset_translator import OffsetTranslator
from src.models.borrower import Address, BorrowerRecord, IncomeRecord
from src.models.document import SourceReference

logger = logging.getLogger(__name__)


@dataclass
class LangExtractResult:
    """Result from LangExtract extraction.

    Attributes:
        borrowers: Extracted borrower records with char offsets
        raw_extractions: Raw LangExtract result for debugging
        alignment_warnings: Any alignment issues detected
    """

    borrowers: list[BorrowerRecord]
    raw_extractions: AnnotatedDocument | None = None
    alignment_warnings: list[str] = field(default_factory=list)


class LangExtractProcessor:
    """Extract borrower data using LangExtract with character-level source grounding.

    Uses Gemini 3.0 Flash via LangExtract for extraction with precise
    character offset tracking. The few-shot examples define the extraction
    schema - no code-defined schema needed.

    Example:
        processor = LangExtractProcessor()
        result = processor.extract(
            document_text=docling_markdown,
            document_id=uuid,
            document_name="loan.pdf"
        )
    """

    def __init__(self, api_key: str | None = None):
        """Initialize with Gemini API key.

        Args:
            api_key: Gemini API key. If None, uses GOOGLE_API_KEY env var.
        """
        # LangExtract uses LANGEXTRACT_API_KEY env var
        key = api_key or os.environ.get("GOOGLE_API_KEY")
        if key:
            os.environ["LANGEXTRACT_API_KEY"] = key

        self.examples = ALL_EXAMPLES
        self._model_id = "gemini-3.0-flash"

    def extract(
        self,
        document_text: str,
        document_id: UUID,
        document_name: str,
        raw_text: str | None = None,
        extraction_passes: int = 2,
    ) -> LangExtractResult:
        """Extract borrower data with character-level source references.

        Args:
            document_text: Docling markdown output to extract from
            document_id: UUID of source document
            document_name: Human-readable document name
            raw_text: Optional raw text for offset translation
            extraction_passes: Number of extraction passes (1-5)

        Returns:
            LangExtractResult with borrowers and metadata
        """
        # Create offset translator for verification
        translator = OffsetTranslator(document_text, raw_text)

        # Run LangExtract
        try:
            result: AnnotatedDocument = lx.extract(
                text_or_documents=document_text,
                prompt_description=self._get_prompt_description(),
                examples=self.examples,
                model_id=self._model_id,
                extraction_passes=extraction_passes,
            )
        except Exception as e:
            logger.error("LangExtract extraction failed: %s", str(e))
            return LangExtractResult(
                borrowers=[],
                alignment_warnings=[f"Extraction failed: {str(e)}"],
            )

        # Convert to BorrowerRecords
        borrowers, warnings = self._convert_to_borrower_records(
            result=result,
            document_id=document_id,
            document_name=document_name,
            source_text=document_text,
            translator=translator,
        )

        return LangExtractResult(
            borrowers=borrowers,
            raw_extractions=result,
            alignment_warnings=warnings,
        )

    def _get_prompt_description(self) -> str:
        """Get the prompt description for LangExtract."""
        return """Extract all borrower information from this loan document including:
- Borrower names and personal identifiers (SSN)
- Contact information (address, phone, email)
- Income records with amounts, periods, years, and employers
- Account numbers and loan numbers

Extract data exactly as it appears in the document. If information is unclear or ambiguous, omit it rather than guessing."""

    def _convert_to_borrower_records(
        self,
        result: AnnotatedDocument,
        document_id: UUID,
        document_name: str,
        source_text: str,
        translator: OffsetTranslator,
    ) -> tuple[list[BorrowerRecord], list[str]]:
        """Convert LangExtract result to BorrowerRecord objects.

        Groups extractions by borrower and creates SourceReference with
        character offsets for each extraction.
        """
        warnings: list[str] = []
        borrower_map: dict[str, dict] = {}  # name -> accumulated data

        for extraction in result.extractions:
            # Get character offsets
            char_start: int | None = None
            char_end: int | None = None

            char_interval: CharInterval | None = extraction.char_interval
            if char_interval:
                char_start = char_interval.start_pos
                char_end = char_interval.end_pos

                # Verify offset (LXTR-08)
                if not translator.verify_offset(
                    char_start, char_end, extraction.extraction_text
                ):
                    warnings.append(
                        f"Offset verification failed for '{extraction.extraction_text[:30]}...'"
                    )

            # Track alignment status
            alignment = getattr(extraction, "alignment_status", None)
            if alignment and alignment != "match_exact":
                warnings.append(
                    f"Fuzzy alignment for '{extraction.extraction_text[:30]}...': {alignment}"
                )

            # Group by extraction class
            if extraction.extraction_class == "borrower":
                self._process_borrower_extraction(
                    extraction, borrower_map, document_id, document_name,
                    source_text, char_start, char_end
                )
            elif extraction.extraction_class == "income":
                self._process_income_extraction(extraction, borrower_map)
            elif extraction.extraction_class in ("account", "loan"):
                self._process_account_extraction(extraction, borrower_map)

        # Convert accumulated data to BorrowerRecords
        borrowers = []
        for name, data in borrower_map.items():
            try:
                borrower = self._create_borrower_record(name, data)
                borrowers.append(borrower)
            except Exception as e:
                warnings.append(f"Failed to create BorrowerRecord for '{name}': {e}")

        return borrowers, warnings

    def _process_borrower_extraction(
        self,
        extraction,
        borrower_map: dict,
        document_id: UUID,
        document_name: str,
        source_text: str,
        char_start: int | None,
        char_end: int | None,
    ) -> None:
        """Process a borrower extraction and add to borrower_map."""
        name = extraction.extraction_text
        attrs = extraction.attributes or {}

        if name not in borrower_map:
            # Create snippet from source text around extraction
            snippet = ""
            if char_start is not None and char_end is not None:
                # Get surrounding context (up to 200 chars)
                ctx_start = max(0, char_start - 50)
                ctx_end = min(len(source_text), char_end + 150)
                snippet = source_text[ctx_start:ctx_end]
            else:
                snippet = source_text[:200] if len(source_text) > 200 else source_text

            borrower_map[name] = {
                "ssn": attrs.get("ssn"),
                "phone": attrs.get("phone"),
                "email": attrs.get("email"),
                "address": {
                    "street": attrs.get("street"),
                    "city": attrs.get("city"),
                    "state": attrs.get("state"),
                    "zip_code": attrs.get("zip_code"),
                } if attrs.get("street") else None,
                "income_history": [],
                "account_numbers": [],
                "loan_numbers": [],
                "sources": [
                    SourceReference(
                        document_id=document_id,
                        document_name=document_name,
                        page_number=1,  # LangExtract doesn't track page; default to 1
                        snippet=snippet[:500],  # Limit snippet length
                        char_start=char_start,
                        char_end=char_end,
                    )
                ],
            }
        else:
            # Merge additional data
            if attrs.get("ssn") and not borrower_map[name].get("ssn"):
                borrower_map[name]["ssn"] = attrs["ssn"]
            if attrs.get("phone") and not borrower_map[name].get("phone"):
                borrower_map[name]["phone"] = attrs["phone"]
            if attrs.get("email") and not borrower_map[name].get("email"):
                borrower_map[name]["email"] = attrs["email"]

    def _process_income_extraction(self, extraction, borrower_map: dict) -> None:
        """Process an income extraction."""
        attrs = extraction.attributes or {}

        income = {
            "amount": attrs.get("amount"),
            "period": attrs.get("period", "annual"),
            "year": attrs.get("year"),
            "source_type": attrs.get("source_type", "employment"),
            "employer": attrs.get("employer"),
        }

        # Add to first borrower (simplified - real impl would match by employer/context)
        if borrower_map:
            first_borrower = next(iter(borrower_map.values()))
            first_borrower["income_history"].append(income)

    def _process_account_extraction(self, extraction, borrower_map: dict) -> None:
        """Process an account or loan extraction."""
        attrs = extraction.attributes or {}
        number = extraction.extraction_text

        # Add to first borrower
        if borrower_map:
            first_borrower = next(iter(borrower_map.values()))
            if extraction.extraction_class == "loan":
                first_borrower["loan_numbers"].append(number)
            else:
                first_borrower["account_numbers"].append(number)

    def _create_borrower_record(self, name: str, data: dict) -> BorrowerRecord:
        """Create a BorrowerRecord from accumulated extraction data."""
        address = None
        if data.get("address") and data["address"].get("street"):
            addr_data = data["address"]
            address = Address(
                street=addr_data["street"],
                city=addr_data.get("city", "Unknown"),
                state=addr_data.get("state", "XX"),
                zip_code=addr_data.get("zip_code", "00000"),
            )

        income_history = []
        for inc in data.get("income_history", []):
            if inc.get("amount") and inc.get("year"):
                try:
                    amount = Decimal(str(inc["amount"]).replace(",", "").replace("$", ""))
                    year = int(inc["year"])
                    income_history.append(
                        IncomeRecord(
                            amount=amount,
                            period=inc.get("period", "annual"),
                            year=year,
                            source_type=inc.get("source_type", "employment"),
                            employer=inc.get("employer"),
                        )
                    )
                except (ValueError, TypeError):
                    pass  # Skip invalid income records

        return BorrowerRecord(
            id=uuid4(),
            name=name,
            ssn=data.get("ssn"),
            phone=data.get("phone"),
            email=data.get("email"),
            address=address,
            income_history=income_history,
            account_numbers=data.get("account_numbers", []),
            loan_numbers=data.get("loan_numbers", []),
            sources=data.get("sources", []),
            confidence_score=0.8,  # LangExtract extractions are high confidence
        )
```

Key design decisions:
- Uses GOOGLE_API_KEY env var (same as existing Gemini client)
- Groups extractions by borrower name to merge related data
- Populates char_start/char_end in SourceReference
- Includes offset verification (LXTR-08)
- Handles alignment warnings gracefully
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "
from src.extraction.langextract_processor import LangExtractProcessor, LangExtractResult
print('LangExtractProcessor imports OK')
# Note: Full extraction test requires API key
"`
Expected: "LangExtractProcessor imports OK"
  </verify>
  <done>LangExtractProcessor created with Gemini 3.0 Flash integration and char offset support</done>
</task>

</tasks>

<verification>
1. LangExtract library installed:
   ```bash
   cd /Users/gregorydickson/stackpoint/loan/backend && pip show langextract
   ```

2. All imports work:
   ```bash
   cd /Users/gregorydickson/stackpoint/loan/backend && python -c "
   from src.extraction.langextract_processor import LangExtractProcessor
   from src.extraction.offset_translator import OffsetTranslator
   print('Imports OK')
   "
   ```

3. OffsetTranslator verification works:
   ```bash
   cd /Users/gregorydickson/stackpoint/loan/backend && python -c "
   from src.extraction.offset_translator import OffsetTranslator
   t = OffsetTranslator('Hello World')
   assert t.verify_offset(0, 5, 'Hello') == True
   assert t.verify_offset(0, 5, 'World') == False
   print('Verification OK')
   "
   ```

4. pyproject.toml has langextract:
   ```bash
   grep langextract /Users/gregorydickson/stackpoint/loan/backend/pyproject.toml
   ```
</verification>

<success_criteria>
1. langextract>=1.1.1 added to pyproject.toml and installed
2. OffsetTranslator class translates markdown positions to raw text and verifies offsets
3. LangExtractProcessor extracts borrowers using Gemini 3.0 Flash via LangExtract
4. LangExtractProcessor populates char_start/char_end in SourceReference
5. Integration uses existing GOOGLE_API_KEY env var
6. Requirements LXTR-03, LXTR-08, LXTR-09 are satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/11-langextract-core-integration/11-03-SUMMARY.md`
</output>
