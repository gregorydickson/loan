---
phase: 17-testing-quality
plan: 03
type: execute
wave: 2
depends_on: ["17-01", "17-02"]
files_modified:
  - backend/tests/unit/ocr/test_gpu_cold_start.py
  - backend/tests/integration/test_gpu_performance.py
autonomous: true

must_haves:
  truths:
    - "GPU cold start timeout handling is tested"
    - "Circuit breaker fallback on timeout is verified"
    - "Test coverage remains above 80%"
    - "All TEST requirements are satisfied"
  artifacts:
    - path: "backend/tests/unit/ocr/test_gpu_cold_start.py"
      provides: "GPU cold start timeout handling tests"
      exports: ["TestGPUColdStartHandling"]
    - path: "backend/tests/integration/test_gpu_performance.py"
      provides: "GPU service performance tests"
      exports: ["TestGPUServicePerformance"]
  key_links:
    - from: "backend/tests/unit/ocr/test_gpu_cold_start.py"
      to: "backend/src/ocr/lightonocr_client.py"
      via: "timeout configuration"
      pattern: "timeout"
    - from: "backend/tests/integration/test_gpu_performance.py"
      to: "backend/src/ocr/ocr_router.py"
      via: "circuit breaker fallback"
      pattern: "CircuitBreakerError"
---

<objective>
Add GPU cold start performance tests and verify all TEST requirements are satisfied

Purpose: TEST-10 requires GPU service cold start performance tests. This plan adds tests for timeout handling during cold starts and verifies the complete test suite satisfies all Phase 17 requirements.

Output:
- GPU cold start timeout handling tests
- Circuit breaker fallback verification tests
- Final coverage and compliance verification
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/tests/unit/ocr/test_lightonocr_client.py
@backend/tests/unit/ocr/test_ocr_router.py
@backend/src/ocr/lightonocr_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPU cold start timeout tests</name>
  <files>backend/tests/unit/ocr/test_gpu_cold_start.py</files>
  <action>
Create tests for GPU cold start timeout handling (TEST-10).

**Before creating new file:**
1. Review existing `backend/tests/unit/ocr/test_lightonocr_client.py` to check for timeout coverage
2. Specifically check for: test_extract_text_timeout, health_check timeout tests
3. Note which cold start scenarios are already covered vs need new tests

**Create `backend/tests/unit/ocr/test_gpu_cold_start.py`** with tests NOT already covered:

TestGPUColdStartHandling class:
- test_timeout_configuration: Verify default 120s timeout for cold start tolerance
- test_custom_timeout_accepted: Verify custom timeout configuration works
- test_timeout_raises_lightonocr_error: Timeout during request raises LightOnOCRError
- test_connect_timeout_handled: Connection timeout handled gracefully
- test_read_timeout_handled: Read timeout (slow response) handled gracefully

TestGPUHealthCheckTimeout class:
- test_health_check_timeout_returns_false: Health check returns False on timeout
- test_health_check_success_returns_true: Health check returns True when service responds

TestCircuitBreakerColdStartIntegration class:
- test_cold_gpu_triggers_docling_fallback: When GPU health check fails, fallback to Docling
- test_repeated_timeouts_open_circuit_breaker: Verify breaker config (fail_max=3, timeout=60s)

Use mocking patterns from test_lightonocr_client.py. Patch httpx.AsyncClient and id_token.fetch_id_token.
  </action>
  <verify>
```bash
cd backend && python3 -m pytest tests/unit/ocr/test_gpu_cold_start.py -v 2>&1 | grep -E "passed|failed|PASSED|FAILED"
```
All tests should pass.
  </verify>
  <done>GPU cold start timeout tests pass</done>
</task>

<task type="auto">
  <name>Task 2: Comprehensive TEST requirement verification</name>
  <files>backend/tests/</files>
  <action>
Verify all 12 TEST requirements are satisfied by existing test files. This is an explicit verification task, not just checking metrics.

**Step 1: Run full test suite with coverage**
```bash
cd backend && python3 -m pytest --cov=src --cov-report=term-missing --cov-fail-under=80 -v 2>&1 | tee /tmp/test_results.txt
```

**Step 2: Verify mypy compliance**
```bash
cd backend && python3 -m mypy src/ 2>&1 | tee /tmp/mypy_results.txt
```

**Step 3: Explicit TEST requirement verification**

Verify each TEST requirement has test coverage by checking specific files:

| Req | Description | Test File | Verification |
|-----|-------------|-----------|--------------|
| TEST-01 | Few-shot example validation | test_few_shot_examples.py | Check file exists, run: `pytest tests/unit/extraction/test_few_shot_examples.py -v` |
| TEST-02 | Character offset substring matching | test_char_offset_verification.py | Run: `pytest tests/unit/test_char_offset_verification.py -v` Verify TestCharacterOffsetVerification class tests substring matching |
| TEST-03 | LightOnOCR GPU service | test_lightonocr_client.py | Run: `pytest tests/unit/ocr/test_lightonocr_client.py -v` Verify TestLightOnOCRClient class exists |
| TEST-04 | Scanned document detection | test_scanned_detector.py | Run: `pytest tests/unit/ocr/test_scanned_detector.py -v` Verify TestScannedDocumentDetector class exists |
| TEST-05 | E2E Docling extraction | test_e2e_langextract.py | Check test_docling_default_method_still_works exists |
| TEST-06 | E2E LangExtract extraction | test_e2e_langextract.py | Check test_langextract_path_produces_char_offsets exists |
| TEST-07 | Dual pipeline method selection | test_dual_pipeline.py | Run: `pytest tests/integration/test_dual_pipeline.py -v` Verify TestExtractionRouterMethodSelection exists |
| TEST-08 | OCR routing logic | test_ocr_router.py | Run: `pytest tests/unit/ocr/test_ocr_router.py -v` Verify tests for auto/force/skip modes exist |
| TEST-09 | Character offset alignment validation | test_offset_translator.py | Run: `pytest tests/unit/extraction/test_offset_translator.py -v` Verify TestOffsetVerificationIntegration class tests alignment |
| TEST-10 | GPU cold start performance | test_gpu_cold_start.py | Run: `pytest tests/unit/ocr/test_gpu_cold_start.py -v` |
| TEST-11 | Coverage >= 80% | pytest --cov | Verify coverage output shows >= 80% |
| TEST-12 | mypy strict mode | mypy src/ | Verify "Success: no issues found" message |

**Step 4: Create verification checklist output**

For each requirement, explicitly run the test file and confirm pass/fail. Document:
- Total tests: X
- Passed: Y
- Failed: 0 (must be 0)
- Coverage: Z%
- mypy errors: 0

If any TEST requirement is NOT covered, document which specific tests are missing.
  </action>
  <verify>
```bash
cd backend && python3 -m pytest --cov=src --cov-fail-under=80 -q && python3 -m mypy src/
```
Both commands must succeed.
  </verify>
  <done>All 12 TEST requirements verified with explicit test file mapping, coverage >= 80%, mypy passes with 0 errors</done>
</task>

</tasks>

<verification>
Final verification commands:

```bash
cd backend && \
  python3 -m mypy src/ && \
  python3 -m pytest --cov=src --cov-report=term --cov-fail-under=80 -q
```

Both must succeed for phase completion.
</verification>

<success_criteria>
- [ ] test_gpu_cold_start.py created and passing
- [ ] All GPU cold start tests pass
- [ ] Full test suite passes with 0 failures
- [ ] Test coverage >= 80%
- [ ] mypy strict mode passes with 0 errors
- [ ] TEST-01: test_few_shot_examples.py verified
- [ ] TEST-02: test_char_offset_verification.py verified (substring matching)
- [ ] TEST-03: test_lightonocr_client.py verified
- [ ] TEST-04: test_scanned_detector.py verified
- [ ] TEST-05: E2E Docling test verified
- [ ] TEST-06: E2E LangExtract test verified
- [ ] TEST-07: test_dual_pipeline.py verified (method selection)
- [ ] TEST-08: test_ocr_router.py verified (auto/force/skip)
- [ ] TEST-09: test_offset_translator.py verified (alignment validation)
- [ ] TEST-10: test_gpu_cold_start.py verified
- [ ] TEST-11: coverage >= 80% verified
- [ ] TEST-12: mypy passes verified
</success_criteria>

<output>
After completion, create `.planning/phases/17-testing-quality/17-03-SUMMARY.md`
</output>
