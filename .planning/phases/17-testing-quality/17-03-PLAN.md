---
phase: 17-testing-quality
plan: 03
type: execute
wave: 2
depends_on: ["17-01", "17-02"]
files_modified:
  - backend/tests/unit/ocr/test_gpu_cold_start.py
  - backend/tests/integration/test_gpu_performance.py
autonomous: true

must_haves:
  truths:
    - "GPU cold start timeout handling is tested"
    - "Circuit breaker fallback on timeout is verified"
    - "Test coverage remains above 80%"
    - "All TEST requirements are satisfied"
  artifacts:
    - path: "backend/tests/unit/ocr/test_gpu_cold_start.py"
      provides: "GPU cold start timeout handling tests"
      exports: ["TestGPUColdStartHandling"]
    - path: "backend/tests/integration/test_gpu_performance.py"
      provides: "GPU service performance tests"
      exports: ["TestGPUServicePerformance"]
  key_links:
    - from: "backend/tests/unit/ocr/test_gpu_cold_start.py"
      to: "backend/src/ocr/lightonocr_client.py"
      via: "timeout configuration"
      pattern: "timeout"
    - from: "backend/tests/integration/test_gpu_performance.py"
      to: "backend/src/ocr/ocr_router.py"
      via: "circuit breaker fallback"
      pattern: "CircuitBreakerError"
---

<objective>
Add GPU cold start performance tests and verify all TEST requirements are satisfied

Purpose: TEST-10 requires GPU service cold start performance tests. This plan adds tests for timeout handling during cold starts and verifies the complete test suite satisfies all Phase 17 requirements.

Output:
- GPU cold start timeout handling tests
- Circuit breaker fallback verification tests
- Final coverage and compliance verification
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@backend/tests/unit/ocr/test_lightonocr_client.py
@backend/tests/unit/ocr/test_ocr_router.py
@backend/src/ocr/lightonocr_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create GPU cold start timeout tests</name>
  <files>backend/tests/unit/ocr/test_gpu_cold_start.py</files>
  <action>
Create tests for GPU cold start timeout handling (TEST-10).

Create `backend/tests/unit/ocr/test_gpu_cold_start.py`:

```python
"""TEST-10: GPU service cold start performance tests.

These tests verify that the system handles GPU cold starts gracefully:
1. Timeout configuration is respected
2. Timeouts trigger appropriate error handling
3. Circuit breaker opens after repeated timeouts
4. Fallback to Docling OCR works when GPU is slow

Cold start context:
- GPU Cloud Run services have 2-5 minute cold start time
- Default timeout is 120s to accommodate this
- Circuit breaker (fail_max=3) prevents repeated slow failures

Requirements: TEST-10, LOCR-08
"""

import pytest
from datetime import datetime, timezone
from unittest.mock import AsyncMock, MagicMock, patch
import httpx

from src.ocr.lightonocr_client import LightOnOCRClient, LightOnOCRError


class TestGPUColdStartHandling:
    """TEST-10: Tests for GPU cold start timeout handling."""

    @pytest.fixture
    def mock_id_token(self):
        """Mock Google auth token."""
        with patch("src.ocr.lightonocr_client.id_token.fetch_id_token") as mock:
            mock.return_value = "mock-token"
            yield mock

    @pytest.mark.asyncio
    async def test_timeout_configuration(self, mock_id_token):
        """Client respects configured timeout."""
        # Default timeout is 120s for cold start tolerance
        client = LightOnOCRClient(
            service_url="https://test.run.app",
            timeout=120.0,
        )
        assert client.timeout == 120.0

    @pytest.mark.asyncio
    async def test_custom_timeout_accepted(self, mock_id_token):
        """Client accepts custom timeout for different scenarios."""
        client = LightOnOCRClient(
            service_url="https://test.run.app",
            timeout=300.0,  # 5 minutes for very cold starts
        )
        assert client.timeout == 300.0

    @pytest.mark.asyncio
    async def test_timeout_raises_lightonocr_error(self, mock_id_token):
        """Timeout during request raises LightOnOCRError."""
        client = LightOnOCRClient(
            service_url="https://test.run.app",
            timeout=5.0,  # Short timeout for test
        )

        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.__aenter__.return_value = mock_client
            mock_client.__aexit__.return_value = None
            mock_client.post.side_effect = httpx.TimeoutException(
                "Request timed out (cold start)"
            )
            mock_client_class.return_value = mock_client

            with pytest.raises(LightOnOCRError) as exc_info:
                await client.extract_text(b"\x89PNG...")

            assert "timed out" in str(exc_info.value).lower()

    @pytest.mark.asyncio
    async def test_connect_timeout_handled(self, mock_id_token):
        """Connection timeout (service unreachable) handled gracefully."""
        client = LightOnOCRClient(
            service_url="https://test.run.app",
            timeout=5.0,
        )

        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.__aenter__.return_value = mock_client
            mock_client.__aexit__.return_value = None
            mock_client.post.side_effect = httpx.ConnectTimeout(
                "Failed to connect to GPU service"
            )
            mock_client_class.return_value = mock_client

            with pytest.raises(LightOnOCRError) as exc_info:
                await client.extract_text(b"\x89PNG...")

            # Should provide useful error message
            assert "connect" in str(exc_info.value).lower() or "timed out" in str(exc_info.value).lower()

    @pytest.mark.asyncio
    async def test_read_timeout_handled(self, mock_id_token):
        """Read timeout (response slow) handled gracefully."""
        client = LightOnOCRClient(
            service_url="https://test.run.app",
            timeout=5.0,
        )

        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.__aenter__.return_value = mock_client
            mock_client.__aexit__.return_value = None
            mock_client.post.side_effect = httpx.ReadTimeout(
                "Server response timeout"
            )
            mock_client_class.return_value = mock_client

            with pytest.raises(LightOnOCRError):
                await client.extract_text(b"\x89PNG...")


class TestGPUHealthCheckTimeout:
    """Tests for health check during cold start."""

    @pytest.fixture
    def mock_id_token(self):
        with patch("src.ocr.lightonocr_client.id_token.fetch_id_token") as mock:
            mock.return_value = "mock-token"
            yield mock

    @pytest.mark.asyncio
    async def test_health_check_timeout_returns_false(self, mock_id_token):
        """Health check returns False on timeout (service cold)."""
        client = LightOnOCRClient(
            service_url="https://test.run.app",
            timeout=5.0,
        )

        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.__aenter__.return_value = mock_client
            mock_client.__aexit__.return_value = None
            mock_client.get.side_effect = httpx.TimeoutException("Health check timeout")
            mock_client_class.return_value = mock_client

            result = await client.health_check()
            assert result is False

    @pytest.mark.asyncio
    async def test_health_check_success_returns_true(self, mock_id_token):
        """Health check returns True when service responds."""
        client = LightOnOCRClient(
            service_url="https://test.run.app",
            timeout=5.0,
        )

        with patch("httpx.AsyncClient") as mock_client_class:
            mock_client = AsyncMock()
            mock_client.__aenter__.return_value = mock_client
            mock_client.__aexit__.return_value = None
            mock_client.get.return_value = MagicMock(status_code=200)
            mock_client_class.return_value = mock_client

            result = await client.health_check()
            assert result is True


class TestCircuitBreakerColdStartIntegration:
    """Tests for circuit breaker behavior during cold starts."""

    @pytest.fixture
    def mock_components(self):
        """Create mocked OCR router components."""
        from src.ocr.scanned_detector import ScannedDocumentDetector, DetectionResult

        gpu = MagicMock()
        gpu.health_check = AsyncMock(return_value=False)  # GPU cold/unhealthy

        docling = MagicMock()
        docling.enable_tables = True
        docling.max_pages = 100

        detector = MagicMock(spec=ScannedDocumentDetector)
        detector.detect.return_value = DetectionResult(
            needs_ocr=True,
            scanned_pages=[0],
            total_pages=1,
            scanned_ratio=1.0,
        )

        return gpu, docling, detector

    @pytest.mark.asyncio
    async def test_cold_gpu_triggers_docling_fallback(self, mock_components):
        """When GPU is cold (health check fails), fallback to Docling."""
        from src.ocr.ocr_router import OCRRouter

        gpu, docling, detector = mock_components

        router = OCRRouter(
            gpu_client=gpu,
            docling_processor=docling,
            detector=detector,
        )

        with patch("src.ocr.ocr_router.DoclingProcessor") as MockProcessor:
            mock_instance = MagicMock()
            mock_instance.process_bytes.return_value = MagicMock(text="Fallback OCR")
            MockProcessor.return_value = mock_instance

            result = await router.process(b"pdf bytes", "test.pdf", mode="auto")

            # Should use Docling fallback
            assert result.ocr_method == "docling"

    @pytest.mark.asyncio
    async def test_repeated_timeouts_open_circuit_breaker(self, mock_components):
        """Repeated GPU timeouts should open circuit breaker."""
        from src.ocr.ocr_router import OCRRouter, _gpu_ocr_breaker

        gpu, docling, detector = mock_components
        # Simulate timeout on health check
        gpu.health_check.side_effect = httpx.TimeoutException("GPU cold")

        router = OCRRouter(
            gpu_client=gpu,
            docling_processor=docling,
            detector=detector,
        )

        # Circuit breaker should track failures
        # After fail_max (3) failures, breaker opens
        # Note: This is tested at unit level in test_ocr_router.py
        # Here we verify the integration pattern
        assert _gpu_ocr_breaker.fail_max == 3
        assert _gpu_ocr_breaker.timeout_duration.total_seconds() == 60
```

The tests verify cold start handling without actually waiting for real cold starts.
  </action>
  <verify>
```bash
cd backend && python3 -m pytest tests/unit/ocr/test_gpu_cold_start.py -v 2>&1 | grep -E "passed|failed|PASSED|FAILED"
```
All tests should pass.
  </verify>
  <done>GPU cold start timeout tests pass</done>
</task>

<task type="auto">
  <name>Task 2: Final test suite verification</name>
  <files>backend/tests/</files>
  <action>
Run complete test suite and verify all TEST requirements are satisfied.

**Step 1:** Run full test suite with coverage:

```bash
cd backend && python3 -m pytest --cov=src --cov-report=term-missing --cov-fail-under=80 -v 2>&1 | tee /tmp/test_results.txt
```

**Step 2:** Verify mypy compliance:

```bash
cd backend && python3 -m mypy src/ 2>&1 | tee /tmp/mypy_results.txt
```

**Step 3:** Create requirements checklist verification:

| Requirement | Test File | Status |
|-------------|-----------|--------|
| TEST-01 | test_few_shot_examples.py | Verify exists and passes |
| TEST-02 | test_char_offset_verification.py | Already exists |
| TEST-03 | test_lightonocr_client.py | Already exists |
| TEST-04 | test_scanned_detector.py | Already exists |
| TEST-05 | test_e2e_langextract.py (docling test) | Verify exists |
| TEST-06 | test_e2e_langextract.py | Verify exists |
| TEST-07 | test_dual_pipeline.py | Already exists |
| TEST-08 | test_ocr_router.py | Already exists |
| TEST-09 | test_char_offset_verification.py | Already exists |
| TEST-10 | test_gpu_cold_start.py | Verify exists |
| TEST-11 | Coverage >= 80% | Verify metric |
| TEST-12 | mypy passes | Verify metric |

**Step 4:** Document final counts:

- Total tests: X
- Passed: Y
- Failed: 0
- Coverage: Z%
- mypy errors: 0

If any issues, fix them before completing.
  </action>
  <verify>
```bash
cd backend && python3 -m pytest --cov=src --cov-fail-under=80 -q && python3 -m mypy src/
```
Both commands must succeed.
  </verify>
  <done>All tests pass, coverage >= 80%, mypy passes with 0 errors</done>
</task>

</tasks>

<verification>
Final verification commands:

```bash
cd backend && \
  python3 -m mypy src/ && \
  python3 -m pytest --cov=src --cov-report=term --cov-fail-under=80 -q
```

Both must succeed for phase completion.
</verification>

<success_criteria>
- [ ] test_gpu_cold_start.py created and passing
- [ ] All GPU cold start tests pass
- [ ] Full test suite passes with 0 failures
- [ ] Test coverage >= 80%
- [ ] mypy strict mode passes with 0 errors
- [ ] All TEST-01 through TEST-12 requirements satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/17-testing-quality/17-03-SUMMARY.md`
</output>
