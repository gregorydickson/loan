---
phase: 07-documentation-testing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docs/SYSTEM_DESIGN.md
autonomous: true

must_haves:
  truths:
    - "SYSTEM_DESIGN.md explains the complete system architecture"
    - "Mermaid diagrams visualize component interactions and data flow"
    - "Document extraction pipeline is fully described with each stage"
    - "AI/LLM integration strategy explains model selection and prompt engineering"
    - "Scaling analysis provides projections for 10x and 100x volume"
  artifacts:
    - path: "docs/SYSTEM_DESIGN.md"
      provides: "Comprehensive system design documentation"
      min_lines: 400
      contains: ["## Architecture Overview", "## Data Pipeline", "## AI/LLM Integration", "## Scaling Analysis"]
  key_links:
    - from: "docs/SYSTEM_DESIGN.md"
      to: "Backend source code patterns"
      via: "References to actual implementation"
      pattern: "src/.*\\.py|backend/"
---

<objective>
Create comprehensive system design documentation for the loan document extraction system.

Purpose: Provide future maintainers and stakeholders with a complete understanding of the system architecture, data flow, and design rationale.

Output: docs/SYSTEM_DESIGN.md containing architecture overview, component diagrams (Mermaid), data pipeline description, AI/LLM integration strategy, and scaling analysis.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/07-documentation-testing/07-CONTEXT.md
@.planning/phases/07-documentation-testing/07-RESEARCH.md

# Reference existing code for accurate documentation
@backend/src/main.py
@backend/src/extraction/extractor.py
@backend/src/extraction/llm_client.py
@backend/src/ingestion/docling_processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Architecture Overview and Component Diagrams</name>
  <files>docs/SYSTEM_DESIGN.md</files>
  <action>
Create docs/SYSTEM_DESIGN.md with the following structure:

**1. Architecture Overview Section:**
- Executive summary (2-3 paragraphs) explaining the system's purpose and core value proposition
- High-level component diagram using Mermaid flowchart showing:
  - Frontend (Next.js)
  - Backend (FastAPI)
  - Document Processing (Docling)
  - LLM Extraction (Gemini)
  - Storage (PostgreSQL, GCS)
  - Infrastructure (Cloud Run, Cloud Tasks)
- Technology stack table listing each component with version and purpose

**2. Component Interaction Diagram:**
- Mermaid sequence diagram showing the document upload -> extraction -> storage flow
- Include all services involved: Frontend, API, Docling, Gemini, PostgreSQL, GCS

**3. Data Flow Section:**
- Describe how data moves through the system
- Include entity relationship context (Document -> Borrower -> IncomeRecord -> SourceReference)
- Explain the source attribution chain (how every extracted field traces back to source document/page)

**Tone:** Write conversationally as if explaining to a colleague. Clear and direct.

**Source:** Leverage decisions from .planning/STATE.md for accuracy.
  </action>
  <verify>
File exists and contains:
- `## Architecture Overview` section with Mermaid diagram
- `## Component Interaction` section with sequence diagram
- `## Data Flow` section
- At least 2 Mermaid diagrams
  </verify>
  <done>
Architecture overview, component diagrams, and data flow documented with Mermaid visualizations
  </done>
</task>

<task type="auto">
  <name>Task 2: Document Pipeline and AI/LLM Integration</name>
  <files>docs/SYSTEM_DESIGN.md</files>
  <action>
Extend SYSTEM_DESIGN.md with the following sections:

**4. Document Extraction Pipeline Section:**
- Pipeline flowchart (Mermaid) showing: Upload -> Hash Check -> GCS Store -> Docling Parse -> Complexity Assessment -> LLM Extract -> Validate -> Deduplicate -> Consistency Check -> Store
- Describe each pipeline stage:
  - Document ingestion (Docling: PDF, DOCX, images with OCR)
  - Text extraction with page boundary preservation
  - Complexity classification (triggers Flash vs Pro model selection)
  - Chunking strategy (4000 tokens, 200 overlap)
  - LLM extraction (structured output with Gemini)
  - Validation layer (SSN, phone, zip formats)
  - Deduplication (SSN > account > fuzzy name matching)
  - Consistency validation (address conflicts, income anomalies)

**5. AI/LLM Integration Strategy Section:**
- Model selection rationale (Gemini Flash for speed, Pro for accuracy on complex docs)
- Dynamic model selection based on complexity classifier
- Prompt engineering approach (Pydantic schema for structured output)
- Confidence scoring formula explanation
- Error recovery strategy (retry with exponential backoff, chunk size reduction on timeout)
- Cost analysis table: Flash vs Pro tokens, estimated cost per document

**6. Error Handling Section:**
- How each error type is handled (rate limits, timeouts, validation failures)
- Graceful degradation patterns
- Status tracking (PENDING, PROCESSING, COMPLETED, FAILED)

**Source:** Reference backend/src/extraction/ modules for implementation details.
  </action>
  <verify>
File contains:
- `## Document Extraction Pipeline` with pipeline flowchart
- `## AI/LLM Integration` section
- `## Error Handling` section
- Cost analysis table for Flash vs Pro
  </verify>
  <done>
Pipeline, AI/LLM integration, and error handling fully documented with implementation-accurate details
  </done>
</task>

<task type="auto">
  <name>Task 3: Add Scaling Analysis and Complete Documentation</name>
  <files>docs/SYSTEM_DESIGN.md</files>
  <action>
Complete SYSTEM_DESIGN.md with:

**7. Scaling Analysis Section:**
- Current architecture bottlenecks
- 10x scale projections:
  - Documents per day estimates
  - Database query optimization (selectinload patterns already in place)
  - GCS storage costs
  - LLM token costs
- 100x scale projections:
  - Cloud Run auto-scaling behavior (0-10 backend, 0-5 frontend)
  - Cloud Tasks queue rate limits (10/s, 5 concurrent)
  - Database connection pooling
  - Potential optimizations: batch processing, caching, CDN
- Mermaid chart or table showing resource projections

**8. Security Considerations Section:**
- Private VPC for Cloud SQL (no public IP)
- Secret Manager for credentials
- Non-root containers
- CORS configuration

**9. Deployment Architecture Section:**
- Terraform-managed infrastructure
- Cloud Run services with direct VPC egress
- Deployment script workflow

**Final checks:**
- Ensure all Mermaid diagrams render correctly (test syntax)
- Cross-reference with STATE.md decisions for accuracy
- Total document should be 400+ lines providing comprehensive coverage
  </action>
  <verify>
```bash
# Check file exists and has expected sections
grep -c "## Scaling Analysis" docs/SYSTEM_DESIGN.md && \
grep -c "## Security Considerations" docs/SYSTEM_DESIGN.md && \
grep -c "## Deployment Architecture" docs/SYSTEM_DESIGN.md && \
wc -l docs/SYSTEM_DESIGN.md
```
File should have 400+ lines and all major sections present.
  </verify>
  <done>
Complete SYSTEM_DESIGN.md with architecture, pipeline, AI strategy, scaling analysis, security, and deployment sections
  </done>
</task>

</tasks>

<verification>
1. docs/SYSTEM_DESIGN.md exists with 400+ lines
2. Contains all required sections: Architecture Overview, Data Pipeline, AI/LLM Integration, Scaling Analysis
3. Contains at least 3 Mermaid diagrams (architecture, sequence, pipeline)
4. References actual implementation details from codebase
5. Covers requirements DOCS-01 through DOCS-19
</verification>

<success_criteria>
- SYSTEM_DESIGN.md contains architecture overview with Mermaid component diagram
- SYSTEM_DESIGN.md contains pipeline flowchart with all processing stages
- SYSTEM_DESIGN.md explains AI/LLM integration including model selection and prompt engineering
- SYSTEM_DESIGN.md includes scaling projections for 10x and 100x volume
- Document is comprehensive (400+ lines) and accurate to implementation
</success_criteria>

<output>
After completion, create `.planning/phases/07-documentation-testing/07-01-SUMMARY.md`
</output>
