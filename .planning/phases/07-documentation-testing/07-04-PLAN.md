---
phase: 07-documentation-testing
plan: 04
type: execute
wave: 2
depends_on: []
files_modified:
  - backend/tests/unit/test_docling_processor.py
  - backend/tests/unit/test_document_service.py
  - backend/tests/unit/test_gcs_client.py
  - backend/tests/unit/test_models.py
  - backend/tests/unit/test_borrower_repository.py
  - backend/tests/unit/test_borrower_routes.py
  - backend/tests/integration/test_documents_api.py
  - backend/tests/extraction/test_extractor.py
  - backend/tests/extraction/test_llm_client.py
  - backend/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "pytest coverage report shows >80% backend coverage"
    - "All test files cover error paths and edge cases"
    - "Integration tests validate upload-to-query flow"
    - "pytest-xdist installed for parallel test execution"
  artifacts:
    - path: "backend/tests/unit/test_docling_processor.py"
      provides: "Comprehensive Docling processor tests"
      contains: ["test_process_pdf", "test_process_docx", "test_process_image"]
    - path: "backend/tests/integration/test_e2e_flow.py"
      provides: "End-to-end flow test"
      contains: ["test_upload_to_extraction_flow"]
  key_links:
    - from: "backend/tests/"
      to: "backend/src/"
      via: "Tests cover source modules"
      pattern: "from src\\."
---

<objective>
Expand test coverage to achieve >80% backend coverage by adding targeted tests for uncovered modules and error paths.

Purpose: Ensure production-quality test suite that validates all critical functionality, error handling, and edge cases.

Output: Extended test files with comprehensive coverage, pytest-xdist for parallel execution, and validated 80%+ coverage.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/07-documentation-testing/07-CONTEXT.md
@.planning/phases/07-documentation-testing/07-RESEARCH.md

# Existing test infrastructure
@backend/tests/conftest.py
@backend/tests/integration/conftest.py
@backend/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Establish Coverage Baseline and Add pytest-xdist</name>
  <files>backend/pyproject.toml</files>
  <action>
First, establish the current coverage baseline by running existing tests. Then add pytest-xdist for parallel execution.

**Step 1: Run coverage baseline**
```bash
cd backend
pytest --cov=src --cov-report=term-missing --cov-report=html
```
Note which modules have low coverage for targeting.

**Step 2: Update pyproject.toml to add pytest-xdist:**

Add to `[project.optional-dependencies]` dev section:
```toml
"pytest-xdist>=3.0.0",
```

**Step 3: Add coverage configuration:**

Add to pyproject.toml:
```toml
[tool.coverage.run]
source = ["src"]
branch = true
omit = ["*/__init__.py"]

[tool.coverage.report]
exclude_also = [
    "def __repr__",
    "if TYPE_CHECKING:",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "@abstractmethod",
]
fail_under = 80
show_missing = true
```

**Step 4: Install updated dependencies:**
```bash
pip install -e ".[dev]"
```
  </action>
  <verify>
```bash
cd backend
pip show pytest-xdist  # Should show installed
grep "fail_under = 80" pyproject.toml  # Coverage threshold set
```
  </verify>
  <done>
Coverage baseline established, pytest-xdist added, coverage fail threshold configured at 80%
  </done>
</task>

<task type="auto">
  <name>Task 2: Expand Unit Tests for Core Modules</name>
  <files>
    backend/tests/unit/test_docling_processor.py
    backend/tests/unit/test_document_service.py
    backend/tests/unit/test_gcs_client.py
    backend/tests/unit/test_models.py
  </files>
  <action>
Expand existing unit tests to cover error paths and edge cases.

**test_docling_processor.py - Add tests for:**
- PDF processing with various page counts (TEST-01)
- DOCX processing (TEST-01)
- Image processing with OCR (TEST-01)
- Error handling for corrupted files
- Empty document handling
- Large document handling (memory considerations)

Pattern:
```python
@pytest.mark.asyncio
async def test_process_pdf_extracts_text():
    """Test PDF processing extracts text with page boundaries."""
    ...

@pytest.mark.asyncio
async def test_process_pdf_handles_corrupted_file():
    """Test graceful handling of corrupted PDF."""
    ...

@pytest.mark.asyncio
async def test_process_docx_preserves_layout():
    """Test DOCX processing preserves layout information."""
    ...

@pytest.mark.asyncio
async def test_process_image_with_ocr():
    """Test image processing uses OCR for text extraction."""
    ...
```

**test_document_service.py - Add tests for (TEST-02):**
- Upload with mock GCS and Docling
- Duplicate detection (same file hash)
- Error handling for GCS failures
- Error handling for Docling failures
- Status transitions (PENDING -> PROCESSING -> COMPLETED/FAILED)

**test_gcs_client.py - Add tests for (TEST-03):**
- Upload returns gs:// URI
- Download retrieves file content
- Exists check returns correct boolean
- Signed URL generation
- Error handling for missing bucket

**test_models.py - Add tests for (TEST-10):**
- Document model relationships
- Borrower model relationships
- IncomeRecord model relationships
- SourceReference model relationships
- All models serialize to JSON correctly

Use MagicMock(spec=Class) pattern for type-safe mocks as documented in research.
  </action>
  <verify>
```bash
cd backend
pytest tests/unit/test_docling_processor.py tests/unit/test_document_service.py tests/unit/test_gcs_client.py tests/unit/test_models.py -v
```
All new tests pass.
  </verify>
  <done>
Unit tests expanded for Docling processor, document service, GCS client, and database models with error path coverage
  </done>
</task>

<task type="auto">
  <name>Task 3: Expand Extraction and API Tests, Add E2E Test</name>
  <files>
    backend/tests/extraction/test_extractor.py
    backend/tests/extraction/test_llm_client.py
    backend/tests/unit/test_borrower_repository.py
    backend/tests/unit/test_borrower_routes.py
    backend/tests/integration/test_e2e_flow.py
  </files>
  <action>
Add comprehensive tests for extraction logic and create E2E integration test.

**test_extractor.py - Add tests for (TEST-07):**
- Chunking large documents correctly
- Deduplication by SSN
- Deduplication by account number
- Deduplication by fuzzy name match
- Aggregating results from multiple chunks
- Source attribution tracking

**test_llm_client.py - Add tests for (TEST-05):**
- Successful extraction with mock response
- Retry on rate limit (429)
- Retry on server error (5xx)
- Timeout handling
- None response handling (max_output_tokens exceeded)
- Token usage tracking

Pattern for mock Gemini response:
```python
def create_mock_response(text: str, prompt_tokens: int = 100, candidate_tokens: int = 50):
    """Create mock Gemini response with usage metadata."""
    mock = MagicMock()
    mock.text = text
    mock.usage_metadata = MagicMock()
    mock.usage_metadata.prompt_token_count = prompt_tokens
    mock.usage_metadata.candidates_token_count = candidate_tokens
    return mock
```

**test_borrower_repository.py - Add tests for (TEST-11):**
- Create borrower with related entities
- Get borrower by ID with relationships loaded
- Search by name
- Search by account number
- Pagination
- Empty results handling

**test_borrower_routes.py - Add tests for (TEST-12, TEST-13):**
- GET /api/borrowers list endpoint
- GET /api/borrowers/{id} detail endpoint
- GET /api/borrowers/search with name query
- GET /api/borrowers/search with account query
- 404 for non-existent borrower
- Proper HTTP status codes

**NEW: test_e2e_flow.py - Create E2E integration test (TEST-14):**
```python
"""End-to-end integration test for document upload to borrower query flow."""
import pytest
from httpx import AsyncClient

@pytest.mark.integration
@pytest.mark.asyncio
async def test_upload_to_extraction_flow(client: AsyncClient, mock_gemini_client):
    """Test complete flow: upload document -> process -> extract borrowers -> query."""
    # 1. Upload document
    response = await client.post(
        "/api/documents",
        files={"file": ("test.pdf", b"PDF content", "application/pdf")}
    )
    assert response.status_code == 201
    doc_id = response.json()["id"]

    # 2. Verify processing completed
    status_response = await client.get(f"/api/documents/{doc_id}/status")
    assert status_response.json()["status"] in ["completed", "failed"]

    # 3. Query borrowers (if extraction succeeded)
    if status_response.json()["status"] == "completed":
        borrowers_response = await client.get("/api/borrowers")
        assert borrowers_response.status_code == 200
```

Mark integration tests with `@pytest.mark.integration` for selective running.
  </action>
  <verify>
```bash
cd backend
pytest tests/extraction/ tests/unit/test_borrower_repository.py tests/unit/test_borrower_routes.py tests/integration/test_e2e_flow.py -v
pytest --cov=src --cov-report=term-missing --cov-fail-under=80
```
All tests pass and coverage is >= 80%.
  </verify>
  <done>
Extraction tests, API route tests, and E2E integration test added. Coverage validated at 80%+.
  </done>
</task>

</tasks>

<verification>
1. pytest-xdist installed and available
2. Coverage configuration in pyproject.toml with fail_under=80
3. All test files updated with expanded coverage
4. `pytest --cov=src --cov-fail-under=80` passes
5. Tests cover requirements TEST-01 through TEST-14
</verification>

<success_criteria>
- pytest coverage report shows >80% backend coverage
- All unit tests for Docling, document service, GCS client, models pass
- All extraction tests pass (LLM client, extractor, validation)
- All API route tests pass (documents, borrowers)
- E2E integration test validates upload-to-query flow
- Tests can run in parallel with pytest-xdist
</success_criteria>

<output>
After completion, create `.planning/phases/07-documentation-testing/07-04-SUMMARY.md`
</output>
