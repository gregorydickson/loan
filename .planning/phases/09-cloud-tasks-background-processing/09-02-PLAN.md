---
phase: 09-cloud-tasks-background-processing
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - backend/src/api/tasks.py
  - backend/src/api/__init__.py
  - backend/src/main.py
  - backend/src/api/dependencies.py
autonomous: true

must_haves:
  truths:
    - "POST /api/tasks/process-document endpoint receives Cloud Tasks callbacks"
    - "Task handler updates document status to PROCESSING at start"
    - "Task handler runs extraction and updates status to COMPLETED or FAILED"
    - "Retry count checked to set FAILED only on final retry"
  artifacts:
    - path: "backend/src/api/tasks.py"
      provides: "Cloud Tasks handler endpoint"
      exports: ["router", "ProcessDocumentRequest"]
    - path: "backend/src/main.py"
      provides: "Task router registration"
      contains: "tasks_router"
  key_links:
    - from: "backend/src/api/tasks.py"
      to: "src.ingestion.document_service"
      via: "extraction logic import"
      pattern: "DocumentService"
    - from: "backend/src/main.py"
      to: "backend/src/api/tasks.py"
      via: "router include"
      pattern: "include_router.*tasks_router"
---

<objective>
Create Cloud Tasks handler endpoint for async document processing

Purpose: Build the task handler that receives callbacks from Cloud Tasks, runs extraction, and updates document status. This is the worker side of the async processing flow.

Output: POST /api/tasks/process-document endpoint that processes documents asynchronously with proper retry handling.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-cloud-tasks-background-processing/09-RESEARCH.md
@.planning/phases/09-cloud-tasks-background-processing/09-01-SUMMARY.md

# Existing code
@backend/src/api/documents.py
@backend/src/api/dependencies.py
@backend/src/main.py
@backend/src/ingestion/document_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create task handler endpoint module</name>
  <files>backend/src/api/tasks.py</files>
  <action>
Create backend/src/api/tasks.py with the Cloud Tasks handler:

```python
"""Cloud Tasks handler endpoints.

Receives callbacks from Cloud Tasks for async document processing.
Cloud Run validates OIDC tokens automatically via IAM.
"""

from __future__ import annotations

import logging
from uuid import UUID

from fastapi import APIRouter, HTTPException, Request, status
from pydantic import BaseModel, Field

from src.api.dependencies import (
    BorrowerExtractorDep,
    BorrowerRepoDep,
    DoclingProcessorDep,
    DocumentRepoDep,
    GCSClientDep,
)
from src.storage.models import DocumentStatus

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/tasks", tags=["tasks"])

# Maximum retry attempts (0-indexed, so 4 = 5 total attempts)
MAX_RETRY_COUNT = 4


class ProcessDocumentRequest(BaseModel):
    """Request payload from Cloud Tasks."""

    document_id: UUID = Field(..., description="Document UUID to process")
    filename: str = Field(..., description="Original filename")


class ProcessDocumentResponse(BaseModel):
    """Response for task processing."""

    status: str = Field(..., description="Processing result: completed, failed, or retrying")
    error: str | None = Field(None, description="Error message if failed")


@router.post(
    "/process-document",
    response_model=ProcessDocumentResponse,
    status_code=status.HTTP_200_OK,
    summary="Process document (Cloud Tasks handler)",
    description="""Handle document processing task from Cloud Tasks.

Cloud Run validates OIDC token automatically via IAM.
Returns 2xx for success, 5xx triggers Cloud Tasks retry.
After max retries, marks document as FAILED and returns 200.""",
)
async def process_document(
    request: Request,
    payload: ProcessDocumentRequest,
    document_repo: DocumentRepoDep,
    docling_processor: DoclingProcessorDep,
    borrower_extractor: BorrowerExtractorDep,
    borrower_repo: BorrowerRepoDep,
    gcs_client: GCSClientDep,
) -> ProcessDocumentResponse:
    """Process a document from Cloud Tasks callback.

    Args:
        request: FastAPI request (for headers)
        payload: Task payload with document_id and filename
        document_repo: Document repository
        docling_processor: Docling processor for text extraction
        borrower_extractor: LLM extractor for borrower data
        borrower_repo: Borrower repository for persistence
        gcs_client: GCS client for file download

    Returns:
        ProcessDocumentResponse with status

    Raises:
        HTTPException 503: On transient errors (triggers retry)
    """
    # Extract Cloud Tasks metadata from headers
    task_name = request.headers.get("X-CloudTasks-TaskName", "unknown")
    retry_count = int(request.headers.get("X-CloudTasks-TaskRetryCount", "0"))

    logger.info(
        "Processing document task: document_id=%s, task=%s, retry=%d",
        payload.document_id,
        task_name,
        retry_count,
    )

    # Get document from database
    document = await document_repo.get_by_id(payload.document_id)
    if document is None:
        logger.error("Document not found: %s", payload.document_id)
        # Return 200 - no point retrying if document doesn't exist
        return ProcessDocumentResponse(
            status="failed",
            error=f"Document not found: {payload.document_id}",
        )

    # Skip if already processed (idempotency)
    if document.status in (DocumentStatus.COMPLETED, DocumentStatus.FAILED):
        logger.info(
            "Document %s already processed (status=%s), skipping",
            payload.document_id,
            document.status.value,
        )
        return ProcessDocumentResponse(status=document.status.value)

    # Update status to PROCESSING
    await document_repo.update_status(
        payload.document_id,
        DocumentStatus.PROCESSING,
    )

    try:
        # Download document content from GCS
        if not document.gcs_uri:
            raise ValueError("Document has no GCS URI")

        # Extract path from gs:// URI
        gcs_path = document.gcs_uri.replace(f"gs://{gcs_client.bucket_name}/", "")
        content = gcs_client.download(gcs_path)

        # Process with Docling
        from src.ingestion.docling_processor import DocumentProcessingError

        result = docling_processor.process_bytes(content, payload.filename)

        # Update page count
        await document_repo.update_status(
            payload.document_id,
            DocumentStatus.PROCESSING,  # Still processing during extraction
            page_count=result.page_count,
        )

        # Extract borrowers
        extraction_result = borrower_extractor.extract(
            document=result,
            document_id=payload.document_id,
            document_name=payload.filename,
        )

        # Persist extracted borrowers
        from src.ingestion.document_service import DocumentService

        # Create temporary service for persistence helper
        # (This is a bit awkward but avoids duplicating persistence logic)
        temp_service = DocumentService.__new__(DocumentService)
        temp_service.borrower_repository = borrower_repo

        for borrower_record in extraction_result.borrowers:
            try:
                await temp_service._persist_borrower(borrower_record, payload.document_id)
                logger.info(
                    "Persisted borrower '%s' from document %s",
                    borrower_record.name,
                    payload.document_id,
                )
            except Exception as e:
                logger.warning(
                    "Failed to persist borrower '%s': %s",
                    borrower_record.name,
                    str(e),
                )

        # Mark as completed
        await document_repo.update_status(
            payload.document_id,
            DocumentStatus.COMPLETED,
            page_count=result.page_count,
        )

        logger.info(
            "Document %s processed successfully: %d borrowers extracted",
            payload.document_id,
            len(extraction_result.borrowers),
        )

        return ProcessDocumentResponse(status="completed")

    except DocumentProcessingError as e:
        # Docling processing failed - permanent error, don't retry
        error_msg = f"Document processing failed: {e.message}"
        await document_repo.update_status(
            payload.document_id,
            DocumentStatus.FAILED,
            error_message=error_msg,
        )
        logger.error("Docling processing failed for %s: %s", payload.document_id, e.message)
        return ProcessDocumentResponse(status="failed", error=error_msg)

    except Exception as e:
        error_msg = str(e)
        logger.error(
            "Task failed for document %s (retry %d): %s",
            payload.document_id,
            retry_count,
            error_msg,
        )

        # Check if this is the final retry
        if retry_count >= MAX_RETRY_COUNT:
            # Final retry exhausted - mark as failed
            await document_repo.update_status(
                payload.document_id,
                DocumentStatus.FAILED,
                error_message=f"Processing failed after {retry_count + 1} attempts: {error_msg}",
            )
            return ProcessDocumentResponse(
                status="failed",
                error=f"Max retries exhausted: {error_msg}",
            )

        # Return 503 to trigger Cloud Tasks retry
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Processing failed (attempt {retry_count + 1}): {error_msg}",
        ) from e
```

Key design decisions:
- MAX_RETRY_COUNT = 4 (5 total attempts, matching Cloud Tasks queue config)
- Check document status for idempotency (skip if already COMPLETED/FAILED)
- Set PROCESSING immediately at task start
- Return 200 for permanent failures (no retry)
- Return 503 for transient failures (triggers retry)
- Only set FAILED on final retry attempt
- Use existing _persist_borrower logic from DocumentService
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "from src.api.tasks import router, ProcessDocumentRequest; print('Import OK')"`
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -m mypy src/api/tasks.py`
  </verify>
  <done>Task handler endpoint module created with proper retry logic</done>
</task>

<task type="auto">
  <name>Task 2: Register tasks router in main.py</name>
  <files>backend/src/main.py, backend/src/api/__init__.py</files>
  <action>
1. Update backend/src/main.py to include the tasks router:

Add import at top with other router imports:
```python
from src.api.tasks import router as tasks_router
```

Add router registration after existing routers:
```python
app.include_router(tasks_router)
```

2. Update backend/src/api/__init__.py to export the tasks router (if not already exporting anything, add):
```python
from src.api.tasks import router as tasks_router

__all__ = ["tasks_router"]
```

The order should be:
- documents_router
- borrowers_router
- tasks_router
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "from src.main import app; print([r.path for r in app.routes])"`
Verify /api/tasks/process-document appears in routes
  </verify>
  <done>Tasks router is registered in FastAPI app</done>
</task>

<task type="auto">
  <name>Task 3: Add GCS download capability check</name>
  <files>backend/src/storage/gcs_client.py</files>
  <action>
Verify the GCSClient has a download method. If missing, add it:

Check the existing GCSClient class in backend/src/storage/gcs_client.py.

If download method exists, skip this task.

If download method is missing, add:
```python
def download(self, blob_name: str) -> bytes:
    """Download file content from GCS.

    Args:
        blob_name: Path to blob in bucket

    Returns:
        File content as bytes

    Raises:
        google.cloud.exceptions.NotFound: If blob doesn't exist
    """
    blob = self.bucket.blob(blob_name)
    return blob.download_as_bytes()
```

Also ensure bucket_name is accessible as a property:
```python
@property
def bucket_name(self) -> str:
    """Get bucket name."""
    return self._bucket_name  # or however the bucket name is stored
```
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "from src.storage.gcs_client import GCSClient; print(hasattr(GCSClient, 'download'))"`
  </verify>
  <done>GCSClient has download method for retrieving document content</done>
</task>

</tasks>

<verification>
1. POST /api/tasks/process-document endpoint exists and responds
2. Endpoint handles Cloud Tasks headers (X-CloudTasks-TaskRetryCount)
3. Document status transitions: PENDING -> PROCESSING -> COMPLETED/FAILED
4. GCSClient.download method exists
</verification>

<success_criteria>
- `curl -X POST http://localhost:8000/api/tasks/process-document -H "Content-Type: application/json" -d '{"document_id":"...", "filename":"test.pdf"}'` returns 200 or 404 (not 500)
- FastAPI /docs shows /api/tasks/process-document endpoint
- mypy passes on src/api/tasks.py
</success_criteria>

<output>
After completion, create `.planning/phases/09-cloud-tasks-background-processing/09-02-SUMMARY.md`
</output>
