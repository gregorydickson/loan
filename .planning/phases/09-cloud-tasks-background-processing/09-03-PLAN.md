---
phase: 09-cloud-tasks-background-processing
plan: 03
type: execute
wave: 3
depends_on: ["09-01", "09-02"]
files_modified:
  - backend/src/ingestion/document_service.py
  - backend/src/api/dependencies.py
  - backend/src/api/documents.py
autonomous: true

must_haves:
  truths:
    - "DocumentService.upload() queues Cloud Task instead of sync processing"
    - "Upload endpoint returns immediately with PENDING status"
    - "CloudTasksClient is injected via FastAPI dependency"
    - "Local development works without Cloud Tasks (mock client)"
  artifacts:
    - path: "backend/src/ingestion/document_service.py"
      provides: "Async document upload with task queueing"
      contains: "cloud_tasks_client"
    - path: "backend/src/api/dependencies.py"
      provides: "CloudTasksClient dependency injection"
      contains: "get_cloud_tasks_client"
  key_links:
    - from: "backend/src/ingestion/document_service.py"
      to: "src.ingestion.cloud_tasks_client"
      via: "task creation"
      pattern: "create_document_processing_task"
    - from: "backend/src/api/dependencies.py"
      to: "src.ingestion.cloud_tasks_client"
      via: "dependency factory"
      pattern: "CloudTasksClient"
---

<objective>
Wire async task queueing into document upload flow

Purpose: Replace synchronous extraction in DocumentService.upload() with Cloud Task queueing. Upload returns immediately with PENDING status, extraction runs asynchronously via the task handler.

Output: DocumentService that queues tasks and returns immediately, with proper fallback for local development.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-cloud-tasks-background-processing/09-RESEARCH.md
@.planning/phases/09-cloud-tasks-background-processing/09-01-SUMMARY.md
@.planning/phases/09-cloud-tasks-background-processing/09-02-SUMMARY.md

# Current implementation
@backend/src/ingestion/document_service.py
@backend/src/api/dependencies.py
@backend/src/api/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add CloudTasksClient dependency injection</name>
  <files>backend/src/api/dependencies.py</files>
  <action>
Add CloudTasksClient dependency to backend/src/api/dependencies.py:

1. Add import at top:
```python
from src.ingestion.cloud_tasks_client import CloudTasksClient
```

2. Add singleton pattern for CloudTasksClient (after existing singletons):
```python
# CloudTasksClient dependency
_cloud_tasks_client: CloudTasksClient | None = None


def get_cloud_tasks_client() -> CloudTasksClient | None:
    """Get or create CloudTasksClient singleton.

    Returns:
        CloudTasksClient if configured, None for local development.

    Note:
        Returns None if Cloud Tasks settings are not configured.
        This allows running locally without Cloud Tasks.
    """
    global _cloud_tasks_client

    if _cloud_tasks_client is None:
        # Check if Cloud Tasks is configured
        if not settings.gcp_project_id or not settings.cloud_run_service_url:
            # Local development - no Cloud Tasks
            return None

        _cloud_tasks_client = CloudTasksClient(
            project_id=settings.gcp_project_id,
            location=settings.gcp_location,
            queue_id=settings.cloud_tasks_queue,
            service_url=settings.cloud_run_service_url,
            service_account_email=settings.cloud_run_service_account,
        )

    return _cloud_tasks_client


CloudTasksClientDep = Annotated[CloudTasksClient | None, Depends(get_cloud_tasks_client)]
```

3. Update get_document_service to accept optional CloudTasksClient:
```python
def get_document_service(
    repository: DocumentRepoDep,
    gcs_client: GCSClientDep,
    docling_processor: DoclingProcessorDep,
    borrower_extractor: BorrowerExtractorDep,
    borrower_repository: BorrowerRepoDep,
    cloud_tasks_client: CloudTasksClientDep,
) -> DocumentService:
    """Get document service with all dependencies."""
    return DocumentService(
        repository=repository,
        gcs_client=gcs_client,
        docling_processor=docling_processor,
        borrower_extractor=borrower_extractor,
        borrower_repository=borrower_repository,
        cloud_tasks_client=cloud_tasks_client,
    )
```

4. Add to __all__:
```python
"CloudTasksClientDep",
"get_cloud_tasks_client",
```
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "from src.api.dependencies import get_cloud_tasks_client; print(get_cloud_tasks_client())"`
Should print None (local dev without config)
  </verify>
  <done>CloudTasksClient dependency is available via FastAPI DI</done>
</task>

<task type="auto">
  <name>Task 2: Modify DocumentService to queue tasks</name>
  <files>backend/src/ingestion/document_service.py</files>
  <action>
Update backend/src/ingestion/document_service.py to use async task queueing:

1. Add import at top:
```python
from src.ingestion.cloud_tasks_client import CloudTasksClient
```

2. Update __init__ to accept optional CloudTasksClient:
```python
def __init__(
    self,
    repository: DocumentRepository,
    gcs_client: GCSClient,
    docling_processor: DoclingProcessor,
    borrower_extractor: BorrowerExtractor,
    borrower_repository: BorrowerRepository,
    cloud_tasks_client: CloudTasksClient | None = None,
) -> None:
    """Initialize DocumentService.

    Args:
        repository: DocumentRepository for document database operations
        gcs_client: GCSClient for file storage
        docling_processor: DoclingProcessor for document processing
        borrower_extractor: BorrowerExtractor for LLM-based extraction
        borrower_repository: BorrowerRepository for persisting borrowers
        cloud_tasks_client: CloudTasksClient for async task queueing (None for sync mode)
    """
    self.repository = repository
    self.gcs_client = gcs_client
    self.docling_processor = docling_processor
    self.borrower_extractor = borrower_extractor
    self.borrower_repository = borrower_repository
    self.cloud_tasks_client = cloud_tasks_client
```

3. Replace the upload() method body after GCS upload (step 7 onwards) with:
```python
        # 7. Queue for async processing OR process synchronously
        if self.cloud_tasks_client is not None:
            # Async mode: Queue Cloud Task and return immediately
            try:
                self.cloud_tasks_client.create_document_processing_task(
                    document_id=document_id,
                    filename=filename,
                )
                logger.info(
                    "Queued document %s for async processing",
                    document_id,
                )
            except Exception as e:
                logger.error("Failed to queue task for document %s: %s", document_id, e)
                # Mark as failed if we can't queue
                await self.repository.update_status(
                    document_id,
                    DocumentStatus.FAILED,
                    error_message=f"Failed to queue processing: {e}",
                )
                # Refresh and return the failed document
                refreshed = await self.repository.get_by_id(document_id)
                if refreshed is not None:
                    document = refreshed

            # Return immediately with PENDING status
            return document

        # Sync mode: Process immediately (for local development)
        # ... keep existing sync processing code (steps 7-9 from original) ...
```

4. Keep the existing synchronous processing code but wrap it in an else block for when cloud_tasks_client is None. This maintains backward compatibility for local development.

The key changes:
- If cloud_tasks_client is provided: queue task, return PENDING immediately
- If cloud_tasks_client is None: run existing sync processing (Docling + extraction)
- Update docstring to reflect new async behavior

5. Update class docstring to reflect async vs sync modes:
```python
"""Orchestrates document upload, processing, and extraction.

Upload workflow (async mode - Cloud Tasks configured):
1. Validate file (type, size)
2. Compute file hash (SHA-256)
3. Check for duplicate (reject if exists)
4. Create database record (PENDING status)
5. Upload to GCS
6. Queue Cloud Task for processing
7. Return immediately with PENDING status

Upload workflow (sync mode - local development):
1-5. Same as async
6. Process document with Docling
7. Update status to COMPLETED/FAILED
8. Extract borrowers via BorrowerExtractor
9. Persist extracted borrowers with source references
10. Return document with final status
"""
```
  </action>
  <verify>
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -c "from src.ingestion.document_service import DocumentService; print('Import OK')"`
Run: `cd /Users/gregorydickson/stackpoint/loan/backend && python -m mypy src/ingestion/document_service.py`
  </verify>
  <done>DocumentService queues Cloud Tasks when configured, falls back to sync for local dev</done>
</task>

<task type="auto">
  <name>Task 3: Update upload endpoint response messaging</name>
  <files>backend/src/api/documents.py</files>
  <action>
Update the upload_document endpoint in backend/src/api/documents.py to properly handle the PENDING status:

The current code already handles PENDING in the message generation, but update the docstring and description to reflect async behavior:

1. Update endpoint description:
```python
@router.post(
    "/",
    response_model=DocumentUploadResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Upload a document",
    description="""Upload a document for processing. Supports PDF, DOCX, PNG, and JPG files.

In production (Cloud Tasks configured): Returns immediately with status='pending'.
Poll GET /api/documents/{id}/status to check processing progress.

In development (no Cloud Tasks): Processing is synchronous.
Response will include status='completed' or status='failed'.""",
)
```

2. Update DocumentUploadResponse docstring:
```python
class DocumentUploadResponse(BaseModel):
    """Response for document upload.

    In async mode (production): status='pending', poll /status endpoint.
    In sync mode (development): status='completed' or 'failed' immediately.
    """
```

3. Ensure the message handling covers PENDING:
```python
# Generate processing-aware message
if document.status == DocumentStatus.COMPLETED:
    message = f"Document processed successfully with {document.page_count} page(s)"
elif document.status == DocumentStatus.FAILED:
    message = (
        f"Document upload succeeded but processing failed: "
        f"{document.error_message or 'Unknown error'}"
    )
else:
    # PENDING or PROCESSING
    message = "Document uploaded and queued for processing. Poll status endpoint for updates."
```
  </action>
  <verify>
Start server: `cd /Users/gregorydickson/stackpoint/loan/backend && uvicorn src.main:app --reload &`
Check docs: `curl http://localhost:8000/docs` and verify endpoint description mentions async
  </verify>
  <done>Upload endpoint documentation reflects async behavior</done>
</task>

</tasks>

<verification>
1. DocumentService accepts optional cloud_tasks_client parameter
2. With cloud_tasks_client=None, sync processing still works (local dev)
3. Upload endpoint returns appropriate message for PENDING status
4. mypy passes on all modified files
</verification>

<success_criteria>
- Local development (no Cloud Tasks config) still works: upload -> sync extraction -> COMPLETED
- With Cloud Tasks config: upload -> queue task -> return PENDING immediately
- `python -m mypy src/ingestion/document_service.py src/api/dependencies.py src/api/documents.py` passes
- Existing tests still pass: `pytest tests/integration/test_e2e_extraction.py -v`
</success_criteria>

<output>
After completion, create `.planning/phases/09-cloud-tasks-background-processing/09-03-SUMMARY.md`
</output>
