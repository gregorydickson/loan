---
phase: 10-v2-setup-preparation
plan: 04
type: execute
wave: 2
depends_on:
  - "10-01"
files_modified:
  - docs/vllm-validation.md
  - scripts/validate-vllm.sh
autonomous: false

user_setup:
  - service: vLLM/LightOnOCR
    why: "Local GPU validation requires CUDA-capable hardware"
    requirements:
      - item: "NVIDIA GPU with 8GB+ VRAM"
        reason: "LightOnOCR-2-1B model requires GPU memory"
      - item: "CUDA drivers installed"
        reason: "vLLM requires CUDA for GPU acceleration"

must_haves:
  truths:
    - "vLLM installation instructions documented"
    - "LightOnOCR model serving command documented"
    - "Validation script exists for testing"
  artifacts:
    - path: "docs/vllm-validation.md"
      provides: "vLLM validation guide"
      contains: "vllm serve"
    - path: "scripts/validate-vllm.sh"
      provides: "Validation test script"
      contains: "lightonai/LightOnOCR"
  key_links:
    - from: "scripts/validate-vllm.sh"
      to: "vLLM server"
      via: "curl API call"
      pattern: "localhost:8000"
---

<objective>
Create vLLM validation scripts and documentation for LightOnOCR model testing.

Purpose: Validate that vLLM can serve the LightOnOCR-2-1B model before committing to GPU service architecture in Phase 13. Local validation confirms the model works before cloud deployment.
Output: Validation scripts and documentation; user confirms vLLM works locally.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-v2-setup-preparation/10-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create vLLM validation documentation and scripts</name>
  <files>
    docs/vllm-validation.md
    scripts/validate-vllm.sh
  </files>
  <action>
Create docs/vllm-validation.md with complete validation guide:

```markdown
# vLLM LightOnOCR Validation Guide

## Purpose

Validate that vLLM can serve the LightOnOCR-2-1B model locally before cloud deployment.

## Requirements

- NVIDIA GPU with 8GB+ VRAM (L4 in cloud has 24GB)
- CUDA drivers installed
- Python 3.12+

## Installation

```bash
# Create virtual environment
uv venv --python 3.12 --seed
source .venv/bin/activate

# Install vLLM (version 0.11.2+ required for LightOnOCR support)
uv pip install vllm==0.11.2
```

## Serving the Model

Start vLLM server with LightOnOCR:

```bash
vllm serve lightonai/LightOnOCR-2-1B \
    --limit-mm-per-prompt '{"image": 1}' \
    --mm-processor-cache-gb 0 \
    --no-enable-prefix-caching \
    --port 8000
```

**Flags explained:**
- `--limit-mm-per-prompt '{"image": 1}'`: One image per request (OCR use case)
- `--mm-processor-cache-gb 0`: Disable multimodal cache (saves memory)
- `--no-enable-prefix-caching`: Disable prefix caching (not useful for OCR)

## Testing

Run validation script:

```bash
./scripts/validate-vllm.sh
```

Or test manually:

```bash
# Test with a simple image (base64 encoded)
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "lightonai/LightOnOCR-2-1B",
    "messages": [{"role": "user", "content": [
      {"type": "text", "text": "Extract all text from this image"},
      {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
    ]}],
    "max_tokens": 2000
  }'
```

## Expected Output

Successful validation shows:
1. Model loads without CUDA OOM errors
2. Server responds on port 8000
3. API returns extracted text from test image

## Troubleshooting

| Error | Cause | Solution |
|-------|-------|----------|
| CUDA out of memory | Insufficient GPU VRAM | Use GPU with 8GB+ VRAM |
| Model not supported | vLLM version too old | Upgrade to vLLM 0.11.2+ |
| Connection refused | Server not running | Start vLLM serve command |

## Cloud Deployment

Once validated locally, the same model serves on Cloud Run with L4 GPU (Phase 13).
```

Create scripts/validate-vllm.sh:

```bash
#!/bin/bash
# vLLM LightOnOCR Validation Script
# Tests that vLLM server is running and responds correctly

set -euo pipefail

VLLM_HOST="${VLLM_HOST:-localhost}"
VLLM_PORT="${VLLM_PORT:-8000}"
BASE_URL="http://${VLLM_HOST}:${VLLM_PORT}"

echo "Validating vLLM LightOnOCR at ${BASE_URL}"

# Check if server is running
echo "1. Checking server health..."
if ! curl -s "${BASE_URL}/health" >/dev/null 2>&1; then
    echo "ERROR: vLLM server not responding at ${BASE_URL}"
    echo ""
    echo "Start the server with:"
    echo "  vllm serve lightonai/LightOnOCR-2-1B \\"
    echo "    --limit-mm-per-prompt '{\"image\": 1}' \\"
    echo "    --mm-processor-cache-gb 0 \\"
    echo "    --no-enable-prefix-caching \\"
    echo "    --port 8000"
    exit 1
fi
echo "   Server is healthy"

# Check models endpoint
echo "2. Checking loaded models..."
MODELS=$(curl -s "${BASE_URL}/v1/models" | python3 -c "import sys,json; print(json.load(sys.stdin)['data'][0]['id'])" 2>/dev/null || echo "")
if [[ "$MODELS" != *"LightOnOCR"* && "$MODELS" != "lightonai/LightOnOCR-2-1B" ]]; then
    echo "WARNING: LightOnOCR model may not be loaded correctly"
    echo "   Found: $MODELS"
else
    echo "   Model loaded: $MODELS"
fi

# Simple text completion test (without image to keep it quick)
echo "3. Testing API response..."
RESPONSE=$(curl -s "${BASE_URL}/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "lightonai/LightOnOCR-2-1B",
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10
  }' 2>/dev/null || echo "ERROR")

if [[ "$RESPONSE" == "ERROR" ]] || [[ "$RESPONSE" == *"error"* ]]; then
    echo "WARNING: API returned error - this may be expected for text-only input"
    echo "   LightOnOCR is designed for images, not text prompts"
else
    echo "   API responded successfully"
fi

echo ""
echo "Validation complete!"
echo "vLLM is serving LightOnOCR-2-1B at ${BASE_URL}"
echo ""
echo "For full validation, test with an actual image using:"
echo "  curl ${BASE_URL}/v1/chat/completions -d '{image_request}'"
```

Make script executable: chmod +x scripts/validate-vllm.sh
  </action>
  <verify>
    ls -la docs/vllm-validation.md shows documentation
    ls -la scripts/validate-vllm.sh shows executable script
  </verify>
  <done>vLLM validation documentation and scripts created</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: User validates vLLM with LightOnOCR locally</name>
  <what-built>
    vLLM validation documentation and scripts created:
    - docs/vllm-validation.md: Complete setup and testing guide
    - scripts/validate-vllm.sh: Automated validation script
  </what-built>
  <how-to-verify>
    If you have a local NVIDIA GPU with 8GB+ VRAM:

    1. Install vLLM:
       ```bash
       uv venv --python 3.12 --seed
       source .venv/bin/activate
       uv pip install vllm==0.11.2
       ```

    2. Start vLLM server (will download model on first run):
       ```bash
       vllm serve lightonai/LightOnOCR-2-1B \
         --limit-mm-per-prompt '{"image": 1}' \
         --mm-processor-cache-gb 0 \
         --no-enable-prefix-caching
       ```

    3. Run validation script (in new terminal):
       ```bash
       ./scripts/validate-vllm.sh
       ```

    If you DO NOT have a local GPU:
    - Skip this validation step
    - GPU service will be validated during Phase 13 cloud deployment
  </how-to-verify>
  <resume-signal>
    Type one of:
    - "validated" - vLLM successfully serves LightOnOCR locally
    - "skip-no-gpu" - No local GPU available, defer to Phase 13
    - "issue: [description]" - Problem encountered (describe issue)
  </resume-signal>
</task>

</tasks>

<verification>
- [ ] docs/vllm-validation.md contains complete setup guide
- [ ] scripts/validate-vllm.sh is executable
- [ ] User has validated vLLM OR acknowledged skip (no local GPU)
</verification>

<success_criteria>
vLLM validation resources created and either:
1. User confirmed vLLM serves LightOnOCR locally, OR
2. User acknowledged no local GPU (validation deferred to Phase 13)
</success_criteria>

<output>
After completion, create `.planning/phases/10-v2-setup-preparation/10-04-SUMMARY.md`
</output>
