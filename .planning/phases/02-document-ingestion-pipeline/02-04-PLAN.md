---
phase: 02-document-ingestion-pipeline
plan: 04
type: execute
wave: 3
depends_on: [02-01, 02-02, 02-03]
files_modified:
  - backend/src/ingestion/document_service.py
  - backend/src/api/dependencies.py
  - backend/tests/integration/test_documents_api.py
  - backend/tests/integration/conftest.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Uploaded PDF is processed by Docling and status becomes COMPLETED"
    - "page_count is populated after successful processing"
    - "Corrupted PDF upload results in FAILED status with error_message"
    - "Processing errors do not crash the application"
  artifacts:
    - path: "backend/src/ingestion/document_service.py"
      provides: "Docling integration in upload flow"
      contains: "DoclingProcessor"
    - path: "backend/tests/integration/test_documents_api.py"
      provides: "End-to-end processing tests"
      contains: "test_upload_processes_pdf_to_completed"
  key_links:
    - from: "backend/src/ingestion/document_service.py"
      to: "DoclingProcessor.process_bytes()"
      via: "method call after GCS upload"
      pattern: "docling_processor\\.process_bytes"
    - from: "backend/src/ingestion/document_service.py"
      to: "update_processing_result()"
      via: "status update after processing"
      pattern: "update_processing_result.*success=True"
---

<objective>
Wire DoclingProcessor into DocumentService upload flow and add end-to-end integration tests

Purpose: Close the two gaps identified in 02-VERIFICATION.md:
1. Gap 1: Docling processing is not wired into upload flow (documents stay PENDING forever)
2. Gap 2: Error handling is untested because processing never runs

Output: DocumentService.upload() calls DoclingProcessor.process_bytes() synchronously, updating status to COMPLETED/FAILED with page_count or error_message
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-ingestion-pipeline/02-02-SUMMARY.md
@.planning/phases/02-document-ingestion-pipeline/02-03-SUMMARY.md
@.planning/phases/02-document-ingestion-pipeline/02-VERIFICATION.md

Key files to modify:
@backend/src/ingestion/document_service.py
@backend/src/ingestion/docling_processor.py
@backend/tests/integration/conftest.py
@backend/tests/integration/test_documents_api.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire DoclingProcessor into DocumentService.upload()</name>
  <files>
    backend/src/ingestion/document_service.py
    backend/src/api/dependencies.py
  </files>
  <action>
Modify DocumentService to integrate Docling processing:

1. Update DocumentService.__init__ to accept DoclingProcessor:
   ```python
   def __init__(
       self,
       repository: DocumentRepository,
       gcs_client: GCSClient,
       docling_processor: DoclingProcessor,  # NEW
   ) -> None:
       self.repository = repository
       self.gcs_client = gcs_client
       self.docling_processor = docling_processor  # NEW
   ```

2. Import DoclingProcessor and DocumentProcessingError at top of file:
   ```python
   from src.ingestion.docling_processor import DoclingProcessor, DocumentProcessingError
   ```

3. Replace the "Phase 3" comment block (lines 202-203) with actual processing:
   ```python
   # Process document with Docling (synchronous for Phase 2)
   try:
       result = self.docling_processor.process_bytes(content, filename)
       await self.update_processing_result(
           document_id,
           success=True,
           page_count=result.page_count,
       )
       # Refresh document to get updated status
       document = await self.repository.get_by_id(document_id)
   except DocumentProcessingError as e:
       await self.update_processing_result(
           document_id,
           success=False,
           error_message=f"Document processing failed: {e.message}",
       )
       # Refresh document to get updated status
       document = await self.repository.get_by_id(document_id)
   ```

4. Update docstring to reflect that processing IS synchronous (remove "NOTE: Actual Docling processing is ASYNC" from class and method docstrings)

5. Update dependencies.py to inject DoclingProcessor:
   - Add get_docling_processor() dependency function
   - Update get_document_service() to accept and inject DoclingProcessor
   - Create type alias DoclingProcessorDep

Preserve all existing behavior:
- Duplicate detection still works
- GCS upload still happens
- Validation still runs
- Error handling for GCS failures unchanged
  </action>
  <verify>
Run existing tests - they should fail because DoclingProcessor is now required:
```bash
cd backend && python -m pytest tests/unit/test_document_service.py -v 2>&1 | head -30
```
This failure confirms the wiring is in place (tests need mock DoclingProcessor).
  </verify>
  <done>
DocumentService constructor requires DoclingProcessor parameter. Upload flow calls process_bytes() after GCS upload. Status updates to COMPLETED on success, FAILED on DocumentProcessingError.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update test fixtures and add processing integration tests</name>
  <files>
    backend/tests/integration/conftest.py
    backend/tests/integration/test_documents_api.py
    backend/tests/unit/test_document_service.py
  </files>
  <action>
1. Update conftest.py to provide mock DoclingProcessor:
   ```python
   from src.ingestion.docling_processor import DoclingProcessor, DocumentContent, PageContent, DocumentProcessingError

   @pytest.fixture
   def mock_docling_processor():
       """Create mock DoclingProcessor that returns successful result."""
       processor = MagicMock(spec=DoclingProcessor)
       processor.process_bytes = MagicMock(return_value=DocumentContent(
           text="Test document content",
           pages=[PageContent(page_number=1, text="Page 1 content", tables=[])],
           page_count=1,
           tables=[],
           metadata={"status": "SUCCESS"},
       ))
       return processor

   @pytest.fixture
   def mock_docling_processor_failing():
       """Create mock DoclingProcessor that fails (for error testing)."""
       processor = MagicMock(spec=DoclingProcessor)
       processor.process_bytes = MagicMock(
           side_effect=DocumentProcessingError("Invalid PDF format", details="Corrupted file")
       )
       return processor
   ```

2. Update client fixture to inject mock DoclingProcessor:
   - Add get_docling_processor override
   - Pass mock_docling_processor to fixture

3. Add new fixture client_with_failing_docling for error tests

4. Add new integration tests to test_documents_api.py:
   ```python
   class TestDocumentProcessing:
       """Tests for end-to-end document processing (Gap 1 closure)."""

       @pytest.mark.asyncio
       async def test_upload_processes_pdf_to_completed(self, client: AsyncClient):
           """Test that upload processes document and status becomes COMPLETED."""
           files = {"file": ("test.pdf", b"%PDF-1.4 test", "application/pdf")}
           response = await client.post("/api/documents/", files=files)

           assert response.status_code == 201
           data = response.json()
           # CRITICAL: Status should now be 'completed' (not 'pending')
           assert data["status"] == "completed"
           assert data["page_count"] == 1

       @pytest.mark.asyncio
       async def test_upload_populates_page_count(self, client: AsyncClient):
           """Test that page_count is set after processing."""
           files = {"file": ("doc.pdf", b"%PDF-1.4 content", "application/pdf")}
           response = await client.post("/api/documents/", files=files)

           assert response.status_code == 201
           assert response.json()["page_count"] is not None
           assert response.json()["page_count"] >= 1

   class TestDocumentProcessingErrors:
       """Tests for processing error handling (Gap 2 closure)."""

       @pytest.mark.asyncio
       async def test_corrupted_pdf_becomes_failed(self, client_with_failing_docling: AsyncClient):
           """Test that corrupted PDF results in FAILED status (INGEST-14)."""
           files = {"file": ("bad.pdf", b"not a pdf", "application/pdf")}
           response = await client_with_failing_docling.post("/api/documents/", files=files)

           # Upload succeeds (201) but status is 'failed'
           assert response.status_code == 201
           data = response.json()
           assert data["status"] == "failed"
           assert "error" in data["message"].lower() or data.get("error_message")

       @pytest.mark.asyncio
       async def test_processing_error_does_not_crash(self, client_with_failing_docling: AsyncClient):
           """Test that processing error doesn't crash the app (INGEST-14)."""
           files = {"file": ("corrupt.pdf", b"corrupted", "application/pdf")}
           response = await client_with_failing_docling.post("/api/documents/", files=files)

           # Should return 201 (upload succeeded), not 500 (crash)
           assert response.status_code == 201
           # Server is still running - make another request
           health = await client_with_failing_docling.get("/health")
           assert health.status_code == 200
   ```

5. Update unit tests in test_document_service.py to mock DoclingProcessor:
   - Add mock_docling_processor fixture
   - Update DocumentService instantiation in all tests
   - Verify existing tests pass with mock
  </action>
  <verify>
Run all tests including new ones:
```bash
cd backend && python -m pytest tests/ -v --tb=short 2>&1 | tail -40
```
All tests should pass, including new processing tests.
  </verify>
  <done>
New tests exist: test_upload_processes_pdf_to_completed, test_upload_populates_page_count, test_corrupted_pdf_becomes_failed, test_processing_error_does_not_crash. All existing tests updated to use mock DoclingProcessor. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update API response to include processing status details</name>
  <files>
    backend/src/api/documents.py
  </files>
  <action>
Update the upload endpoint response to communicate processing result:

1. Modify POST /api/documents/ response message based on status:
   ```python
   if document.status == DocumentStatus.COMPLETED:
       message = f"Document processed successfully with {document.page_count} page(s)"
   elif document.status == DocumentStatus.FAILED:
       message = f"Document upload succeeded but processing failed: {document.error_message or 'Unknown error'}"
   else:
       message = "Document uploaded. Processing status: pending"
   ```

2. Include error_message in response when status is FAILED:
   ```python
   return JSONResponse(
       status_code=201,
       content={
           "id": str(document.id),
           "filename": document.filename,
           "file_hash": document.file_hash,
           "file_type": document.file_type,
           "status": document.status,
           "page_count": document.page_count,
           "error_message": document.error_message,  # NEW: include for debugging
           "message": message,
       },
   )
   ```

3. Keep the 201 status code even for processing failures (upload DID succeed, processing failed - this is by design so clients know the document exists)
  </action>
  <verify>
Run integration tests to verify responses:
```bash
cd backend && python -m pytest tests/integration/test_documents_api.py -v -k "process" 2>&1
```
Tests should pass with new response format.
  </verify>
  <done>
Upload endpoint returns status="completed" with page_count for successful processing, status="failed" with error_message for processing failures. Response includes processing-aware message.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Run full test suite:
```bash
cd backend && python -m pytest tests/ -v --tb=short
```

2. Verify Phase 2 success criteria are now met:
   - Criterion 2: "Docling processes the uploaded PDF" - upload now calls DoclingProcessor
   - Criterion 5: "Failed processing records error status" - DocumentProcessingError handled and status set to FAILED

3. Check for ruff violations:
```bash
cd backend && ruff check src/ tests/
```

4. Verify wiring exists:
```bash
grep -n "DoclingProcessor" backend/src/ingestion/document_service.py
# Should find: import and constructor parameter
grep -n "process_bytes" backend/src/ingestion/document_service.py
# Should find: call in upload method
```
</verification>

<success_criteria>
1. DocumentService.upload() calls DoclingProcessor.process_bytes() after GCS upload
2. Successful processing: document.status becomes "completed", page_count is populated
3. Failed processing: document.status becomes "failed", error_message is populated
4. All 57+ existing tests pass (no regressions)
5. New integration tests verify end-to-end processing flow
6. No ruff errors in modified files
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-ingestion-pipeline/02-04-SUMMARY.md`
</output>
