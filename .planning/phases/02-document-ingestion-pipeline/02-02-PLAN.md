---
phase: 02-document-ingestion-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/src/ingestion/__init__.py
  - backend/src/ingestion/docling_processor.py
  - backend/src/storage/gcs_client.py
  - backend/tests/unit/test_docling_processor.py
  - backend/tests/unit/test_gcs_client.py
autonomous: true

must_haves:
  truths:
    - "DoclingProcessor extracts text from PDF files with page boundaries"
    - "DoclingProcessor extracts text from DOCX files"
    - "DoclingProcessor handles images with OCR"
    - "DoclingProcessor extracts tables as structured data"
    - "GCS client uploads files and returns gs:// URI"
    - "GCS client downloads files by path"
    - "GCS client generates signed URLs for temporary access"
    - "GCS client checks file existence"
  artifacts:
    - path: "backend/src/ingestion/docling_processor.py"
      provides: "Docling wrapper for document conversion"
      exports: ["DoclingProcessor", "PageContent", "DocumentContent"]
    - path: "backend/src/storage/gcs_client.py"
      provides: "GCS client for file operations"
      exports: ["GCSClient"]
  key_links:
    - from: "backend/src/ingestion/docling_processor.py"
      to: "docling"
      via: "imports DocumentConverter"
      pattern: "from docling\\.document_converter import"
    - from: "backend/src/storage/gcs_client.py"
      to: "google-cloud-storage"
      via: "imports storage Client"
      pattern: "from google\\.cloud import storage"
---

<objective>
Create the Docling processor wrapper and GCS client for document processing and storage.

Purpose: Enable document text extraction from PDF/DOCX/images and file storage in Google Cloud Storage with signed URL support for secure access.

Output:
- DoclingProcessor class wrapping DocumentConverter with memory-safe patterns
- GCSClient class for upload/download/signed URLs
- Comprehensive unit tests for both components
- Error handling with proper exceptions
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-ingestion-pipeline/02-RESEARCH.md
@backend/src/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: DoclingProcessor with memory-safe document conversion</name>
  <files>
    backend/src/ingestion/__init__.py
    backend/src/ingestion/docling_processor.py
    backend/tests/unit/test_docling_processor.py
  </files>
  <action>
Create the DoclingProcessor wrapper following patterns from 02-RESEARCH.md:

**backend/src/ingestion/__init__.py:**
```python
"""Document ingestion module for processing uploaded files."""

from src.ingestion.docling_processor import DoclingProcessor, DocumentContent, PageContent

__all__ = ["DoclingProcessor", "DocumentContent", "PageContent"]
```

**backend/src/ingestion/docling_processor.py:**

```python
"""Docling wrapper for document conversion with memory-safe patterns.

CRITICAL: Create a fresh DocumentConverter instance for each document to avoid
memory leaks (see GitHub issue #2209). Do NOT reuse converter instances.
"""

import tempfile
from pathlib import Path
from typing import Any

from pydantic import BaseModel, Field

# Docling imports
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions


class PageContent(BaseModel):
    """Content extracted from a single page."""

    page_number: int = Field(..., ge=1, description="1-indexed page number")
    text: str = Field(default="", description="Text content from this page")
    tables: list[dict[str, Any]] = Field(
        default_factory=list, description="Tables extracted from this page"
    )


class DocumentContent(BaseModel):
    """Complete document extraction result."""

    text: str = Field(..., description="Full document text as markdown")
    pages: list[PageContent] = Field(
        default_factory=list, description="Page-by-page content"
    )
    page_count: int = Field(..., ge=0, description="Total number of pages")
    tables: list[dict[str, Any]] = Field(
        default_factory=list, description="All tables from document"
    )
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Extraction metadata"
    )


class DocumentProcessingError(Exception):
    """Raised when document processing fails."""

    def __init__(self, message: str, details: str | None = None):
        self.message = message
        self.details = details
        super().__init__(message)


class DoclingProcessor:
    """Wrapper for Docling document conversion.

    IMPORTANT: This class creates a fresh DocumentConverter for each conversion
    to prevent memory leaks. Do NOT cache the converter instance.
    """

    def __init__(
        self,
        enable_ocr: bool = True,
        enable_tables: bool = True,
        max_pages: int = 100,
    ) -> None:
        """Initialize processor with configuration.

        Args:
            enable_ocr: Enable OCR for scanned/image documents
            enable_tables: Enable table structure extraction
            max_pages: Maximum pages to process (default 100, prevents hangs on large PDFs)
        """
        self.enable_ocr = enable_ocr
        self.enable_tables = enable_tables
        self.max_pages = max_pages

    def _create_converter(self) -> DocumentConverter:
        """Create a fresh converter instance (memory-safe pattern)."""
        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = self.enable_ocr
        pipeline_options.do_table_structure = self.enable_tables

        format_options = {
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options),
        }

        return DocumentConverter(format_options=format_options)

    def process(self, file_path: Path) -> DocumentContent:
        """Process a document file and extract structured content.

        Args:
            file_path: Path to the document file (PDF, DOCX, PNG, JPG)

        Returns:
            DocumentContent with text, pages, tables, and metadata

        Raises:
            DocumentProcessingError: If conversion fails
        """
        if not file_path.exists():
            raise DocumentProcessingError(f"File not found: {file_path}")

        # Create fresh converter to avoid memory leaks
        converter = self._create_converter()

        try:
            result = converter.convert(
                source=file_path,
                raises_on_error=False,
                max_num_pages=self.max_pages,
            )
        except Exception as e:
            raise DocumentProcessingError(
                f"Conversion failed for {file_path.name}",
                details=str(e),
            ) from e

        # Check for conversion errors
        if result.status.name == "FAILURE":
            error_details = "; ".join(str(e) for e in result.errors) if result.errors else "Unknown error"
            raise DocumentProcessingError(
                f"Document conversion failed: {file_path.name}",
                details=error_details,
            )

        doc = result.document

        # Extract page-level content
        pages: list[PageContent] = []
        all_tables: list[dict[str, Any]] = []

        # Get page count
        page_count = len(doc.pages) if hasattr(doc, 'pages') and doc.pages else 0

        # Extract text as markdown (preserves structure)
        try:
            full_text = doc.export_to_markdown()
        except Exception:
            full_text = ""

        # Build page content (simplified - Docling's page structure varies by format)
        for i in range(1, page_count + 1):
            pages.append(PageContent(
                page_number=i,
                text="",  # Page-level text extraction varies by document type
                tables=[],
            ))

        # Extract tables if available
        if hasattr(doc, 'tables'):
            for table in doc.tables:
                table_data: dict[str, Any] = {
                    "rows": [],
                    "headers": [],
                }
                if hasattr(table, 'data'):
                    table_data["rows"] = table.data
                all_tables.append(table_data)

        return DocumentContent(
            text=full_text,
            pages=pages,
            page_count=page_count,
            tables=all_tables,
            metadata={
                "status": result.status.name,
                "source_file": file_path.name,
            },
        )

    def process_bytes(
        self,
        data: bytes,
        filename: str,
    ) -> DocumentContent:
        """Process document from bytes (for uploaded files).

        Args:
            data: Raw file bytes
            filename: Original filename (used to determine file type)

        Returns:
            DocumentContent with extracted data
        """
        suffix = Path(filename).suffix or ".pdf"

        with tempfile.NamedTemporaryFile(suffix=suffix, delete=True) as tmp:
            tmp.write(data)
            tmp.flush()
            return self.process(Path(tmp.name))
```

**backend/tests/unit/test_docling_processor.py:**

```python
"""Unit tests for DoclingProcessor.

These tests use mock to avoid actual Docling processing in unit tests.
Integration tests will test actual document processing.
"""

import pytest
from pathlib import Path
from unittest.mock import MagicMock, patch, PropertyMock

from src.ingestion.docling_processor import (
    DoclingProcessor,
    DocumentContent,
    PageContent,
    DocumentProcessingError,
)


class TestDoclingProcessorInit:
    """Tests for DoclingProcessor initialization."""

    def test_default_configuration(self):
        """Test default processor configuration."""
        processor = DoclingProcessor()
        assert processor.enable_ocr is True
        assert processor.enable_tables is True
        assert processor.max_pages == 100

    def test_custom_configuration(self):
        """Test custom processor configuration."""
        processor = DoclingProcessor(
            enable_ocr=False,
            enable_tables=False,
            max_pages=50,
        )
        assert processor.enable_ocr is False
        assert processor.enable_tables is False
        assert processor.max_pages == 50


class TestDoclingProcessorProcess:
    """Tests for DoclingProcessor.process method."""

    def test_file_not_found_raises_error(self):
        """Test that missing file raises DocumentProcessingError."""
        processor = DoclingProcessor()
        with pytest.raises(DocumentProcessingError) as exc_info:
            processor.process(Path("/nonexistent/file.pdf"))
        assert "File not found" in str(exc_info.value)

    @patch("src.ingestion.docling_processor.DocumentConverter")
    def test_successful_pdf_processing(self, mock_converter_class: MagicMock, tmp_path: Path):
        """Test successful PDF processing."""
        # Create a test file
        test_file = tmp_path / "test.pdf"
        test_file.write_bytes(b"%PDF-1.4 test content")

        # Mock the converter and result
        mock_converter = MagicMock()
        mock_converter_class.return_value = mock_converter

        mock_doc = MagicMock()
        mock_doc.export_to_markdown.return_value = "# Test Document\n\nSome content"
        mock_doc.pages = [MagicMock(), MagicMock()]  # 2 pages
        mock_doc.tables = []

        mock_result = MagicMock()
        mock_result.status.name = "SUCCESS"
        mock_result.document = mock_doc
        mock_result.errors = []

        mock_converter.convert.return_value = mock_result

        # Process the document
        processor = DoclingProcessor()
        result = processor.process(test_file)

        # Verify result
        assert isinstance(result, DocumentContent)
        assert result.text == "# Test Document\n\nSome content"
        assert result.page_count == 2
        assert len(result.pages) == 2
        assert result.metadata["status"] == "SUCCESS"

    @patch("src.ingestion.docling_processor.DocumentConverter")
    def test_conversion_failure_raises_error(self, mock_converter_class: MagicMock, tmp_path: Path):
        """Test that conversion failure raises DocumentProcessingError."""
        test_file = tmp_path / "bad.pdf"
        test_file.write_bytes(b"not a pdf")

        mock_converter = MagicMock()
        mock_converter_class.return_value = mock_converter

        mock_result = MagicMock()
        mock_result.status.name = "FAILURE"
        mock_result.errors = ["Invalid PDF format"]

        mock_converter.convert.return_value = mock_result

        processor = DoclingProcessor()
        with pytest.raises(DocumentProcessingError) as exc_info:
            processor.process(test_file)

        assert "conversion failed" in str(exc_info.value).lower()

    @patch("src.ingestion.docling_processor.DocumentConverter")
    def test_converter_exception_raises_error(self, mock_converter_class: MagicMock, tmp_path: Path):
        """Test that converter exception is wrapped in DocumentProcessingError."""
        test_file = tmp_path / "test.pdf"
        test_file.write_bytes(b"%PDF-1.4")

        mock_converter = MagicMock()
        mock_converter_class.return_value = mock_converter
        mock_converter.convert.side_effect = RuntimeError("Memory error")

        processor = DoclingProcessor()
        with pytest.raises(DocumentProcessingError) as exc_info:
            processor.process(test_file)

        assert "Conversion failed" in str(exc_info.value)


class TestDoclingProcessorProcessBytes:
    """Tests for DoclingProcessor.process_bytes method."""

    @patch("src.ingestion.docling_processor.DocumentConverter")
    def test_process_bytes_creates_temp_file(self, mock_converter_class: MagicMock):
        """Test that process_bytes creates temp file with correct suffix."""
        mock_converter = MagicMock()
        mock_converter_class.return_value = mock_converter

        mock_doc = MagicMock()
        mock_doc.export_to_markdown.return_value = "Content"
        mock_doc.pages = []
        mock_doc.tables = []

        mock_result = MagicMock()
        mock_result.status.name = "SUCCESS"
        mock_result.document = mock_doc

        mock_converter.convert.return_value = mock_result

        processor = DoclingProcessor()
        result = processor.process_bytes(b"test data", "document.pdf")

        assert isinstance(result, DocumentContent)
        # Verify convert was called (temp file was created)
        mock_converter.convert.assert_called_once()


class TestPageContent:
    """Tests for PageContent model."""

    def test_page_content_creation(self):
        """Test creating PageContent with all fields."""
        page = PageContent(
            page_number=1,
            text="Page content",
            tables=[{"headers": ["A"], "rows": [["1"]]}],
        )
        assert page.page_number == 1
        assert page.text == "Page content"
        assert len(page.tables) == 1

    def test_page_content_defaults(self):
        """Test PageContent default values."""
        page = PageContent(page_number=1)
        assert page.text == ""
        assert page.tables == []


class TestDocumentContent:
    """Tests for DocumentContent model."""

    def test_document_content_creation(self):
        """Test creating DocumentContent."""
        content = DocumentContent(
            text="# Document",
            pages=[PageContent(page_number=1)],
            page_count=1,
            tables=[],
            metadata={"status": "SUCCESS"},
        )
        assert content.text == "# Document"
        assert content.page_count == 1
        assert len(content.pages) == 1

    def test_document_content_defaults(self):
        """Test DocumentContent default values."""
        content = DocumentContent(text="test", page_count=0)
        assert content.pages == []
        assert content.tables == []
        assert content.metadata == {}
```
  </action>
  <verify>
- `python -c "from src.ingestion import DoclingProcessor, DocumentContent, PageContent"` succeeds
- `cd backend && python -m pytest tests/unit/test_docling_processor.py -v` - all tests pass
- `ruff check backend/src/ingestion/` passes
  </verify>
  <done>
DoclingProcessor wraps Docling DocumentConverter with memory-safe patterns (fresh instance per conversion), handles PDF/DOCX/images, extracts text with page boundaries, handles errors gracefully. All unit tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: GCS client with upload, download, and signed URLs</name>
  <files>
    backend/src/storage/gcs_client.py
    backend/tests/unit/test_gcs_client.py
  </files>
  <action>
Create GCS client for document storage following patterns from 02-RESEARCH.md:

**backend/src/storage/gcs_client.py:**

```python
"""Google Cloud Storage client for document storage.

Uses Application Default Credentials (ADC) for authentication:
- Local development: Set GOOGLE_APPLICATION_CREDENTIALS env var
- Cloud Run: Uses attached service account automatically
"""

from datetime import timedelta
from typing import BinaryIO

from google.cloud import storage
from google.cloud.exceptions import NotFound


class GCSError(Exception):
    """Base exception for GCS operations."""

    pass


class GCSUploadError(GCSError):
    """Raised when upload fails."""

    pass


class GCSDownloadError(GCSError):
    """Raised when download fails."""

    pass


class GCSClient:
    """Google Cloud Storage client for document operations.

    Uses Application Default Credentials for seamless Cloud Run deployment.
    """

    def __init__(self, bucket_name: str) -> None:
        """Initialize GCS client.

        Args:
            bucket_name: Name of the GCS bucket to use
        """
        if not bucket_name:
            raise ValueError("bucket_name cannot be empty")

        # Uses Application Default Credentials (ADC)
        self.client = storage.Client()
        self.bucket_name = bucket_name
        self._bucket = None

    @property
    def bucket(self) -> storage.Bucket:
        """Lazy-load bucket to allow testing without real GCS."""
        if self._bucket is None:
            self._bucket = self.client.bucket(self.bucket_name)
        return self._bucket

    def upload(
        self,
        data: bytes,
        path: str,
        content_type: str = "application/octet-stream",
    ) -> str:
        """Upload bytes to GCS.

        Args:
            data: File content as bytes
            path: Destination path within bucket (e.g., "documents/uuid/file.pdf")
            content_type: MIME type of the file

        Returns:
            gs:// URI of uploaded file

        Raises:
            GCSUploadError: If upload fails
        """
        try:
            blob = self.bucket.blob(path)
            blob.upload_from_string(data, content_type=content_type)
            return f"gs://{self.bucket_name}/{path}"
        except Exception as e:
            raise GCSUploadError(f"Failed to upload to {path}: {e}") from e

    def upload_from_file(
        self,
        file_obj: BinaryIO,
        path: str,
        content_type: str = "application/octet-stream",
    ) -> str:
        """Upload from file-like object (memory efficient for large files).

        Args:
            file_obj: File-like object to upload
            path: Destination path within bucket
            content_type: MIME type of the file

        Returns:
            gs:// URI of uploaded file

        Raises:
            GCSUploadError: If upload fails
        """
        try:
            blob = self.bucket.blob(path)
            file_obj.seek(0)
            blob.upload_from_file(file_obj, content_type=content_type)
            return f"gs://{self.bucket_name}/{path}"
        except Exception as e:
            raise GCSUploadError(f"Failed to upload from file to {path}: {e}") from e

    def download(self, path: str) -> bytes:
        """Download file content as bytes.

        Args:
            path: Path within bucket to download

        Returns:
            File content as bytes

        Raises:
            GCSDownloadError: If download fails or file not found
        """
        try:
            blob = self.bucket.blob(path)
            return blob.download_as_bytes()
        except NotFound:
            raise GCSDownloadError(f"File not found: {path}")
        except Exception as e:
            raise GCSDownloadError(f"Failed to download {path}: {e}") from e

    def download_to_file(self, path: str, file_obj: BinaryIO) -> None:
        """Download file to file-like object.

        Args:
            path: Path within bucket to download
            file_obj: File-like object to write to

        Raises:
            GCSDownloadError: If download fails
        """
        try:
            blob = self.bucket.blob(path)
            blob.download_to_file(file_obj)
            file_obj.seek(0)
        except NotFound:
            raise GCSDownloadError(f"File not found: {path}")
        except Exception as e:
            raise GCSDownloadError(f"Failed to download {path}: {e}") from e

    def exists(self, path: str) -> bool:
        """Check if file exists in bucket.

        Args:
            path: Path within bucket

        Returns:
            True if file exists, False otherwise
        """
        blob = self.bucket.blob(path)
        return blob.exists()

    def delete(self, path: str) -> None:
        """Delete a file from storage.

        Args:
            path: Path within bucket to delete

        Raises:
            GCSError: If deletion fails
        """
        try:
            blob = self.bucket.blob(path)
            blob.delete()
        except NotFound:
            pass  # Already deleted, not an error
        except Exception as e:
            raise GCSError(f"Failed to delete {path}: {e}") from e

    def get_signed_url(
        self,
        path: str,
        expiration_minutes: int = 15,
        method: str = "GET",
    ) -> str:
        """Generate a signed URL for temporary access.

        Args:
            path: Path within bucket
            expiration_minutes: How long the URL is valid (default 15 min)
            method: HTTP method (GET for download, PUT for upload)

        Returns:
            Signed URL string

        Note:
            Requires service account credentials for signing.
            On Cloud Run, uses impersonated credentials automatically.
        """
        blob = self.bucket.blob(path)
        return blob.generate_signed_url(
            version="v4",
            expiration=timedelta(minutes=expiration_minutes),
            method=method,
        )

    def parse_gcs_uri(self, uri: str) -> tuple[str, str]:
        """Parse a gs:// URI into bucket and path.

        Args:
            uri: GCS URI (e.g., "gs://bucket-name/path/to/file")

        Returns:
            Tuple of (bucket_name, path)

        Raises:
            ValueError: If URI format is invalid
        """
        if not uri.startswith("gs://"):
            raise ValueError(f"Invalid GCS URI format: {uri}")

        parts = uri[5:].split("/", 1)
        if len(parts) != 2:
            raise ValueError(f"Invalid GCS URI format: {uri}")

        return parts[0], parts[1]
```

**Update backend/src/storage/__init__.py** to export GCSClient:
```python
"""Storage module for database and GCS operations."""

from src.storage.database import (
    DBSession,
    async_session_factory,
    engine,
    get_db_session,
)
from src.storage.gcs_client import GCSClient, GCSError, GCSDownloadError, GCSUploadError
from src.storage.models import (
    AccountNumber,
    Base,
    Borrower,
    Document,
    DocumentStatus,
    IncomeRecord,
    SourceReference,
)
from src.storage.repositories import DocumentRepository

__all__ = [
    # Database
    "Base",
    "Document",
    "DocumentStatus",
    "Borrower",
    "IncomeRecord",
    "AccountNumber",
    "SourceReference",
    "engine",
    "async_session_factory",
    "get_db_session",
    "DBSession",
    "DocumentRepository",
    # GCS
    "GCSClient",
    "GCSError",
    "GCSUploadError",
    "GCSDownloadError",
]
```

**backend/tests/unit/test_gcs_client.py:**

```python
"""Unit tests for GCS client.

Uses mocking to test without actual GCS access.
Integration tests will verify real GCS operations.
"""

import io
import pytest
from unittest.mock import MagicMock, patch, PropertyMock

from src.storage.gcs_client import (
    GCSClient,
    GCSError,
    GCSUploadError,
    GCSDownloadError,
)


class TestGCSClientInit:
    """Tests for GCSClient initialization."""

    def test_empty_bucket_name_raises_error(self):
        """Test that empty bucket name raises ValueError."""
        with pytest.raises(ValueError, match="bucket_name cannot be empty"):
            GCSClient("")

    @patch("src.storage.gcs_client.storage.Client")
    def test_creates_storage_client(self, mock_client_class: MagicMock):
        """Test that GCSClient creates a storage Client."""
        client = GCSClient("test-bucket")
        mock_client_class.assert_called_once()
        assert client.bucket_name == "test-bucket"


class TestGCSClientUpload:
    """Tests for GCSClient.upload method."""

    @patch("src.storage.gcs_client.storage.Client")
    def test_upload_returns_gs_uri(self, mock_client_class: MagicMock):
        """Test that upload returns correct gs:// URI."""
        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        uri = client.upload(b"test data", "documents/test.pdf", "application/pdf")

        assert uri == "gs://my-bucket/documents/test.pdf"
        mock_blob.upload_from_string.assert_called_once_with(
            b"test data", content_type="application/pdf"
        )

    @patch("src.storage.gcs_client.storage.Client")
    def test_upload_failure_raises_error(self, mock_client_class: MagicMock):
        """Test that upload failure raises GCSUploadError."""
        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_blob.upload_from_string.side_effect = Exception("Network error")
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        with pytest.raises(GCSUploadError, match="Failed to upload"):
            client.upload(b"test", "path.pdf")


class TestGCSClientDownload:
    """Tests for GCSClient.download method."""

    @patch("src.storage.gcs_client.storage.Client")
    def test_download_returns_bytes(self, mock_client_class: MagicMock):
        """Test that download returns file content."""
        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_blob.download_as_bytes.return_value = b"file content"
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        data = client.download("documents/test.pdf")

        assert data == b"file content"

    @patch("src.storage.gcs_client.storage.Client")
    def test_download_not_found_raises_error(self, mock_client_class: MagicMock):
        """Test that downloading non-existent file raises error."""
        from google.cloud.exceptions import NotFound

        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_blob.download_as_bytes.side_effect = NotFound("Not found")
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        with pytest.raises(GCSDownloadError, match="File not found"):
            client.download("nonexistent.pdf")


class TestGCSClientExists:
    """Tests for GCSClient.exists method."""

    @patch("src.storage.gcs_client.storage.Client")
    def test_exists_returns_true(self, mock_client_class: MagicMock):
        """Test exists returns True for existing file."""
        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_blob.exists.return_value = True
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        assert client.exists("documents/test.pdf") is True

    @patch("src.storage.gcs_client.storage.Client")
    def test_exists_returns_false(self, mock_client_class: MagicMock):
        """Test exists returns False for non-existent file."""
        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_blob.exists.return_value = False
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        assert client.exists("nonexistent.pdf") is False


class TestGCSClientDelete:
    """Tests for GCSClient.delete method."""

    @patch("src.storage.gcs_client.storage.Client")
    def test_delete_succeeds(self, mock_client_class: MagicMock):
        """Test successful deletion."""
        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        client.delete("documents/test.pdf")

        mock_blob.delete.assert_called_once()

    @patch("src.storage.gcs_client.storage.Client")
    def test_delete_not_found_succeeds(self, mock_client_class: MagicMock):
        """Test that deleting non-existent file doesn't raise error."""
        from google.cloud.exceptions import NotFound

        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_blob.delete.side_effect = NotFound("Not found")
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        # Should not raise
        client.delete("nonexistent.pdf")


class TestGCSClientSignedUrl:
    """Tests for GCSClient.get_signed_url method."""

    @patch("src.storage.gcs_client.storage.Client")
    def test_get_signed_url_returns_url(self, mock_client_class: MagicMock):
        """Test that get_signed_url returns a URL."""
        mock_client = MagicMock()
        mock_client_class.return_value = mock_client

        mock_bucket = MagicMock()
        mock_client.bucket.return_value = mock_bucket

        mock_blob = MagicMock()
        mock_blob.generate_signed_url.return_value = "https://storage.googleapis.com/signed-url"
        mock_bucket.blob.return_value = mock_blob

        client = GCSClient("my-bucket")
        url = client.get_signed_url("documents/test.pdf", expiration_minutes=30)

        assert "https://" in url
        mock_blob.generate_signed_url.assert_called_once()


class TestGCSClientParseUri:
    """Tests for GCSClient.parse_gcs_uri method."""

    @patch("src.storage.gcs_client.storage.Client")
    def test_parse_valid_uri(self, mock_client_class: MagicMock):
        """Test parsing valid gs:// URI."""
        client = GCSClient("test-bucket")
        bucket, path = client.parse_gcs_uri("gs://my-bucket/path/to/file.pdf")
        assert bucket == "my-bucket"
        assert path == "path/to/file.pdf"

    @patch("src.storage.gcs_client.storage.Client")
    def test_parse_invalid_uri_raises_error(self, mock_client_class: MagicMock):
        """Test that invalid URI raises ValueError."""
        client = GCSClient("test-bucket")

        with pytest.raises(ValueError, match="Invalid GCS URI"):
            client.parse_gcs_uri("https://storage.googleapis.com/file")

        with pytest.raises(ValueError, match="Invalid GCS URI"):
            client.parse_gcs_uri("gs://bucket-only")
```
  </action>
  <verify>
- `python -c "from src.storage.gcs_client import GCSClient, GCSError"` succeeds
- `python -c "from src.storage import GCSClient"` succeeds (via __init__)
- `cd backend && python -m pytest tests/unit/test_gcs_client.py -v` - all tests pass
- `ruff check backend/src/storage/gcs_client.py` passes
  </verify>
  <done>
GCS client provides upload/download/exists/delete/signed URL operations with proper error handling and ADC authentication. All unit tests pass using mocks.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `python -c "from src.ingestion import DoclingProcessor"` - imports work
2. `python -c "from src.storage import GCSClient"` - imports work
3. `cd backend && python -m pytest tests/unit/test_docling_processor.py tests/unit/test_gcs_client.py -v` - all tests pass
4. `ruff check backend/src/ingestion/ backend/src/storage/` - no lint errors
</verification>

<success_criteria>
1. DoclingProcessor creates fresh DocumentConverter per document (memory-safe)
2. DoclingProcessor handles PDF, DOCX, and image files
3. DoclingProcessor returns DocumentContent with text, pages, page_count, tables
4. DoclingProcessor wraps errors in DocumentProcessingError
5. GCSClient uploads bytes and returns gs:// URI
6. GCSClient downloads files as bytes
7. GCSClient generates signed URLs for temporary access
8. GCSClient checks file existence
9. All unit tests pass (mocked, no real GCS/Docling required)
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-ingestion-pipeline/02-02-SUMMARY.md`
</output>
