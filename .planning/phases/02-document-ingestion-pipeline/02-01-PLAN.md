---
phase: 02-document-ingestion-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/storage/__init__.py
  - backend/src/storage/models.py
  - backend/src/storage/database.py
  - backend/src/storage/repositories.py
  - backend/alembic.ini
  - backend/alembic/env.py
  - backend/alembic/script.py.mako
  - backend/alembic/versions/001_initial_schema.py
  - backend/tests/unit/test_repositories.py
autonomous: true

must_haves:
  truths:
    - "Document records can be created in database with all required fields"
    - "Documents can be retrieved by ID and by file_hash"
    - "Document status can be updated (PENDING -> PROCESSING -> COMPLETED/FAILED)"
    - "Alembic migrations run successfully against PostgreSQL"
    - "Duplicate file_hash triggers unique constraint violation"
  artifacts:
    - path: "backend/src/storage/models.py"
      provides: "SQLAlchemy ORM models for Document, Borrower, IncomeRecord, AccountNumber, SourceReference"
      exports: ["Base", "Document", "DocumentStatus", "Borrower", "IncomeRecord", "AccountNumber", "SourceReference"]
    - path: "backend/src/storage/database.py"
      provides: "Async engine and session factory"
      exports: ["engine", "async_session_factory", "get_db_session", "DBSession"]
    - path: "backend/src/storage/repositories.py"
      provides: "Repository pattern for async database operations"
      exports: ["DocumentRepository"]
    - path: "backend/alembic/versions/001_initial_schema.py"
      provides: "Initial database migration"
      contains: "def upgrade"
  key_links:
    - from: "backend/src/storage/repositories.py"
      to: "backend/src/storage/models.py"
      via: "imports Document, DocumentStatus"
      pattern: "from src\\.storage\\.models import"
    - from: "backend/src/storage/database.py"
      to: "backend/src/storage/models.py"
      via: "imports Base for metadata"
      pattern: "from src\\.storage\\.models import Base"
    - from: "backend/alembic/env.py"
      to: "backend/src/storage/models.py"
      via: "imports Base.metadata for autogenerate"
      pattern: "target_metadata = Base\\.metadata"
---

<objective>
Create SQLAlchemy ORM models and Alembic migrations for the document ingestion pipeline database layer.

Purpose: Establish the database foundation for storing documents, borrowers, and source references with proper async support for the FastAPI application.

Output:
- SQLAlchemy 2.0 async-compatible models (Document, Borrower, IncomeRecord, AccountNumber, SourceReference)
- Async database engine and session factory with dependency injection
- Alembic async migration configuration
- Initial migration creating all tables
- DocumentRepository with async CRUD operations
- Unit tests for repository operations
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-ingestion-pipeline/02-RESEARCH.md
@backend/src/config.py
@backend/src/models/borrower.py
@backend/src/models/document.py
@backend/src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: SQLAlchemy ORM models and database setup</name>
  <files>
    backend/src/storage/__init__.py
    backend/src/storage/models.py
    backend/src/storage/database.py
  </files>
  <action>
Create the storage module with SQLAlchemy 2.0 async-compatible ORM models:

**backend/src/storage/__init__.py:**
- Export Base, all models, engine, session factory, DBSession type alias

**backend/src/storage/models.py:**
Create models following SQLAlchemy 2.0 patterns with `Mapped` and `mapped_column`:

1. `Base(DeclarativeBase)` - Base class for all models

2. `DocumentStatus(str, Enum)` - Enum with values: PENDING, PROCESSING, COMPLETED, FAILED

3. `Document(Base)`:
   - `id: Mapped[UUID]` - Primary key, default uuid4
   - `filename: Mapped[str]` - String(255), not null
   - `file_hash: Mapped[str]` - String(64), unique, indexed, not null (for duplicate detection)
   - `file_type: Mapped[str]` - String(10), not null
   - `file_size_bytes: Mapped[int]` - Integer, not null
   - `gcs_uri: Mapped[str | None]` - String(500), nullable
   - `status: Mapped[DocumentStatus]` - Enum, default PENDING
   - `error_message: Mapped[str | None]` - Text, nullable
   - `page_count: Mapped[int | None]` - Integer, nullable
   - `created_at: Mapped[datetime]` - DateTime with timezone, default now(timezone.utc)
   - `processed_at: Mapped[datetime | None]` - DateTime, nullable

4. `Borrower(Base)`:
   - `id: Mapped[UUID]` - Primary key, default uuid4
   - `name: Mapped[str]` - String(255), not null
   - `ssn_hash: Mapped[str | None]` - String(64), indexed, nullable (hashed SSN for matching)
   - `address_json: Mapped[str | None]` - Text, nullable (JSON serialized address)
   - `confidence_score: Mapped[Decimal]` - Numeric(3, 2), not null
   - `created_at: Mapped[datetime]` - DateTime with timezone
   - Relationships: income_records, account_numbers, source_references

5. `IncomeRecord(Base)`:
   - `id: Mapped[UUID]` - Primary key
   - `borrower_id: Mapped[UUID]` - ForeignKey to borrowers.id, not null
   - `amount: Mapped[Decimal]` - Numeric(12, 2), not null
   - `period: Mapped[str]` - String(20), not null (annual, monthly, weekly, biweekly)
   - `year: Mapped[int]` - Integer, not null
   - `source_type: Mapped[str]` - String(50), not null
   - `employer: Mapped[str | None]` - String(255), nullable
   - Relationship: borrower (back_populates)

6. `AccountNumber(Base)`:
   - `id: Mapped[UUID]` - Primary key
   - `borrower_id: Mapped[UUID]` - ForeignKey to borrowers.id, not null
   - `number: Mapped[str]` - String(50), not null
   - `account_type: Mapped[str]` - String(20), not null (bank, loan)
   - Relationship: borrower (back_populates)

7. `SourceReference(Base)`:
   - `id: Mapped[UUID]` - Primary key
   - `borrower_id: Mapped[UUID]` - ForeignKey to borrowers.id, not null
   - `document_id: Mapped[UUID]` - ForeignKey to documents.id, not null
   - `page_number: Mapped[int]` - Integer, not null
   - `section: Mapped[str | None]` - String(100), nullable
   - `snippet: Mapped[str]` - Text, not null
   - Relationships: borrower, document (back_populates)

**backend/src/storage/database.py:**
1. Create async engine from settings.database_url with pool configuration
2. Create async_sessionmaker with expire_on_commit=False (critical for async)
3. Create `get_db_session()` dependency yielding AsyncSession with commit/rollback
4. Define `DBSession = Annotated[AsyncSession, Depends(get_db_session)]` type alias

Use `from sqlalchemy.dialects.postgresql import UUID as PGUUID` for UUID columns.
Use `datetime.now(timezone.utc)` for timestamp defaults (not deprecated utcnow).
  </action>
  <verify>
- `python -c "from src.storage.models import Base, Document, DocumentStatus, Borrower, IncomeRecord, AccountNumber, SourceReference"` succeeds
- `python -c "from src.storage.database import engine, async_session_factory, get_db_session, DBSession"` succeeds
- `ruff check backend/src/storage/` passes
  </verify>
  <done>
All SQLAlchemy models compile without errors, database module provides async session factory and dependency injection pattern.
  </done>
</task>

<task type="auto">
  <name>Task 2: Alembic async migration setup with initial schema</name>
  <files>
    backend/alembic.ini
    backend/alembic/env.py
    backend/alembic/script.py.mako
    backend/alembic/versions/001_initial_schema.py
  </files>
  <action>
Initialize Alembic with async template and create initial migration:

**Step 1: Initialize Alembic**
```bash
cd backend && alembic init -t async alembic
```
This creates the alembic/ directory with async template files.

**Step 2: Configure alembic.ini**
- Set `sqlalchemy.url` to use environment variable: `%(DATABASE_URL)s`
- Or set directly to: `postgresql+asyncpg://postgres:postgres@localhost:5432/loan_extraction`

**Step 3: Modify alembic/env.py**
Update the auto-generated env.py:
1. Import Base from src.storage.models
2. Set `target_metadata = Base.metadata`
3. Update database URL to read from settings:
```python
from src.config import settings
config.set_main_option("sqlalchemy.url", str(settings.database_url))
```
4. Keep the async migration functions from the template

**Step 4: Create initial migration**
Create `backend/alembic/versions/001_initial_schema.py` manually (not autogenerate, to control naming):

```python
"""Initial schema for document ingestion pipeline

Revision ID: 001
Revises:
Create Date: 2026-01-23

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision: str = '001'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Documents table
    op.create_table(
        'documents',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('filename', sa.String(255), nullable=False),
        sa.Column('file_hash', sa.String(64), nullable=False, unique=True, index=True),
        sa.Column('file_type', sa.String(10), nullable=False),
        sa.Column('file_size_bytes', sa.Integer(), nullable=False),
        sa.Column('gcs_uri', sa.String(500), nullable=True),
        sa.Column('status', sa.Enum('pending', 'processing', 'completed', 'failed', name='documentstatus'),
                  nullable=False, server_default='pending'),
        sa.Column('error_message', sa.Text(), nullable=True),
        sa.Column('page_count', sa.Integer(), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.Column('processed_at', sa.DateTime(timezone=True), nullable=True),
    )

    # Borrowers table
    op.create_table(
        'borrowers',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('name', sa.String(255), nullable=False),
        sa.Column('ssn_hash', sa.String(64), nullable=True, index=True),
        sa.Column('address_json', sa.Text(), nullable=True),
        sa.Column('confidence_score', sa.Numeric(3, 2), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
    )

    # Income records table
    op.create_table(
        'income_records',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('borrower_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('borrowers.id'), nullable=False),
        sa.Column('amount', sa.Numeric(12, 2), nullable=False),
        sa.Column('period', sa.String(20), nullable=False),
        sa.Column('year', sa.Integer(), nullable=False),
        sa.Column('source_type', sa.String(50), nullable=False),
        sa.Column('employer', sa.String(255), nullable=True),
    )

    # Account numbers table
    op.create_table(
        'account_numbers',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('borrower_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('borrowers.id'), nullable=False),
        sa.Column('number', sa.String(50), nullable=False),
        sa.Column('account_type', sa.String(20), nullable=False),
    )

    # Source references table (linking borrowers to documents)
    op.create_table(
        'source_references',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('borrower_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('borrowers.id'), nullable=False),
        sa.Column('document_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('documents.id'), nullable=False),
        sa.Column('page_number', sa.Integer(), nullable=False),
        sa.Column('section', sa.String(100), nullable=True),
        sa.Column('snippet', sa.Text(), nullable=False),
    )


def downgrade() -> None:
    op.drop_table('source_references')
    op.drop_table('account_numbers')
    op.drop_table('income_records')
    op.drop_table('borrowers')
    op.drop_table('documents')
    op.execute("DROP TYPE IF EXISTS documentstatus")
```
  </action>
  <verify>
- `cd backend && alembic --help` works (alembic installed)
- `cd backend && alembic history` shows the 001 migration
- Start PostgreSQL: `docker-compose up -d postgres`
- Run migration: `cd backend && alembic upgrade head`
- Verify tables created: `docker exec -it $(docker ps -qf name=postgres) psql -U postgres -d loan_extraction -c "\dt"`
  </verify>
  <done>
Alembic configured for async PostgreSQL, initial migration creates all 5 tables (documents, borrowers, income_records, account_numbers, source_references) with correct types and constraints.
  </done>
</task>

<task type="auto">
  <name>Task 3: DocumentRepository with async CRUD and unit tests</name>
  <files>
    backend/src/storage/repositories.py
    backend/tests/unit/test_repositories.py
  </files>
  <action>
Create DocumentRepository with async database operations:

**backend/src/storage/repositories.py:**

```python
"""Repository pattern for async database operations."""

from datetime import datetime, timezone
from typing import Sequence
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from src.storage.models import Document, DocumentStatus


class DocumentRepository:
    """Repository for Document database operations."""

    def __init__(self, session: AsyncSession) -> None:
        self.session = session

    async def create(self, document: Document) -> Document:
        """Create a new document record."""
        self.session.add(document)
        await self.session.flush()
        await self.session.refresh(document)
        return document

    async def get_by_id(self, document_id: UUID) -> Document | None:
        """Get document by ID."""
        result = await self.session.execute(
            select(Document).where(Document.id == document_id)
        )
        return result.scalar_one_or_none()

    async def get_by_hash(self, file_hash: str) -> Document | None:
        """Get document by file hash (for duplicate detection)."""
        result = await self.session.execute(
            select(Document).where(Document.file_hash == file_hash)
        )
        return result.scalar_one_or_none()

    async def update_status(
        self,
        document_id: UUID,
        status: DocumentStatus,
        error_message: str | None = None,
        page_count: int | None = None,
    ) -> Document | None:
        """Update document processing status."""
        document = await self.get_by_id(document_id)
        if document:
            document.status = status
            document.error_message = error_message
            if page_count is not None:
                document.page_count = page_count
            if status == DocumentStatus.COMPLETED:
                document.processed_at = datetime.now(timezone.utc)
            await self.session.flush()
        return document

    async def list_documents(
        self, limit: int = 100, offset: int = 0
    ) -> Sequence[Document]:
        """List documents with pagination."""
        result = await self.session.execute(
            select(Document)
            .order_by(Document.created_at.desc())
            .offset(offset)
            .limit(limit)
        )
        return result.scalars().all()

    async def list_pending(self, limit: int = 100) -> Sequence[Document]:
        """List documents pending processing."""
        result = await self.session.execute(
            select(Document)
            .where(Document.status == DocumentStatus.PENDING)
            .order_by(Document.created_at.asc())
            .limit(limit)
        )
        return result.scalars().all()
```

**backend/tests/unit/test_repositories.py:**

Create unit tests using pytest-asyncio with SQLite in-memory for fast testing:

```python
"""Unit tests for DocumentRepository."""

import pytest
from uuid import uuid4

from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine

from src.storage.models import Base, Document, DocumentStatus
from src.storage.repositories import DocumentRepository


@pytest.fixture
async def async_engine():
    """Create async SQLite engine for testing."""
    engine = create_async_engine(
        "sqlite+aiosqlite:///:memory:",
        echo=False,
    )
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    yield engine
    await engine.dispose()


@pytest.fixture
async def session(async_engine):
    """Create async session for testing."""
    session_factory = async_sessionmaker(
        async_engine,
        class_=AsyncSession,
        expire_on_commit=False,
    )
    async with session_factory() as session:
        yield session


@pytest.fixture
def sample_document() -> Document:
    """Create a sample document for testing."""
    return Document(
        id=uuid4(),
        filename="test.pdf",
        file_hash="abc123def456",
        file_type="pdf",
        file_size_bytes=1024,
        status=DocumentStatus.PENDING,
    )


class TestDocumentRepository:
    """Tests for DocumentRepository."""

    async def test_create_document(self, session: AsyncSession, sample_document: Document):
        """Test creating a document."""
        repo = DocumentRepository(session)
        created = await repo.create(sample_document)

        assert created.id == sample_document.id
        assert created.filename == "test.pdf"
        assert created.status == DocumentStatus.PENDING
        assert created.created_at is not None

    async def test_get_by_id(self, session: AsyncSession, sample_document: Document):
        """Test retrieving document by ID."""
        repo = DocumentRepository(session)
        await repo.create(sample_document)

        found = await repo.get_by_id(sample_document.id)
        assert found is not None
        assert found.filename == "test.pdf"

    async def test_get_by_id_not_found(self, session: AsyncSession):
        """Test retrieving non-existent document."""
        repo = DocumentRepository(session)
        found = await repo.get_by_id(uuid4())
        assert found is None

    async def test_get_by_hash(self, session: AsyncSession, sample_document: Document):
        """Test retrieving document by file hash."""
        repo = DocumentRepository(session)
        await repo.create(sample_document)

        found = await repo.get_by_hash("abc123def456")
        assert found is not None
        assert found.id == sample_document.id

    async def test_get_by_hash_not_found(self, session: AsyncSession):
        """Test retrieving non-existent hash."""
        repo = DocumentRepository(session)
        found = await repo.get_by_hash("nonexistent")
        assert found is None

    async def test_update_status_to_completed(self, session: AsyncSession, sample_document: Document):
        """Test updating document status to completed."""
        repo = DocumentRepository(session)
        await repo.create(sample_document)

        updated = await repo.update_status(
            sample_document.id,
            DocumentStatus.COMPLETED,
            page_count=5,
        )

        assert updated is not None
        assert updated.status == DocumentStatus.COMPLETED
        assert updated.page_count == 5
        assert updated.processed_at is not None

    async def test_update_status_to_failed(self, session: AsyncSession, sample_document: Document):
        """Test updating document status to failed with error."""
        repo = DocumentRepository(session)
        await repo.create(sample_document)

        updated = await repo.update_status(
            sample_document.id,
            DocumentStatus.FAILED,
            error_message="OCR failed",
        )

        assert updated is not None
        assert updated.status == DocumentStatus.FAILED
        assert updated.error_message == "OCR failed"

    async def test_list_documents_pagination(self, session: AsyncSession):
        """Test listing documents with pagination."""
        repo = DocumentRepository(session)

        # Create 5 documents
        for i in range(5):
            doc = Document(
                id=uuid4(),
                filename=f"doc{i}.pdf",
                file_hash=f"hash{i}",
                file_type="pdf",
                file_size_bytes=1000 + i,
                status=DocumentStatus.PENDING,
            )
            await repo.create(doc)

        # Get first page
        page1 = await repo.list_documents(limit=2, offset=0)
        assert len(page1) == 2

        # Get second page
        page2 = await repo.list_documents(limit=2, offset=2)
        assert len(page2) == 2

    async def test_list_pending(self, session: AsyncSession):
        """Test listing pending documents."""
        repo = DocumentRepository(session)

        # Create mix of statuses
        pending = Document(id=uuid4(), filename="pending.pdf", file_hash="h1",
                          file_type="pdf", file_size_bytes=100, status=DocumentStatus.PENDING)
        completed = Document(id=uuid4(), filename="done.pdf", file_hash="h2",
                            file_type="pdf", file_size_bytes=100, status=DocumentStatus.COMPLETED)

        await repo.create(pending)
        await repo.create(completed)

        pending_list = await repo.list_pending()
        assert len(pending_list) == 1
        assert pending_list[0].filename == "pending.pdf"
```

Also add aiosqlite to dev dependencies for testing:

Update backend/pyproject.toml [project.optional-dependencies] dev section to include:
```
"aiosqlite>=0.21.0",
```
  </action>
  <verify>
- `pip install aiosqlite` (if not already installed)
- `cd backend && python -m pytest tests/unit/test_repositories.py -v` - all tests pass
- Tests cover: create, get_by_id, get_by_hash, update_status (completed/failed), list_documents, list_pending
  </verify>
  <done>
DocumentRepository implements all async CRUD operations with comprehensive unit tests passing. Tests use SQLite in-memory for fast execution.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `docker-compose up -d postgres` starts PostgreSQL
2. `cd backend && alembic upgrade head` creates all tables
3. `docker exec -it $(docker ps -qf name=postgres) psql -U postgres -d loan_extraction -c "\dt"` shows 5 tables
4. `cd backend && python -m pytest tests/unit/test_repositories.py -v` - all tests pass
5. Import check: `python -c "from src.storage import Base, Document, DocumentRepository"`
</verification>

<success_criteria>
1. SQLAlchemy models (Document, Borrower, IncomeRecord, AccountNumber, SourceReference) import without errors
2. Database module provides async session factory and DBSession type alias
3. Alembic migration creates all 5 tables in PostgreSQL
4. file_hash column has unique constraint (duplicate detection foundation)
5. DocumentRepository async CRUD operations work (create, get_by_id, get_by_hash, update_status, list)
6. All repository unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-ingestion-pipeline/02-01-SUMMARY.md`
</output>
