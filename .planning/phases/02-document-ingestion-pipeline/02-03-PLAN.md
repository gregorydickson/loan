---
phase: 02-document-ingestion-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/src/ingestion/document_service.py
  - backend/src/api/__init__.py
  - backend/src/api/documents.py
  - backend/src/api/dependencies.py
  - backend/src/main.py
  - backend/tests/unit/test_document_service.py
  - backend/tests/integration/test_documents_api.py
  - backend/tests/integration/__init__.py
  - backend/tests/integration/conftest.py
autonomous: true

must_haves:
  truths:
    - "Uploading a file computes SHA-256 hash before database insert"
    - "Duplicate file uploads (same hash) are rejected with 409 Conflict"
    - "Successful upload creates document record with status PENDING"
    - "Successful upload stores file in GCS with gs:// URI"
    - "POST /api/documents accepts multipart file upload"
    - "Failed processing updates document status to FAILED with error message"
    - "Processing errors do not crash the application"
  artifacts:
    - path: "backend/src/ingestion/document_service.py"
      provides: "DocumentService orchestrating upload flow"
      exports: ["DocumentService", "DuplicateDocumentError"]
    - path: "backend/src/api/documents.py"
      provides: "Document upload API endpoint"
      exports: ["router"]
    - path: "backend/src/api/dependencies.py"
      provides: "FastAPI dependencies for GCS and services"
      exports: ["get_gcs_client", "get_document_service"]
  key_links:
    - from: "backend/src/ingestion/document_service.py"
      to: "backend/src/storage/repositories.py"
      via: "uses DocumentRepository"
      pattern: "DocumentRepository"
    - from: "backend/src/ingestion/document_service.py"
      to: "backend/src/storage/gcs_client.py"
      via: "uses GCSClient"
      pattern: "GCSClient"
    - from: "backend/src/api/documents.py"
      to: "backend/src/ingestion/document_service.py"
      via: "uses DocumentService"
      pattern: "DocumentService"
    - from: "backend/src/main.py"
      to: "backend/src/api/documents.py"
      via: "includes router"
      pattern: "include_router.*documents"
---

<objective>
Create the DocumentService and upload API endpoint for the document ingestion pipeline.

Purpose: Enable file uploads through the REST API with duplicate detection, GCS storage, and database tracking.

Output:
- DocumentService orchestrating upload/processing flow
- POST /api/documents endpoint accepting file uploads
- File hash computation for duplicate detection
- Error handling for duplicates and processing failures
- Unit tests for service layer
- Integration tests for API endpoint
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-ingestion-pipeline/02-RESEARCH.md
@backend/src/config.py
@backend/src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: DocumentService with upload orchestration</name>
  <files>
    backend/src/ingestion/document_service.py
    backend/tests/unit/test_document_service.py
  </files>
  <action>
Create DocumentService following patterns from 02-RESEARCH.md:

**backend/src/ingestion/document_service.py:**

```python
"""Document service orchestrating upload and processing flow.

Handles:
- File hash computation for duplicate detection
- GCS upload
- Database record creation
- Status tracking
"""

import hashlib
from datetime import datetime, timezone
from uuid import UUID, uuid4

from src.storage.gcs_client import GCSClient
from src.storage.models import Document, DocumentStatus
from src.storage.repositories import DocumentRepository


class DuplicateDocumentError(Exception):
    """Raised when a duplicate document is detected via file hash."""

    def __init__(self, existing_id: UUID, file_hash: str):
        self.existing_id = existing_id
        self.file_hash = file_hash
        super().__init__(f"Duplicate document exists with ID: {existing_id}")


class DocumentUploadError(Exception):
    """Raised when document upload fails."""

    pass


class DocumentService:
    """Orchestrates document upload and processing.

    Workflow:
    1. Compute file hash (SHA-256)
    2. Check for duplicate (reject if exists)
    3. Create database record (PENDING status)
    4. Upload to GCS
    5. Update record with GCS URI
    """

    # Mapping of MIME types to file type strings
    MIME_TYPE_MAP: dict[str, str] = {
        "application/pdf": "pdf",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
        "image/png": "png",
        "image/jpeg": "jpg",
        "image/jpg": "jpg",
    }

    ALLOWED_MIME_TYPES: set[str] = set(MIME_TYPE_MAP.keys())

    # Maximum file size: 50MB
    MAX_FILE_SIZE: int = 50 * 1024 * 1024

    def __init__(
        self,
        repository: DocumentRepository,
        gcs_client: GCSClient,
    ) -> None:
        """Initialize DocumentService.

        Args:
            repository: DocumentRepository for database operations
            gcs_client: GCSClient for file storage
        """
        self.repository = repository
        self.gcs_client = gcs_client

    @staticmethod
    def compute_file_hash(content: bytes) -> str:
        """Compute SHA-256 hash of file content.

        Args:
            content: File content as bytes

        Returns:
            Hex string of SHA-256 hash
        """
        return hashlib.sha256(content).hexdigest()

    def validate_file(
        self,
        content: bytes,
        content_type: str | None,
        filename: str,
    ) -> tuple[str, str]:
        """Validate uploaded file.

        Args:
            content: File content
            content_type: MIME type from upload
            filename: Original filename

        Returns:
            Tuple of (validated_content_type, file_type)

        Raises:
            ValueError: If file is invalid
        """
        # Check file size
        if len(content) > self.MAX_FILE_SIZE:
            max_mb = self.MAX_FILE_SIZE // (1024 * 1024)
            raise ValueError(f"File too large. Maximum size is {max_mb}MB")

        # Check content type
        if not content_type:
            # Try to infer from filename
            ext = filename.lower().rsplit(".", 1)[-1] if "." in filename else ""
            ext_to_mime = {
                "pdf": "application/pdf",
                "docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                "png": "image/png",
                "jpg": "image/jpeg",
                "jpeg": "image/jpeg",
            }
            content_type = ext_to_mime.get(ext, "")

        if content_type not in self.ALLOWED_MIME_TYPES:
            allowed = ", ".join(sorted(self.ALLOWED_MIME_TYPES))
            raise ValueError(f"Unsupported file type: {content_type}. Allowed: {allowed}")

        file_type = self.MIME_TYPE_MAP[content_type]
        return content_type, file_type

    async def upload(
        self,
        filename: str,
        content: bytes,
        content_type: str | None = None,
    ) -> Document:
        """Upload a document: validate, hash check, store, track.

        Args:
            filename: Original filename
            content: File content as bytes
            content_type: MIME type of the file

        Returns:
            Created Document record

        Raises:
            DuplicateDocumentError: If file with same hash exists
            ValueError: If file is invalid
            DocumentUploadError: If upload fails
        """
        # 1. Validate file
        content_type, file_type = self.validate_file(content, content_type, filename)

        # 2. Compute hash for duplicate detection
        file_hash = self.compute_file_hash(content)

        # 3. Check for existing document with same hash
        existing = await self.repository.get_by_hash(file_hash)
        if existing:
            raise DuplicateDocumentError(existing.id, file_hash)

        # 4. Create document record with PENDING status
        document_id = uuid4()
        document = Document(
            id=document_id,
            filename=filename,
            file_hash=file_hash,
            file_type=file_type,
            file_size_bytes=len(content),
            status=DocumentStatus.PENDING,
        )
        document = await self.repository.create(document)

        # 5. Upload to GCS
        try:
            gcs_path = f"documents/{document_id}/{filename}"
            gcs_uri = self.gcs_client.upload(content, gcs_path, content_type)

            # 6. Update document with GCS URI
            document.gcs_uri = gcs_uri
            await self.repository.session.flush()

        except Exception as e:
            # Mark as failed if GCS upload fails
            await self.repository.update_status(
                document_id,
                DocumentStatus.FAILED,
                error_message=f"GCS upload failed: {e}",
            )
            raise DocumentUploadError(f"Failed to upload to storage: {e}") from e

        return document

    async def get_document(self, document_id: UUID) -> Document | None:
        """Get document by ID.

        Args:
            document_id: Document UUID

        Returns:
            Document if found, None otherwise
        """
        return await self.repository.get_by_id(document_id)

    async def update_processing_result(
        self,
        document_id: UUID,
        success: bool,
        page_count: int | None = None,
        error_message: str | None = None,
    ) -> Document | None:
        """Update document after processing completes.

        Args:
            document_id: Document UUID
            success: Whether processing succeeded
            page_count: Number of pages (if successful)
            error_message: Error message (if failed)

        Returns:
            Updated document
        """
        status = DocumentStatus.COMPLETED if success else DocumentStatus.FAILED
        return await self.repository.update_status(
            document_id,
            status,
            error_message=error_message,
            page_count=page_count,
        )
```

**Update backend/src/ingestion/__init__.py:**
```python
"""Document ingestion module for processing uploaded files."""

from src.ingestion.docling_processor import (
    DoclingProcessor,
    DocumentContent,
    DocumentProcessingError,
    PageContent,
)
from src.ingestion.document_service import (
    DocumentService,
    DocumentUploadError,
    DuplicateDocumentError,
)

__all__ = [
    "DoclingProcessor",
    "DocumentContent",
    "DocumentProcessingError",
    "PageContent",
    "DocumentService",
    "DocumentUploadError",
    "DuplicateDocumentError",
]
```

**backend/tests/unit/test_document_service.py:**

```python
"""Unit tests for DocumentService."""

import pytest
from uuid import uuid4
from unittest.mock import AsyncMock, MagicMock, patch

from src.ingestion.document_service import (
    DocumentService,
    DuplicateDocumentError,
    DocumentUploadError,
)
from src.storage.models import Document, DocumentStatus


class TestDocumentServiceValidation:
    """Tests for DocumentService validation."""

    def test_compute_file_hash(self):
        """Test SHA-256 hash computation."""
        content = b"test content"
        hash1 = DocumentService.compute_file_hash(content)
        hash2 = DocumentService.compute_file_hash(content)

        assert hash1 == hash2
        assert len(hash1) == 64  # SHA-256 produces 64 hex chars

    def test_compute_file_hash_different_content(self):
        """Test that different content produces different hash."""
        hash1 = DocumentService.compute_file_hash(b"content 1")
        hash2 = DocumentService.compute_file_hash(b"content 2")
        assert hash1 != hash2

    def test_validate_file_pdf(self):
        """Test PDF validation."""
        service = DocumentService(
            repository=MagicMock(),
            gcs_client=MagicMock(),
        )
        content_type, file_type = service.validate_file(
            b"pdf content",
            "application/pdf",
            "test.pdf",
        )
        assert content_type == "application/pdf"
        assert file_type == "pdf"

    def test_validate_file_docx(self):
        """Test DOCX validation."""
        service = DocumentService(
            repository=MagicMock(),
            gcs_client=MagicMock(),
        )
        docx_mime = "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        content_type, file_type = service.validate_file(
            b"docx content",
            docx_mime,
            "test.docx",
        )
        assert file_type == "docx"

    def test_validate_file_infer_from_filename(self):
        """Test content type inference from filename."""
        service = DocumentService(
            repository=MagicMock(),
            gcs_client=MagicMock(),
        )
        content_type, file_type = service.validate_file(
            b"pdf content",
            None,  # No content type provided
            "document.pdf",
        )
        assert content_type == "application/pdf"
        assert file_type == "pdf"

    def test_validate_file_unsupported_type(self):
        """Test rejection of unsupported file types."""
        service = DocumentService(
            repository=MagicMock(),
            gcs_client=MagicMock(),
        )
        with pytest.raises(ValueError, match="Unsupported file type"):
            service.validate_file(
                b"content",
                "text/plain",
                "test.txt",
            )

    def test_validate_file_too_large(self):
        """Test rejection of oversized files."""
        service = DocumentService(
            repository=MagicMock(),
            gcs_client=MagicMock(),
        )
        large_content = b"x" * (51 * 1024 * 1024)  # 51MB
        with pytest.raises(ValueError, match="File too large"):
            service.validate_file(
                large_content,
                "application/pdf",
                "large.pdf",
            )


class TestDocumentServiceUpload:
    """Tests for DocumentService.upload method."""

    @pytest.fixture
    def mock_repository(self):
        """Create mock repository."""
        repo = AsyncMock()
        repo.get_by_hash = AsyncMock(return_value=None)
        repo.create = AsyncMock()
        repo.session = AsyncMock()
        repo.session.flush = AsyncMock()
        return repo

    @pytest.fixture
    def mock_gcs_client(self):
        """Create mock GCS client."""
        client = MagicMock()
        client.upload = MagicMock(return_value="gs://bucket/documents/id/file.pdf")
        return client

    @pytest.mark.asyncio
    async def test_upload_success(self, mock_repository, mock_gcs_client):
        """Test successful document upload."""
        # Setup mock to return the created document
        created_doc = Document(
            id=uuid4(),
            filename="test.pdf",
            file_hash="abc123",
            file_type="pdf",
            file_size_bytes=100,
            status=DocumentStatus.PENDING,
        )
        mock_repository.create.return_value = created_doc

        service = DocumentService(
            repository=mock_repository,
            gcs_client=mock_gcs_client,
        )

        result = await service.upload(
            filename="test.pdf",
            content=b"pdf content",
            content_type="application/pdf",
        )

        assert result.filename == "test.pdf"
        assert result.status == DocumentStatus.PENDING
        mock_repository.get_by_hash.assert_called_once()
        mock_repository.create.assert_called_once()
        mock_gcs_client.upload.assert_called_once()

    @pytest.mark.asyncio
    async def test_upload_duplicate_rejected(self, mock_repository, mock_gcs_client):
        """Test that duplicate file is rejected."""
        existing_doc = Document(
            id=uuid4(),
            filename="existing.pdf",
            file_hash="samehash",
            file_type="pdf",
            file_size_bytes=100,
            status=DocumentStatus.COMPLETED,
        )
        mock_repository.get_by_hash.return_value = existing_doc

        service = DocumentService(
            repository=mock_repository,
            gcs_client=mock_gcs_client,
        )

        with pytest.raises(DuplicateDocumentError) as exc_info:
            await service.upload(
                filename="duplicate.pdf",
                content=b"same content",
                content_type="application/pdf",
            )

        assert exc_info.value.existing_id == existing_doc.id
        mock_repository.create.assert_not_called()
        mock_gcs_client.upload.assert_not_called()

    @pytest.mark.asyncio
    async def test_upload_gcs_failure_marks_failed(self, mock_repository, mock_gcs_client):
        """Test that GCS upload failure marks document as FAILED."""
        created_doc = Document(
            id=uuid4(),
            filename="test.pdf",
            file_hash="abc123",
            file_type="pdf",
            file_size_bytes=100,
            status=DocumentStatus.PENDING,
        )
        mock_repository.create.return_value = created_doc
        mock_gcs_client.upload.side_effect = Exception("GCS error")

        service = DocumentService(
            repository=mock_repository,
            gcs_client=mock_gcs_client,
        )

        with pytest.raises(DocumentUploadError, match="Failed to upload"):
            await service.upload(
                filename="test.pdf",
                content=b"content",
                content_type="application/pdf",
            )

        # Verify status was updated to FAILED
        mock_repository.update_status.assert_called_once()
        call_args = mock_repository.update_status.call_args
        assert call_args[0][1] == DocumentStatus.FAILED


class TestDocumentServiceProcessing:
    """Tests for DocumentService processing methods."""

    @pytest.mark.asyncio
    async def test_update_processing_success(self):
        """Test updating document after successful processing."""
        mock_repository = AsyncMock()
        updated_doc = Document(
            id=uuid4(),
            filename="test.pdf",
            file_hash="abc",
            file_type="pdf",
            file_size_bytes=100,
            status=DocumentStatus.COMPLETED,
            page_count=5,
        )
        mock_repository.update_status.return_value = updated_doc

        service = DocumentService(
            repository=mock_repository,
            gcs_client=MagicMock(),
        )

        result = await service.update_processing_result(
            document_id=updated_doc.id,
            success=True,
            page_count=5,
        )

        assert result.status == DocumentStatus.COMPLETED
        mock_repository.update_status.assert_called_once_with(
            updated_doc.id,
            DocumentStatus.COMPLETED,
            error_message=None,
            page_count=5,
        )

    @pytest.mark.asyncio
    async def test_update_processing_failure(self):
        """Test updating document after failed processing."""
        mock_repository = AsyncMock()
        doc_id = uuid4()
        mock_repository.update_status.return_value = Document(
            id=doc_id,
            filename="test.pdf",
            file_hash="abc",
            file_type="pdf",
            file_size_bytes=100,
            status=DocumentStatus.FAILED,
            error_message="OCR failed",
        )

        service = DocumentService(
            repository=mock_repository,
            gcs_client=MagicMock(),
        )

        result = await service.update_processing_result(
            document_id=doc_id,
            success=False,
            error_message="OCR failed",
        )

        assert result.status == DocumentStatus.FAILED
        mock_repository.update_status.assert_called_once_with(
            doc_id,
            DocumentStatus.FAILED,
            error_message="OCR failed",
            page_count=None,
        )
```
  </action>
  <verify>
- `python -c "from src.ingestion import DocumentService, DuplicateDocumentError"` succeeds
- `cd backend && python -m pytest tests/unit/test_document_service.py -v` - all tests pass
- `ruff check backend/src/ingestion/document_service.py` passes
  </verify>
  <done>
DocumentService orchestrates upload flow with hash computation, duplicate detection, GCS storage, and database tracking. Errors are handled gracefully with proper status updates. All unit tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: API dependencies and document upload endpoint</name>
  <files>
    backend/src/api/__init__.py
    backend/src/api/dependencies.py
    backend/src/api/documents.py
    backend/src/main.py
  </files>
  <action>
Create API module with dependencies and document upload endpoint:

**backend/src/api/__init__.py:**
```python
"""API module with REST endpoints."""

from src.api.documents import router as documents_router

__all__ = ["documents_router"]
```

**backend/src/api/dependencies.py:**
```python
"""FastAPI dependencies for service injection."""

from typing import Annotated, AsyncGenerator

from fastapi import Depends

from src.config import settings
from src.storage.database import DBSession, get_db_session
from src.storage.gcs_client import GCSClient
from src.storage.repositories import DocumentRepository
from src.ingestion.document_service import DocumentService


# GCS Client dependency
_gcs_client: GCSClient | None = None


def get_gcs_client() -> GCSClient:
    """Get or create GCS client singleton.

    Returns:
        GCSClient instance

    Note:
        Returns a mock client if GCS bucket is not configured.
        This allows running locally without GCS.
    """
    global _gcs_client

    if _gcs_client is None:
        bucket_name = settings.gcs_bucket
        if not bucket_name:
            # For local development without GCS, use a mock
            from unittest.mock import MagicMock
            _gcs_client = MagicMock(spec=GCSClient)
            _gcs_client.upload = MagicMock(return_value="gs://mock-bucket/mock-path")
            _gcs_client.download = MagicMock(return_value=b"mock content")
            _gcs_client.exists = MagicMock(return_value=True)
        else:
            _gcs_client = GCSClient(bucket_name)

    return _gcs_client


GCSClientDep = Annotated[GCSClient, Depends(get_gcs_client)]


def get_document_repository(session: DBSession) -> DocumentRepository:
    """Get document repository with session."""
    return DocumentRepository(session)


DocumentRepoDep = Annotated[DocumentRepository, Depends(get_document_repository)]


def get_document_service(
    repository: DocumentRepoDep,
    gcs_client: GCSClientDep,
) -> DocumentService:
    """Get document service with dependencies."""
    return DocumentService(
        repository=repository,
        gcs_client=gcs_client,
    )


DocumentServiceDep = Annotated[DocumentService, Depends(get_document_service)]
```

**backend/src/api/documents.py:**
```python
"""Document API endpoints."""

from uuid import UUID

from fastapi import APIRouter, HTTPException, UploadFile, status
from pydantic import BaseModel, Field

from src.api.dependencies import DocumentServiceDep, DocumentRepoDep
from src.ingestion.document_service import DuplicateDocumentError, DocumentUploadError
from src.storage.models import DocumentStatus

router = APIRouter(prefix="/api/documents", tags=["documents"])


class DocumentUploadResponse(BaseModel):
    """Response for document upload."""

    id: UUID = Field(..., description="Document ID")
    filename: str = Field(..., description="Original filename")
    file_hash: str = Field(..., description="SHA-256 hash of file")
    file_size_bytes: int = Field(..., description="File size in bytes")
    status: str = Field(..., description="Processing status")
    message: str = Field(..., description="Status message")


class DocumentResponse(BaseModel):
    """Response for document details."""

    id: UUID
    filename: str
    file_hash: str
    file_type: str
    file_size_bytes: int
    gcs_uri: str | None
    status: str
    error_message: str | None
    page_count: int | None


class DocumentListResponse(BaseModel):
    """Response for document list."""

    documents: list[DocumentResponse]
    total: int
    limit: int
    offset: int


@router.post(
    "/",
    response_model=DocumentUploadResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Upload a document",
    description="Upload a document for processing. Supports PDF, DOCX, PNG, and JPG files.",
)
async def upload_document(
    file: UploadFile,
    service: DocumentServiceDep,
) -> DocumentUploadResponse:
    """Upload a document for processing.

    Args:
        file: Uploaded file (multipart form)
        service: DocumentService (injected)

    Returns:
        DocumentUploadResponse with document ID and status

    Raises:
        400: Invalid file type or size
        409: Duplicate file (same hash exists)
        500: Upload failed
    """
    try:
        # Read file content
        content = await file.read()

        # Upload document
        document = await service.upload(
            filename=file.filename or "unknown",
            content=content,
            content_type=file.content_type,
        )

        return DocumentUploadResponse(
            id=document.id,
            filename=document.filename,
            file_hash=document.file_hash,
            file_size_bytes=document.file_size_bytes,
            status=document.status.value,
            message="Document uploaded and queued for processing",
        )

    except ValueError as e:
        # Validation errors (file type, size)
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )

    except DuplicateDocumentError as e:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail={
                "message": "Duplicate document detected",
                "existing_id": str(e.existing_id),
                "file_hash": e.file_hash,
            },
        )

    except DocumentUploadError as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

    finally:
        await file.close()


@router.get(
    "/{document_id}",
    response_model=DocumentResponse,
    summary="Get document by ID",
)
async def get_document(
    document_id: UUID,
    service: DocumentServiceDep,
) -> DocumentResponse:
    """Get document details by ID.

    Args:
        document_id: Document UUID
        service: DocumentService (injected)

    Returns:
        DocumentResponse with document details

    Raises:
        404: Document not found
    """
    document = await service.get_document(document_id)
    if not document:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Document not found: {document_id}",
        )

    return DocumentResponse(
        id=document.id,
        filename=document.filename,
        file_hash=document.file_hash,
        file_type=document.file_type,
        file_size_bytes=document.file_size_bytes,
        gcs_uri=document.gcs_uri,
        status=document.status.value,
        error_message=document.error_message,
        page_count=document.page_count,
    )


@router.get(
    "/",
    response_model=DocumentListResponse,
    summary="List documents",
)
async def list_documents(
    repository: DocumentRepoDep,
    limit: int = 100,
    offset: int = 0,
) -> DocumentListResponse:
    """List documents with pagination.

    Args:
        repository: DocumentRepository (injected)
        limit: Maximum documents to return (default 100)
        offset: Number of documents to skip (default 0)

    Returns:
        DocumentListResponse with documents and pagination info
    """
    documents = await repository.list_documents(limit=limit, offset=offset)

    return DocumentListResponse(
        documents=[
            DocumentResponse(
                id=doc.id,
                filename=doc.filename,
                file_hash=doc.file_hash,
                file_type=doc.file_type,
                file_size_bytes=doc.file_size_bytes,
                gcs_uri=doc.gcs_uri,
                status=doc.status.value,
                error_message=doc.error_message,
                page_count=doc.page_count,
            )
            for doc in documents
        ],
        total=len(documents),  # Simplified - would need count query for true total
        limit=limit,
        offset=offset,
    )
```

**Update backend/src/main.py** to include the router:
```python
"""FastAPI application entry point.

Provides the main application instance with lifespan management
for database connections and other resources.
"""

from contextlib import asynccontextmanager
from typing import AsyncGenerator

from fastapi import FastAPI

from src.config import settings
from src.api.documents import router as documents_router


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Manage application lifespan - startup and shutdown.

    Startup: Initialize database connection pool, verify connectivity.
    Shutdown: Dispose connection pool, clean up resources.
    """
    # Startup
    if settings.debug:
        print(f"Starting application on {settings.api_host}:{settings.api_port}")

    yield  # Application runs

    # Shutdown
    if settings.debug:
        print("Shutting down application")


app = FastAPI(
    title="Loan Document Extraction API",
    description="Extract borrower data from loan documents with source attribution",
    version="0.1.0",
    lifespan=lifespan,
)

# Include routers
app.include_router(documents_router)


@app.get("/health")
async def health_check() -> dict[str, str]:
    """Health check endpoint for load balancers and monitoring."""
    return {"status": "healthy"}
```
  </action>
  <verify>
- `python -c "from src.api import documents_router"` succeeds
- `python -c "from src.api.dependencies import get_document_service"` succeeds
- `cd backend && uvicorn src.main:app --host 0.0.0.0 --port 8000 &` starts without errors
- `curl http://localhost:8000/health` returns {"status": "healthy"}
- `curl http://localhost:8000/docs` shows OpenAPI documentation with /api/documents endpoints
- `ruff check backend/src/api/` passes
  </verify>
  <done>
API module created with POST /api/documents (upload), GET /api/documents/{id} (detail), GET /api/documents (list). Dependencies provide GCS client and DocumentService injection. OpenAPI docs available at /docs.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integration tests for document upload API</name>
  <files>
    backend/tests/integration/__init__.py
    backend/tests/integration/conftest.py
    backend/tests/integration/test_documents_api.py
  </files>
  <action>
Create integration tests for the document API:

**backend/tests/integration/__init__.py:**
```python
"""Integration tests for API endpoints."""
```

**backend/tests/integration/conftest.py:**
```python
"""Fixtures for integration tests."""

import pytest
from httpx import AsyncClient, ASGITransport
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from unittest.mock import MagicMock

from src.main import app
from src.storage.models import Base
from src.storage.database import get_db_session
from src.api.dependencies import get_gcs_client


@pytest.fixture
async def async_engine():
    """Create async SQLite engine for testing."""
    engine = create_async_engine(
        "sqlite+aiosqlite:///:memory:",
        echo=False,
    )
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    yield engine
    await engine.dispose()


@pytest.fixture
async def db_session(async_engine):
    """Create async session for testing."""
    session_factory = async_sessionmaker(
        async_engine,
        class_=AsyncSession,
        expire_on_commit=False,
    )
    async with session_factory() as session:
        yield session


@pytest.fixture
def mock_gcs_client():
    """Create mock GCS client."""
    from src.storage.gcs_client import GCSClient

    client = MagicMock(spec=GCSClient)
    client.upload = MagicMock(return_value="gs://test-bucket/documents/test/file.pdf")
    client.download = MagicMock(return_value=b"file content")
    client.exists = MagicMock(return_value=True)
    return client


@pytest.fixture
async def client(async_engine, db_session, mock_gcs_client):
    """Create test client with mocked dependencies."""
    session_factory = async_sessionmaker(
        async_engine,
        class_=AsyncSession,
        expire_on_commit=False,
    )

    async def override_get_db_session():
        async with session_factory() as session:
            try:
                yield session
                await session.commit()
            except Exception:
                await session.rollback()
                raise

    def override_get_gcs_client():
        return mock_gcs_client

    app.dependency_overrides[get_db_session] = override_get_db_session
    app.dependency_overrides[get_gcs_client] = override_get_gcs_client

    transport = ASGITransport(app=app)
    async with AsyncClient(transport=transport, base_url="http://test") as client:
        yield client

    app.dependency_overrides.clear()
```

**backend/tests/integration/test_documents_api.py:**
```python
"""Integration tests for document API endpoints."""

import pytest
from httpx import AsyncClient


class TestDocumentUpload:
    """Tests for POST /api/documents."""

    @pytest.mark.asyncio
    async def test_upload_pdf_success(self, client: AsyncClient):
        """Test successful PDF upload."""
        files = {
            "file": ("test.pdf", b"%PDF-1.4 test content", "application/pdf")
        }

        response = await client.post("/api/documents/", files=files)

        assert response.status_code == 201
        data = response.json()
        assert data["filename"] == "test.pdf"
        assert data["status"] == "pending"
        assert "id" in data
        assert "file_hash" in data
        assert len(data["file_hash"]) == 64  # SHA-256

    @pytest.mark.asyncio
    async def test_upload_docx_success(self, client: AsyncClient):
        """Test successful DOCX upload."""
        docx_mime = "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        files = {
            "file": ("document.docx", b"docx content", docx_mime)
        }

        response = await client.post("/api/documents/", files=files)

        assert response.status_code == 201
        data = response.json()
        assert data["filename"] == "document.docx"

    @pytest.mark.asyncio
    async def test_upload_image_success(self, client: AsyncClient):
        """Test successful image upload (PNG)."""
        files = {
            "file": ("scan.png", b"\x89PNG\r\n\x1a\n fake png", "image/png")
        }

        response = await client.post("/api/documents/", files=files)

        assert response.status_code == 201
        data = response.json()
        assert data["filename"] == "scan.png"

    @pytest.mark.asyncio
    async def test_upload_unsupported_type_rejected(self, client: AsyncClient):
        """Test that unsupported file types are rejected."""
        files = {
            "file": ("readme.txt", b"text content", "text/plain")
        }

        response = await client.post("/api/documents/", files=files)

        assert response.status_code == 400
        assert "Unsupported file type" in response.json()["detail"]

    @pytest.mark.asyncio
    async def test_upload_duplicate_rejected(self, client: AsyncClient):
        """Test that duplicate uploads are rejected."""
        content = b"%PDF-1.4 duplicate test"
        files = {
            "file": ("first.pdf", content, "application/pdf")
        }

        # First upload succeeds
        response1 = await client.post("/api/documents/", files=files)
        assert response1.status_code == 201

        # Second upload with same content fails
        files2 = {
            "file": ("second.pdf", content, "application/pdf")
        }
        response2 = await client.post("/api/documents/", files=files2)

        assert response2.status_code == 409
        data = response2.json()["detail"]
        assert data["message"] == "Duplicate document detected"
        assert "existing_id" in data


class TestDocumentGet:
    """Tests for GET /api/documents/{id}."""

    @pytest.mark.asyncio
    async def test_get_document_success(self, client: AsyncClient):
        """Test getting document by ID."""
        # First upload a document
        files = {
            "file": ("test.pdf", b"%PDF get test", "application/pdf")
        }
        upload_response = await client.post("/api/documents/", files=files)
        document_id = upload_response.json()["id"]

        # Then get it by ID
        response = await client.get(f"/api/documents/{document_id}")

        assert response.status_code == 200
        data = response.json()
        assert data["id"] == document_id
        assert data["filename"] == "test.pdf"
        assert data["file_type"] == "pdf"

    @pytest.mark.asyncio
    async def test_get_document_not_found(self, client: AsyncClient):
        """Test getting non-existent document."""
        fake_id = "00000000-0000-0000-0000-000000000000"
        response = await client.get(f"/api/documents/{fake_id}")

        assert response.status_code == 404
        assert "not found" in response.json()["detail"].lower()


class TestDocumentList:
    """Tests for GET /api/documents."""

    @pytest.mark.asyncio
    async def test_list_documents_empty(self, client: AsyncClient):
        """Test listing documents when none exist."""
        response = await client.get("/api/documents/")

        assert response.status_code == 200
        data = response.json()
        assert data["documents"] == []
        assert data["total"] == 0

    @pytest.mark.asyncio
    async def test_list_documents_with_data(self, client: AsyncClient):
        """Test listing documents after uploads."""
        # Upload two documents
        for i in range(2):
            files = {
                "file": (f"doc{i}.pdf", f"content {i}".encode(), "application/pdf")
            }
            await client.post("/api/documents/", files=files)

        response = await client.get("/api/documents/")

        assert response.status_code == 200
        data = response.json()
        assert len(data["documents"]) == 2

    @pytest.mark.asyncio
    async def test_list_documents_pagination(self, client: AsyncClient):
        """Test document list pagination."""
        # Upload 3 documents
        for i in range(3):
            files = {
                "file": (f"page{i}.pdf", f"page content {i}".encode(), "application/pdf")
            }
            await client.post("/api/documents/", files=files)

        # Get first page
        response = await client.get("/api/documents/?limit=2&offset=0")
        data = response.json()
        assert len(data["documents"]) == 2
        assert data["limit"] == 2
        assert data["offset"] == 0

        # Get second page
        response = await client.get("/api/documents/?limit=2&offset=2")
        data = response.json()
        assert len(data["documents"]) == 1


class TestHealthCheck:
    """Tests for health check endpoint."""

    @pytest.mark.asyncio
    async def test_health_check(self, client: AsyncClient):
        """Test health check returns healthy."""
        response = await client.get("/health")
        assert response.status_code == 200
        assert response.json() == {"status": "healthy"}
```
  </action>
  <verify>
- `cd backend && python -m pytest tests/integration/test_documents_api.py -v` - all tests pass
- Tests cover: upload success (PDF, DOCX, PNG), upload rejection (unsupported type, duplicate), get document, get not found, list documents, pagination
  </verify>
  <done>
Integration tests cover all document API endpoints with proper database and GCS mocking. Tests verify upload flow, duplicate detection, document retrieval, and listing with pagination.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `docker-compose up -d postgres` - PostgreSQL running
2. `cd backend && alembic upgrade head` - migrations applied
3. `cd backend && uvicorn src.main:app --reload &` - API running
4. `curl http://localhost:8000/docs` - OpenAPI docs show /api/documents endpoints
5. `cd backend && python -m pytest tests/ -v` - all tests pass (unit + integration)
6. Test upload:
   ```bash
   curl -X POST http://localhost:8000/api/documents/ \
     -F "file=@test.pdf;type=application/pdf"
   ```
   Returns 201 with document ID
7. Test duplicate rejection (upload same file again) - returns 409
</verification>

<success_criteria>
1. POST /api/documents accepts multipart file upload
2. SHA-256 hash computed before database insert
3. Duplicate files (same hash) rejected with 409 Conflict
4. Successful upload creates document record with status PENDING
5. Successful upload stores file in GCS (or mock) with gs:// URI
6. GET /api/documents/{id} returns document details
7. GET /api/documents returns paginated list
8. Failed operations return appropriate HTTP status codes
9. All unit tests pass
10. All integration tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-ingestion-pipeline/02-03-SUMMARY.md`
</output>
