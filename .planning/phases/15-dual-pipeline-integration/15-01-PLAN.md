---
phase: 15-dual-pipeline-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/storage/models.py
  - backend/alembic/versions/003_add_extraction_metadata.py
  - backend/src/api/documents.py
autonomous: true

must_haves:
  truths:
    - "User can specify ?method=docling|langextract via API query parameter"
    - "User can specify ?ocr=auto|force|skip via API query parameter"
    - "Default method=docling preserves v1.0 backward compatibility"
    - "Document records track which extraction method was used"
    - "Document records track whether OCR was applied"
  artifacts:
    - path: "backend/src/storage/models.py"
      provides: "extraction_method and ocr_processed columns on Document"
      contains: "extraction_method.*Mapped.*String"
    - path: "backend/alembic/versions/003_add_extraction_metadata.py"
      provides: "Database migration for new columns"
      contains: "revision.*003"
    - path: "backend/src/api/documents.py"
      provides: "Query parameters for method and ocr selection"
      contains: "method.*ExtractionMethod"
  key_links:
    - from: "backend/src/api/documents.py"
      to: "backend/src/storage/models.py"
      via: "Document model usage"
      pattern: "Document"
---

<objective>
Add database schema and API query parameters for dual pipeline selection.

Purpose: Enable users to select extraction method (docling/langextract) and OCR mode (auto/force/skip) via API parameters. Track which method was used on each document for observability.

Output: Document model with extraction metadata columns, Alembic migration, and API endpoint with query parameters.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-dual-pipeline-integration/15-RESEARCH.md

# Existing source files
@backend/src/storage/models.py
@backend/alembic/versions/002_add_char_offsets.py
@backend/src/api/documents.py
@backend/src/extraction/extraction_router.py
@backend/src/ocr/ocr_router.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add extraction metadata to Document model and create migration</name>
  <files>
    backend/src/storage/models.py
    backend/alembic/versions/003_add_extraction_metadata.py
  </files>
  <action>
1. Update Document model in models.py to add two nullable columns:
   - extraction_method: Mapped[str | None] = mapped_column(String(20), nullable=True)
     # Values: "docling", "langextract", or null (legacy/pre-v2.0 documents)
   - ocr_processed: Mapped[bool | None] = mapped_column(Boolean, nullable=True)
     # Values: True if OCR applied, False if skipped, null (legacy)

2. Add Boolean import from sqlalchemy if not present

3. Create Alembic migration 003_add_extraction_metadata.py:
   - revision: str = "003"
   - down_revision: str | None = "002"
   - Add both columns to documents table (nullable=True for backward compatibility)
   - Downgrade drops both columns

Follow the pattern from 002_add_char_offsets.py for migration structure.

Note: Columns must be nullable because:
- Existing documents won't have values (null = legacy)
- Don't want NOT NULL constraint that breaks existing rows
  </action>
  <verify>
    cd /Users/gregorydickson/stackpoint/loan/backend && python -c "from src.storage.models import Document; print('Model OK'); d = Document.__table__.columns; print([c.name for c in d if 'extraction' in c.name or 'ocr' in c.name])"
  </verify>
  <done>
    Document model has extraction_method (String(20), nullable) and ocr_processed (Boolean, nullable) columns. Migration 003 exists and follows project conventions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add method and ocr query parameters to upload endpoint</name>
  <files>
    backend/src/api/documents.py
  </files>
  <action>
1. Add type aliases at module level (import from existing routers or define locally):
   ```python
   from typing import Literal

   ExtractionMethod = Literal["docling", "langextract", "auto"]
   OCRMode = Literal["auto", "force", "skip"]
   ```

2. Update upload_document endpoint signature to add query parameters:
   ```python
   async def upload_document(
       file: UploadFile,
       method: ExtractionMethod = "docling",  # Default preserves v1.0 behavior (DUAL-09)
       ocr: OCRMode = "auto",
       service: DocumentServiceDep,
   ) -> DocumentUploadResponse:
   ```

3. Pass parameters to service.upload() call:
   ```python
   document = await service.upload(
       filename=file.filename or "unknown",
       content=content,
       content_type=file.content_type,
       extraction_method=method,
       ocr_mode=ocr,
   )
   ```

4. Update DocumentUploadResponse to include extraction metadata:
   - Add extraction_method: str | None field
   - Add ocr_processed: bool | None field
   - Populate from document in response

5. Update endpoint docstring to document new parameters

Note: Default method="docling" is critical for backward compatibility (DUAL-09). Existing clients that don't send parameters will get the same v1.0 behavior.
  </action>
  <verify>
    cd /Users/gregorydickson/stackpoint/loan/backend && python -c "from src.api.documents import router, ExtractionMethod, OCRMode; print('Types imported'); print('Endpoint params will be verified by FastAPI')"
  </verify>
  <done>
    Upload endpoint accepts ?method=docling|langextract|auto and ?ocr=auto|force|skip query parameters. Default method=docling preserves backward compatibility.
  </done>
</task>

</tasks>

<verification>
1. Model verification:
   ```bash
   cd /Users/gregorydickson/stackpoint/loan/backend
   python -c "from src.storage.models import Document; print([c.name for c in Document.__table__.columns])"
   ```
   Should include extraction_method and ocr_processed

2. Migration verification:
   ```bash
   ls -la /Users/gregorydickson/stackpoint/loan/backend/alembic/versions/003*.py
   ```
   Should show 003_add_extraction_metadata.py

3. API parameter verification:
   ```bash
   cd /Users/gregorydickson/stackpoint/loan/backend
   python -c "
   from fastapi.testclient import TestClient
   from src.main import app
   # Just verify the app compiles with new endpoint signature
   print('App OK')
   "
   ```

4. Type alias verification:
   ```bash
   cd /Users/gregorydickson/stackpoint/loan/backend
   python -c "from src.api.documents import ExtractionMethod, OCRMode; print(f'ExtractionMethod: {ExtractionMethod}'); print(f'OCRMode: {OCRMode}')"
   ```
</verification>

<success_criteria>
- Document model has extraction_method (String(20), nullable) column
- Document model has ocr_processed (Boolean, nullable) column
- Alembic migration 003 exists with upgrade/downgrade functions
- Upload endpoint accepts method query parameter (docling|langextract|auto)
- Upload endpoint accepts ocr query parameter (auto|force|skip)
- Default method=docling ensures backward compatibility
- DocumentUploadResponse includes extraction metadata fields
- All imports and type definitions valid (no import errors)
</success_criteria>

<output>
After completion, create `.planning/phases/15-dual-pipeline-integration/15-01-SUMMARY.md`
</output>
