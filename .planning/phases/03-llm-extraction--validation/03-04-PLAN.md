---
phase: 03-llm-extraction-validation
plan: 04
type: execute
wave: 2
depends_on: ["03-01", "03-02", "03-03"]
files_modified:
  - backend/src/extraction/schemas.py
  - backend/src/extraction/deduplication.py
  - backend/src/extraction/extractor.py
  - backend/src/extraction/__init__.py
  - backend/tests/extraction/test_deduplication.py
  - backend/tests/extraction/test_extractor.py
autonomous: true

must_haves:
  truths:
    - "Extractor assesses document complexity before calling LLM"
    - "Simple documents routed to Flash model, complex to Pro model"
    - "Large documents chunked before extraction"
    - "Results from multiple chunks aggregated into single borrower list"
    - "Duplicate borrowers merged using SSN, account match, or fuzzy name"
    - "Each extracted borrower has source attribution (document_id, page_number, snippet)"
    - "Confidence scores calculated for all extracted borrowers"
    - "Validation errors tracked per borrower"
  artifacts:
    - path: "backend/src/extraction/schemas.py"
      provides: "Pydantic schemas for LLM extraction (Gemini-compatible)"
      exports: ["ExtractedBorrower", "BorrowerExtractionResult"]
      min_lines: 50
    - path: "backend/src/extraction/deduplication.py"
      provides: "BorrowerDeduplicator with deduplicate() method"
      exports: ["BorrowerDeduplicator"]
      min_lines: 80
    - path: "backend/src/extraction/extractor.py"
      provides: "BorrowerExtractor orchestrator"
      exports: ["BorrowerExtractor", "ExtractionResult"]
      min_lines: 150
  key_links:
    - from: "backend/src/extraction/extractor.py"
      to: "backend/src/extraction/llm_client.py"
      via: "GeminiClient.extract() call"
      pattern: "llm_client\\.extract"
    - from: "backend/src/extraction/extractor.py"
      to: "backend/src/extraction/complexity_classifier.py"
      via: "classifier.classify() call"
      pattern: "classifier\\.classify"
    - from: "backend/src/extraction/extractor.py"
      to: "backend/src/extraction/chunker.py"
      via: "chunker.chunk() call"
      pattern: "chunker\\.chunk"
    - from: "backend/src/extraction/deduplication.py"
      to: "rapidfuzz"
      via: "fuzzy name matching"
      pattern: "fuzz\\.token_sort_ratio"
---

<objective>
Create the extraction orchestrator that ties together all components: complexity classification, LLM extraction, chunking, deduplication, validation, and confidence scoring.

Purpose: This is the main extraction entry point - takes a document, returns validated borrower records with source attribution
Output: BorrowerExtractor.extract() returns list of BorrowerRecord with confidence scores and sources
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-extraction--validation/03-RESEARCH.md
@backend/src/models/borrower.py
@backend/src/models/document.py
@backend/src/ingestion/docling_processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Gemini-compatible extraction schemas</name>
  <files>
    backend/src/extraction/schemas.py
  </files>
  <action>
Create `schemas.py` with Pydantic models compatible with Gemini's structured output.

CRITICAL: Do NOT use `Field(default=...)` - this causes issues with Gemini. Use `Optional[T] = None` instead.

1. `ExtractedAddress(BaseModel)`:
   - `street: str`
   - `city: str`
   - `state: str` (two-letter code)
   - `zip_code: str`

2. `ExtractedIncome(BaseModel)`:
   - `amount: float` (use float not Decimal for JSON)
   - `period: str` (annual, monthly, weekly, biweekly)
   - `year: int`
   - `source_type: str` (employment, self-employment, other)
   - `employer: str | None = None`

3. `ExtractedBorrower(BaseModel)`:
   - `name: str`
   - `ssn: str | None = None`
   - `phone: str | None = None`
   - `email: str | None = None`
   - `address: ExtractedAddress | None = None`
   - `income_history: list[ExtractedIncome] = []`
   - `account_numbers: list[str] = []`
   - `loan_numbers: list[str] = []`

4. `BorrowerExtractionResult(BaseModel)`:
   - `borrowers: list[ExtractedBorrower] = []`
   - `extraction_notes: str | None = None` (quality issues noted by LLM)

NOTE: These schemas are SEPARATE from the models in `src/models/borrower.py` because:
- These are for LLM output parsing (simpler, no UUIDs, no sources)
- The borrower.py models are for storage (with IDs, sources, timestamps)
The extractor will convert between them.
  </action>
  <verify>
`python -c "from src.extraction.schemas import BorrowerExtractionResult; import json; print(json.dumps(BorrowerExtractionResult.model_json_schema(), indent=2)[:100])"` shows valid JSON schema
  </verify>
  <done>
Extraction schemas created, Gemini-compatible (no Field defaults), generate valid JSON schema
  </done>
</task>

<task type="auto">
  <name>Task 2: Create BorrowerDeduplicator with fuzzy matching</name>
  <files>
    backend/src/extraction/deduplication.py
    backend/tests/extraction/test_deduplication.py
  </files>
  <action>
Create `deduplication.py` with:

1. `BorrowerDeduplicator` class:
   - Class constant: `NAME_THRESHOLD = 90` (fuzzy match threshold)

   - `deduplicate(self, records: list[BorrowerRecord]) -> list[BorrowerRecord]`:
     - Iterate through records
     - For each record, check if duplicate of any in merged list
     - If duplicate found, merge records
     - If not duplicate, add to merged list
     - Return merged list

   - `_is_duplicate(self, a: BorrowerRecord, b: BorrowerRecord) -> bool`:
     Priority order:
     1. Exact SSN match (both have SSN, they match)
     2. Exact account number match (any overlap in account_numbers)
     3. Fuzzy name match (>= 90%) + same zip code
     4. Very high name match (>= 95%) without address
     5. Name match (>= 80%) + last 4 of SSN match

     Use `fuzz.token_sort_ratio(a.name.lower(), b.name.lower(), processor=utils.default_process)` from rapidfuzz

   - `_merge_records(self, existing: BorrowerRecord, new: BorrowerRecord) -> BorrowerRecord`:
     - Keep record with higher confidence_score as base
     - Merge income_history (deduplicate by checking equality)
     - Union account_numbers and loan_numbers (as sets)
     - Combine sources lists
     - Fill in missing optional fields (ssn, phone, email, address) from other record
     - Keep max confidence_score

NOTE: Deduplication MERGES records that are definitely the same person. Consistency VALIDATION (Plan 03-05) flags potential issues for human review. These are separate concerns.

Unit tests (test_deduplication.py):
- `test_no_duplicates` - Different borrowers stay separate
- `test_exact_ssn_match` - Same SSN merged
- `test_account_number_match` - Same account merged
- `test_fuzzy_name_match` - "John Smith" and "JOHN SMITH" merged if same zip
- `test_high_name_similarity_no_address` - 95%+ similar names merged
- `test_merge_combines_data` - Income histories combined, accounts unioned
- `test_higher_confidence_preferred` - Base record is higher confidence one
  </action>
  <verify>
`cd backend && python -m pytest tests/extraction/test_deduplication.py -v` passes
  </verify>
  <done>
BorrowerDeduplicator merges duplicates by SSN, account, or fuzzy name matching
  </done>
</task>

<task type="auto">
  <name>Task 3: Create BorrowerExtractor orchestrator</name>
  <files>
    backend/src/extraction/extractor.py
    backend/src/extraction/__init__.py
    backend/tests/extraction/test_extractor.py
  </files>
  <action>
Create `extractor.py` with:

1. `ExtractionResult` dataclass:
   - `borrowers: list[BorrowerRecord]`
   - `complexity: ComplexityAssessment`
   - `chunks_processed: int`
   - `total_tokens: int`
   - `validation_errors: list[ValidationError]`

2. `BorrowerExtractor` class:
   - `__init__(self, llm_client: GeminiClient, classifier: ComplexityClassifier, chunker: DocumentChunker, validator: FieldValidator, confidence_calc: ConfidenceCalculator, deduplicator: BorrowerDeduplicator)`

   - `extract(self, document: DocumentContent, document_id: UUID, document_name: str) -> ExtractionResult`:

     Step 1: Assess complexity
     ```python
     assessment = self.classifier.classify(document.text, document.page_count)
     use_pro = assessment.level == ComplexityLevel.COMPLEX
     ```

     Step 2: Chunk document
     ```python
     chunks = self.chunker.chunk(document.text)
     ```

     Step 3: Extract from each chunk
     - For each chunk:
       - Find page number for chunk position (helper method below)
       - Build prompt: `build_extraction_prompt(chunk.text)`
       - Call `self.llm_client.extract(prompt, BorrowerExtractionResult, use_pro, EXTRACTION_SYSTEM_PROMPT)`
       - Track total tokens
       - If response.parsed, convert ExtractedBorrower to BorrowerRecord:
         - Generate new UUID
         - Convert ExtractedIncome to IncomeRecord (Decimal for amount)
         - Convert ExtractedAddress to Address
         - Add SourceReference with document_id, document_name, page_number, snippet[:200]
         - Set initial confidence_score to 0.5 (will recalculate)
       - Add to all_borrowers list

     Step 4: Deduplicate
     ```python
     merged = self.deduplicator.deduplicate(all_borrowers)
     ```

     Step 5: Validate and score each borrower
     - For each borrower:
       - Validate SSN, phone, zip
       - Track validation errors
       - Calculate confidence with format_validation_passed and source_count=len(borrower.sources)
       - Update borrower.confidence_score

     Return ExtractionResult with all data

   - Helper: `_find_page_for_position(self, document: DocumentContent, char_pos: int) -> int`:
     - If pages have text, search for chunk text in page texts
     - Fallback: estimate based on char_pos / (len(text) / page_count)

   - Helper: `_convert_to_borrower_record(...)`: Convert ExtractedBorrower + source info to BorrowerRecord

Update `__init__.py` to export:
- `BorrowerExtractor`, `ExtractionResult`
- `GeminiClient`, `LLMResponse`
- `ComplexityClassifier`, `ComplexityLevel`
- `DocumentChunker`
- `FieldValidator`, `ConfidenceCalculator`
- `BorrowerDeduplicator`

Unit tests (test_extractor.py):
- `test_extract_simple_document` - Mock LLM response, verify borrower extracted
- `test_uses_flash_for_simple` - Verify use_pro=False for simple docs
- `test_uses_pro_for_complex` - Verify use_pro=True for complex docs
- `test_chunking_triggered` - Large doc splits into chunks
- `test_source_attribution_added` - Borrowers have SourceReference
- `test_deduplication_applied` - Duplicate borrowers merged
- `test_validation_errors_tracked` - Invalid SSN tracked in errors
- `test_confidence_calculated` - Borrowers have confidence_score
  </action>
  <verify>
`cd backend && python -m pytest tests/extraction/test_extractor.py -v` passes
  </verify>
  <done>
BorrowerExtractor orchestrates the full extraction pipeline: classify -> chunk -> extract -> dedupe -> validate -> score
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.extraction import BorrowerExtractor, ExtractionResult"` - Import succeeds
2. `python -m pytest tests/extraction/ -v` - All extraction tests pass
3. Full pipeline test (if Gemini API key available):
   ```python
   from src.extraction import BorrowerExtractor, GeminiClient, ComplexityClassifier, DocumentChunker, FieldValidator, ConfidenceCalculator, BorrowerDeduplicator
   from src.ingestion.docling_processor import DocumentContent
   from uuid import uuid4

   # Create components
   extractor = BorrowerExtractor(
       llm_client=GeminiClient(),
       classifier=ComplexityClassifier(),
       chunker=DocumentChunker(),
       validator=FieldValidator(),
       confidence_calc=ConfidenceCalculator(),
       deduplicator=BorrowerDeduplicator(),
   )

   # Test with sample document
   doc = DocumentContent(text="John Smith, SSN: 123-45-6789, ...", pages=[], page_count=1, tables=[], metadata={})
   result = extractor.extract(doc, uuid4(), "test.pdf")
   print(f"Extracted {len(result.borrowers)} borrowers")
   ```
</verification>

<success_criteria>
- BorrowerExtractor.extract() takes DocumentContent and returns ExtractionResult
- Complexity assessment routes to appropriate model
- Large documents chunked with overlap
- Chunk results aggregated and deduplicated
- Source attribution added to all borrowers
- Confidence scores calculated based on completeness and validation
- Validation errors tracked per extraction
- EXTRACT-19 through EXTRACT-29 requirements addressed
- NOTE: VALID-07, VALID-08, VALID-09 (consistency validation) are addressed by Plan 03-05
- 20+ unit tests passing across all extraction modules
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-extraction--validation/03-04-SUMMARY.md`
</output>
