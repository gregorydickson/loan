---
phase: 03-llm-extraction-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/src/extraction/__init__.py
  - backend/src/extraction/llm_client.py
  - backend/tests/extraction/__init__.py
  - backend/tests/extraction/test_llm_client.py
autonomous: true

must_haves:
  truths:
    - "Gemini client initializes with API key from environment"
    - "Extract method returns structured LLMResponse with parsed Pydantic model"
    - "Retry logic handles 429 rate limiting with exponential backoff"
    - "Server errors (5xx) trigger retry with appropriate wait"
    - "None response handled gracefully when output truncated"
    - "Token usage tracked in response metrics"
  artifacts:
    - path: "backend/src/extraction/llm_client.py"
      provides: "GeminiClient class with extract/extract_async methods"
      exports: ["GeminiClient", "LLMResponse"]
      min_lines: 120
    - path: "backend/tests/extraction/test_llm_client.py"
      provides: "Unit tests for Gemini client with mocked API"
      min_lines: 80
  key_links:
    - from: "backend/src/extraction/llm_client.py"
      to: "google.genai"
      via: "SDK client initialization"
      pattern: "genai\\.Client"
    - from: "backend/src/extraction/llm_client.py"
      to: "tenacity"
      via: "retry decorator"
      pattern: "@retry"
---

<objective>
Create a production-ready Gemini API client with structured output support, retry logic, and usage metrics.

Purpose: Foundation for all LLM extraction - every extraction call will go through this client
Output: `GeminiClient` class with `extract()` and `extract_async()` methods returning `LLMResponse`
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-extraction--validation/03-RESEARCH.md
@backend/src/models/borrower.py
@backend/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extraction package and LLMResponse dataclass</name>
  <files>
    backend/src/extraction/__init__.py
    backend/src/extraction/llm_client.py
  </files>
  <action>
Create `backend/src/extraction/` package with `__init__.py`.

In `llm_client.py`, create:

1. `LLMResponse` dataclass with fields:
   - `content: str` - Raw response text
   - `parsed: BaseModel | None` - Parsed Pydantic model (None if truncated)
   - `input_tokens: int` - Input token count
   - `output_tokens: int` - Output token count
   - `latency_ms: int` - Request latency in milliseconds
   - `model_used: str` - Model name that was called
   - `finish_reason: str` - Why generation stopped

2. `GeminiClient` class with:
   - Class constants: `FLASH_MODEL = "gemini-3-flash-preview"`, `PRO_MODEL = "gemini-3-pro-preview"`
   - `__init__(self, api_key: str | None = None)` - Initialize with API key or from env (GEMINI_API_KEY or GOOGLE_API_KEY)
   - Internal `self.client = genai.Client(api_key=api_key) if api_key else genai.Client()`

Key implementation details:
- Use `from google import genai` and `from google.genai import types, errors`
- Use `tenacity` for retry: `@retry(wait=wait_exponential_jitter(initial=1, max=60, jitter=5), stop=stop_after_attempt(3), retry=retry_if_exception_type(errors.APIError))`
- CRITICAL: Set `temperature=1.0` (Gemini 3 requires this, lower causes looping)
- CRITICAL: Do NOT set `max_output_tokens` (causes None response with structured output)
- Use `response_mime_type="application/json"` and `response_json_schema=schema.model_json_schema()` for structured output
  </action>
  <verify>
`python -c "from src.extraction.llm_client import GeminiClient, LLMResponse"` succeeds in backend directory
  </verify>
  <done>
LLMResponse dataclass and GeminiClient class created with proper imports and retry decorator
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement extract and extract_async methods</name>
  <files>
    backend/src/extraction/llm_client.py
  </files>
  <action>
Implement the extraction methods in GeminiClient:

1. `extract(self, text: str, schema: type[BaseModel], use_pro: bool = False, system_instruction: str | None = None) -> LLMResponse`:
   - Select model based on `use_pro` flag
   - Start timing with `time.perf_counter()`
   - Build `GenerateContentConfig` with:
     - `system_instruction` (if provided)
     - `temperature=1.0`
     - `response_mime_type="application/json"`
     - `response_json_schema=schema.model_json_schema()`
   - Call `self.client.models.generate_content(model=model, contents=text, config=config)`
   - Handle `errors.APIError` - re-raise to trigger tenacity retry
   - Handle `response.text is None` - return LLMResponse with `parsed=None`, `finish_reason="MAX_TOKENS"`
   - Parse response: `parsed = schema.model_validate_json(response.text)`
   - Return LLMResponse with all metrics from `response.usage_metadata`

2. `async extract_async(...)` - Same logic but use `self.client.aio.models.generate_content`

Edge cases to handle:
- `response.usage_metadata.prompt_token_count` may be None - use `or 0`
- `response.usage_metadata.candidates_token_count` may be None - use `or 0`
- `response.candidates` may be empty - check before accessing `finish_reason`
  </action>
  <verify>
`python -c "from src.extraction.llm_client import GeminiClient; c = GeminiClient.__new__(GeminiClient); print(hasattr(c, 'extract'), hasattr(c, 'extract_async'))"` prints `True True`
  </verify>
  <done>
Both sync and async extract methods implemented with proper error handling and metrics tracking
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests with mocked Gemini API</name>
  <files>
    backend/tests/extraction/__init__.py
    backend/tests/extraction/test_llm_client.py
  </files>
  <action>
Create test package and comprehensive unit tests.

Test cases to implement:

1. `test_gemini_client_init_default` - Verify client initializes without API key (uses env)
2. `test_gemini_client_init_with_key` - Verify client accepts explicit API key
3. `test_extract_returns_llm_response` - Mock successful response, verify LLMResponse fields
4. `test_extract_uses_flash_by_default` - Verify FLASH_MODEL used when use_pro=False
5. `test_extract_uses_pro_when_requested` - Verify PRO_MODEL used when use_pro=True
6. `test_extract_handles_none_response` - Mock None text response, verify parsed=None and finish_reason="MAX_TOKENS"
7. `test_extract_parses_pydantic_model` - Verify JSON response parsed into Pydantic model
8. `test_retry_on_api_error` - Mock APIError, verify retry is triggered (use tenacity test utilities or mock call count)

Mocking approach:
- Use `unittest.mock.patch` to mock `genai.Client`
- Create mock response objects with `usage_metadata` and `candidates` attributes
- Use `MagicMock` for nested attributes

Create a simple test schema:
```python
class TestBorrower(BaseModel):
    name: str
    age: int | None = None
```
  </action>
  <verify>
`cd backend && python -m pytest tests/extraction/test_llm_client.py -v` passes all tests
  </verify>
  <done>
8+ unit tests pass, covering initialization, model selection, error handling, and response parsing
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.extraction import llm_client"` - Import succeeds
2. `python -m pytest tests/extraction/ -v` - All tests pass
3. `python -m mypy src/extraction/llm_client.py --strict` - Type checks pass (ignore google-genai stubs if missing)
</verification>

<success_criteria>
- GeminiClient class with extract() and extract_async() methods
- LLMResponse dataclass with all required fields
- Retry logic with tenacity for API errors
- None response handling for truncated output
- 8+ unit tests passing with mocked API
- EXTRACT-01 through EXTRACT-10 requirements addressed
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-extraction--validation/03-01-SUMMARY.md`
</output>
